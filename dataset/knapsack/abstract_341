Password: A parallel implementation via CUDA of the dynamic programming method for the knapsack problem on NVIDIA GPU is presented. A GTX 260 card with 192 cores (1.4 GHz) is used for computational tests and processing times obtained with the parallel code are compared to the sequential one on a CPU with an Intel Xeon 3.0 GHz. The results show a speedup factor of 26 for large size problems. Furthermore, in order to limit the communication between the CPU and the GPU, a compression technique is presented which decreases significantly the memory occupancy. Keywords Combinatorial optimization problems ; Dense dynamic programming ; Parallel computing ; GPU computing ; CUDA 1. Introduction Since a few years, graphics card manufacturers have developed technologies in order to use their cards for High Performance Computing (HPC); we recall that Graphics Processing Units (GPUs) are HPC many-core processors. In particular, NVIDIA has developed the Compute Unified Device Architecture (CUDA) so as to program efficiently its GPUs. CUDA technology is based on a Single Instruction, Multiple Threads (SIMT) programming model. The SIMT model is akin to Single Instruction, Multiple Data (SIMD) model [1] . Using GPU architectures for solving large scale or difficult optimization problems like combinatorial optimization problems is nevertheless a great challenge due to the specificities of GPU architectures. We have been solving recently difficult combinatorial optimization of the knapsack family like multidimensional knapsack problems (see [2]  and  [3] ), multiple knapsack problems [4] and knapsack sharing problems [5] . We are presently interested in the use of GPUs in order to speed up the solution of NP-hard problems. In particular, we are developing a series of parallel codes on GPUs that can be combined in order to produce efficient parallel methods. We have studied parallel branch and bound methods for the solution of knapsack problems (KP) on GPUs in [6] . We have also presented a parallel simplex algorithm on GPUs for linear programming problems and bound computation purpose in [7] . In this paper, we propose a parallel implementation of dynamic programming algorithm on GPU for KP (see also [8] ). A data compression technique is also presented. This technique permits one to reduce significantly memory occupancy and communication between the CPU and the GPU. Parallel implementation of dense dynamic programming algorithm for KP on NVIDIA GPUs via CUDA is detailed. The paper is structured as follows. Section 2 deals with related work. The knapsack problem and its solution via dynamic programming is presented in Section 3 . Section 4 focuses on GPU computing and the parallel dynamic programming method. In Section 5 , we propose a compression method that permits one to reduce significantly the memory occupancy and communication between CPU and GPU. Section 6 deals with computational experiments. Conclusions and future work are presented in Section 7 . 2. Related work Several parallel dynamic programming methods have been proposed for KP in the literature (see, for example: [9]  and  [10] ). In particular, implementations on SIMD machines were performed on a 4K processor ICL DAP [11] , a 16K Connection Machine CM-2 (see [12]  and  [13] ) and a 4K MasPar MP-1 machine [13] . Reference is also made to [14] for a study on a parallel dynamic programming list algorithm using dominance techniques. Several load balancing methods have been proposed in [14]  and  [15] in order to improve algorithm efficiency. The parallel algorithm and load balancing methods have been carried out on an Origin 3800 SGI supercomputer. The reader is also referred to [16] for parallel dynamic programming algorithm for subset sum problems. 3. The knapsack problem The knapsack problem is a NP-hard combinatorial optimization problem. It is one of the most studied discrete optimization problems as it is among the simplest prototypes of integer linear programming problems and it arises as a sub-problem of many complex problems (see, for example [2] , [17] , [18]  and  [19] ).