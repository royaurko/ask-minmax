Password: While the 1980s were focused on the solution of large sized “easy” knapsack problems (KPs), this decade has brought several new algorithms, which are able to solve “hard” large sized instances. We will give an overview of the recent techniques for solving hard KPs, with special emphasis on the addition of cardinality constraints, dynamic programming, and rudimentary divisibility. Computational results, comparing all recent algorithms, are presented. Keywords Knapsack problem ; Branch-and-bound ; Dynamic programming ; State-of-the-art 1. Introduction We consider the classical 0–1 knapsack problem (KP) where a subset of n given items has to be packed in a knapsack of capacity c . Each item has a profit p j and a weight w j and the problem is to select a subset of the items whose total weight does not exceed c and whose total profit is a maximum. We assume, without loss of generality, that all input data are positive integers. Introducing the binary decision variables x j , with x j =1 if item j is selected, and x j =0 otherwise, we get the integer linear programming (ILP) model: The problem, which is NP-hard, has been thoroughly studied in the last few decades and several exact algorithms for its solution can be found in the literature. If we were to give a brief description of the state-of-the-art, we could say that: (i) Problems with exponentially growing coefficients cannot be solved efficiently, and we should not expect them to be solved efficiently due to the NP-hardness of the problem. (ii) Problems with bounded coefficients can however be solved very fast if the LP and the ILP solutions are sufficiently close to each other. This applies to instance types such as the so-called uncorrelated , weakly correlated and subset-sum problems. (iii) Problems with bounded coefficients where the LP solutions differ considerably from the ILP solutions are still difficult to solve by means of non-specialized algorithms. Among these instances we have the so-called strongly correlated problems and different variants. Besides, non-fill problems such as the even–odd problems are difficult to solve. In the following we will give a historical overview of the different approaches, which have gradually extended the classes of instances that can be solved in a reasonably short time. 2. Basic branch-and-bound algorithms Algorithms for KPs are mainly based on two approaches: branch-and-bound and dynamic programming. The actual performance of an algorithm, however, strictly depends on the way tight upper bounds are applied. The first upper bound for KP, based on a continuous relaxation, was presented by Dantzig [3] in mid 1950s. It is obtained by sorting the items so that equation ( 2 ) equation ( 3 ) equation ( 4 ) and the corresponding Dantzig integer solution is x j =1 for j < s and x j =0 for j ⩾ s . No better bounds were presented in the following two decades until Martello and Toth [9] presented a tighter bound by imposing integrality on the critical variable. Since then, several other bounds have been presented based on Lagrangian relaxation, partial enumeration, construction of valid additional constraints, and different relaxations of the latter. The first branch-and-bound algorithms for KP appeared in the early 1970s. Among the most successful we should mention the algorithms by Horowitz and Sahni [7] , Fayard and Plateau [4] , Nauss [13] and Martello and Toth [9] . These algorithms are all based on a depth-first enumeration, in order to limit the space consumption. The branching strategy consists of selecting an item j and generating two children nodes through conditions x j =1 and x j =0. The items are examined in the order given by (2) , and the branching node is selected as the (unique) active node generated by a condition x k =1 or, if there is no such node, as the last active node generated by a condition x k =0. Upper bounds for these algorithms are derived from some kind of continuous relaxation of the currently induced subproblem. A comparison of the algorithms, presented in Martello and Toth [11] , shows that they perform well for small sized “easy” instances. 3. Core algorithms In order to solve large-sized instances, Balas and Zemel [1] proposed “guessing” the optimal values of several decision variables, and focus the enumeration on the most interesting ones. This subset of items, known as the core of the problem, is then solved either by heuristic techniques or by one of the above exact branch-and-bound methods. The core C could be determined by finding the critical variable, x s , and setting C ={ s − δ ,…, s ,…, s + δ } in the sequence given by (2) , for a prefixed value δ . In order to avoid explicit sorting, Balas and Zemel give a procedure based on partitioning techniques that determines both x s and the core in O( n ) time. All variables preceding the core are fixed to 1, while all those following it are fixed to (i.e., for each item j not in the core, x j is set to one, if p j / w j ⩾ p s / w s , or to 0 otherwise). The solution of the core problem yields a lower bound which in many cases corresponds to the optimal solution value. In order to prove optimality of the solution found, either an upper bound having the same value is determined, or reduction procedures are used to prove that items not in the core should have the above specific value. When optimality cannot be proved, items which were not fixed to a specific value by reduction are finally enumerated to optimality. Effective algorithms based on a core problem were presented by Balas and Zemel [1] , Fayard and Plateau [5] , Plateau and Elkihel [16] , and Martello and Toth [10] . A computational comparison of core algorithms presented in [11] shows that the algorithm by Martello and Toth gives the best performance. A final refinement of the core approach was presented in Pisinger [14] , where an expanding core was used to ensure that the core size automatically adapts to the hardness of the problem. The algorithm starts from the Dantzig integer solution, and at each iteration it either inserts a new item or removes an item, depending on whether the weight sum w̄ of the chosen items is less than c or greater than c . Initially the core C contains only item s , but whenever a new item j ∉ C is considered, the core is automatically expanded by sorting some more items. Simple bounding rules obtained from linear relaxation were used to limit the search. With a core C ={ a ,…, b } an upper bound is found to be equation ( 5 ) when the currently chosen items have profit sum p̄ and weight sum w̄. The algorithm backtracks whenever where z is the incumbent solution value.