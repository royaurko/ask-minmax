Over binary input channels, uniform distribution is a universal prior, in the
sense that it allows to maximize the worst case mutual information over all
binary input channels, ensuring at least 94.2% of the capacity. In this paper,
we address a similar question, but with respect to a universal generalized
linear decoder. We look for the best collection of finitely many a posteriori
metrics, to maximize the worst case mismatched mutual information achieved by
decoding with these metrics (instead of an optimal decoder such as the Maximum
Likelihood (ML) tuned to the true channel). It is shown that for binary input
and output channels, two metrics suffice to actually achieve the same
performance as an optimal decoder. In particular, this implies that there exist
a decoder which is generalized linear and achieves at least 94.2% of the
compound capacity on any compound set, without the knowledge of the underlying
set.