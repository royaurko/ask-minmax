Permutation entropy quantifies the diversity of possible orderings of the
values a random or deterministic system can take, as Shannon entropy quantifies
the diversity of values. We show that the metric and permutation entropy
rates--measures of new disorder per new observed value--are equal for ergodic
finite-alphabet information sources (discrete-time stationary stochastic
processes). With this result, we then prove that the same holds for
deterministic dynamical systems defined by ergodic maps on $n$% -dimensional
intervals. This result generalizes a previous one for piecewise monotone
interval maps on the real line (Bandt, Keller and Pompe, "Entropy of interval
maps via permutations",\textit{Nonlinearity} \textbf{15}, 1595-602, (2002)), at
the expense of requiring ergodicity and using a definition of permutation
entropy rate differing in the order of two limits. The case of non-ergodic
finite-alphabet sources is also studied and an inequality developed. Finally,
the equality of permutation and metric entropy rates is extended to ergodic
non-discrete information sources when entropy is replaced by differential
entropy in the usual way.