We devise a new embedding technique, which we call measured descent, based on
decomposing a metric space locally, at varying speeds, according to the density
of some probability measure. This provides a refined and unified framework for
the two primary methods of constructing Frechet embeddings for finite metrics,
due to [Bourgain, 1985] and [Rao, 1999]. We prove that any n-point metric space
(X,d) embeds in Hilbert space with distortion O(sqrt{alpha_X log n}), where
alpha_X is a geometric estimate on the decomposability of X. As an immediate
corollary, we obtain an O(sqrt{(log lambda_X) \log n}) distortion embedding,
where \lambda_X is the doubling constant of X. Since \lambda_X\le n, this
result recovers Bourgain's theorem, but when the metric X is, in a sense,
``low-dimensional,'' improved bounds are achieved.
  Our embeddings are volume-respecting for subsets of arbitrary size. One
consequence is the existence of (k, O(log n)) volume-respecting embeddings for
all 1 \leq k \leq n, which is the best possible, and answers positively a
question posed by U. Feige. Our techniques are also used to answer positively a
question of Y. Rabinovich, showing that any weighted n-point planar graph
embeds in l_\infty^{O(log n)} with O(1) distortion. The O(log n) bound on the
dimension is optimal, and improves upon the previously known bound of O((log
n)^2).