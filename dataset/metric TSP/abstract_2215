We design a data-dependent metric in $\mathbb R^d$ and use it to define the
$k$-nearest neighbors of a given point. Our metric is invariant under all
affine transformations. We show that, with this metric, the standard
$k$-nearest neighbor regression estimate is asymptotically consistent under the
usual conditions on $k$, and minimal requirements on the input data.