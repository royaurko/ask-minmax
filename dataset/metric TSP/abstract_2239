This paper studies the problem of predicting the coding effort for a
subsequent year of development by analysing metrics extracted from project
repositories, with an emphasis on projects containing XML code. The study
considers thirteen open source projects and applies machine learning algorithms
to generate models to predict one-year coding effort, measured in terms of
lines of code added, modified and deleted. Both organisational and code metrics
associated to revisions are taken into account. The results show that coding
effort is highly determined by the expertise of developers while source code
metrics have little effect on improving the accuracy of estimations of coding
effort. The study also shows that models trained on one project are unreliable
at estimating effort in other projects.