We describe a Monte Carlo method to approximate the maximum likelihood
estimate (MLE), when there are missing data and the observed data likelihood is
not available in closed form. This method uses simulated missing data that are
independent and identically distributed and independent of the observed data.
Our Monte Carlo approximation to the MLE is a consistent and asymptotically
normal estimate of the minimizer $\theta^*$ of the Kullback--Leibler
information, as both Monte Carlo and observed data sample sizes go to infinity
simultaneously. Plug-in estimates of the asymptotic variance are provided for
constructing confidence regions for $\theta^*$. We give Logit--Normal
generalized linear mixed model examples, calculated using an R package.