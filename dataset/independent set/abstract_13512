This paper studies the subspace segmentation problem which aims to segment
data drawn from a union of multiple linear subspaces. Recent works by using
sparse representation, low rank representation and their extensions attract
much attention. If the subspaces from which the data drawn are independent or
orthogonal, they are able to obtain a block diagonal affinity matrix, which
usually leads to a correct segmentation. The main differences among them are
their objective functions. We theoretically show that if the objective function
satisfies some conditions, and the data are sufficiently drawn from independent
subspaces, the obtained affinity matrix is always block diagonal. Furthermore,
the data sampling can be insufficient if the subspaces are orthogonal. Some
existing methods are all special cases. Then we present the Least Squares
Regression (LSR) method for subspace segmentation. It takes advantage of data
correlation, which is common in real data. LSR encourages a grouping effect
which tends to group highly correlated data together. Experimental results on
the Hopkins 155 database and Extended Yale Database B show that our method
significantly outperforms state-of-the-art methods. Beyond segmentation
accuracy, all experiments demonstrate that LSR is much more efficient.