Despite its simplicity, the naive Bayes classifier has surprised machine
learning researchers by exhibiting good performance on a variety of learning
problems. Encouraged by these results, researchers have looked to overcome
naive Bayes primary weakness - attribute independence - and improve the
performance of the algorithm. This paper presents a locally weighted version of
naive Bayes that relaxes the independence assumption by learning local models
at prediction time. Experimental results show that locally weighted naive Bayes
rarely degrades accuracy compared to standard naive Bayes and, in many cases,
improves accuracy dramatically. The main advantage of this method compared to
other techniques for enhancing naive Bayes is its conceptual and computational
simplicity.