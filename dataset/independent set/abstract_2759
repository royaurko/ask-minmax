Consider a parametrized family of general hidden Markov models, where both
the observed and unobserved components take values in a complete separable
metric space. We prove that the maximum likelihood estimator (MLE) of the
parameter is strongly consistent under a rather minimal set of assumptions. As
special cases of our main result, we obtain consistency in a large class of
nonlinear state space models, as well as general results on linear Gaussian
state space models and finite state models. A novel aspect of our approach is
an information-theoretic technique for proving identifiability, which does not
require an explicit representation for the relative entropy rate. Our method of
proof could therefore form a foundation for the investigation of MLE
consistency in more general dependent and non-Markovian time series. Also of
independent interest is a general concentration inequality for $V$-uniformly
ergodic Markov chains.