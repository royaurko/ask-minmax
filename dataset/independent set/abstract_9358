Aggregation of noisy observations involves a difficult tradeoff between
observation quality, which can be increased by increasing the number of
observations, and aggregation quality which decreases if the number of
observations is too large. We clarify this behavior for a protypical system in
which arbitrarily large numbers of observations exceeding the system capacity
can be aggregated using lossy data compression. We show the existence of a
scaling relation between the collective error and the system capacity, and show
that large scale lossy aggregation can outperform lossless aggregation above a
critical level of observation noise. Further, we show that universal results
for scaling and critical value of noise which are independent of system
capacity can be obtained by considering asymptotic behavior when the system
capacity increases toward infinity.