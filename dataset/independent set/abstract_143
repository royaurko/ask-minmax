Independence screening is a powerful method for variable selection for `Big
Data' when the number of variables is massive. Commonly used independence
screening methods are based on marginal correlations or variations of it. In
many applications, researchers often have some prior knowledge that a certain
set of variables is related to the response. In such a situation, a natural
assessment on the relative importance of the other predictors is the
conditional contributions of the individual predictors in presence of the known
set of variables. This results in conditional sure independence screening
(CSIS). Conditioning helps for reducing the false positive and the false
negative rates in the variable selection process. In this paper, we propose and
study CSIS in the context of generalized linear models. For
ultrahigh-dimensional statistical problems, we give conditions under which sure
screening is possible and derive an upper bound on the number of selected
variables. We also spell out the situation under which CSIS yields model
selection consistency. Moreover, we provide two data-driven methods to select
the thresholding parameter of conditional screening. The utility of the
procedure is illustrated by simulation studies and analysis of two real data
sets.