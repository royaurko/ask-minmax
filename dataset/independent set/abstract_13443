We provide a unifying framework linking two classes of statistics used in
two-sample and independence testing: on the one hand, the energy distances and
distance covariances from the statistics literature; on the other, maximum mean
discrepancies (MMD), that is, distances between embeddings of distributions to
reproducing kernel Hilbert spaces (RKHS), as established in machine learning.
In the case where the energy distance is computed with a semimetric of negative
type, a positive definite kernel, termed distance kernel, may be defined such
that the MMD corresponds exactly to the energy distance. Conversely, for any
positive definite kernel, we can interpret the MMD as energy distance with
respect to some negative-type semimetric. This equivalence readily extends to
distance covariance using kernels on the product space. We determine the class
of probability distributions for which the test statistics are consistent
against all alternatives. Finally, we investigate the performance of the family
of distance kernels in two-sample and independence tests: we show in particular
that the energy distance most commonly employed in statistics is just one
member of a parametric family of kernels, and that other choices from this
family can yield more powerful tests.