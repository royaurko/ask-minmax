Nonparametric and nonlinear measures of statistical dependence between pairs
of random variables have proved themselves important tools in modern data
analysis, where the emergence of large data sets can support the relaxation of
linearity assumptions implicit in traditional association scores such as
correlation. Recent proposals based around estimating information theoretic
measures such as Mutual Information (MI) have been particularly popular. Here
we describe a Bayesian nonparametric procedure that leads to a tractable,
explicit and analytic quantification of the probability of dependence, using
Polya tree priors on the space of probability measures. Our procedure can
accommodate known uncertainty in the form of the underlying sampling
distribution and provides an explicit posterior probability measure of both
dependence and independence. Well known advantages of having an explicit
probability measure include the easy comparison of evidence across different
studies, the inclusion of prior information, and the integration of results
within decision analysis.