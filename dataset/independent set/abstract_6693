Consider the setting of sparse graphs on N vertices, where the vertices have
distinct "names", which are strings of length O(log N) from a fixed finite
alphabet. For many natural probability models, the entropy grows as cN log N
for some model-dependent rate constant c. The mathematical content of this
paper is the (often easy) calculation of c for a variety of models, in
particular for various standard random graph models adapted to this setting.
Our broader purpose is to publicize this particular setting as a natural
setting for future theoretical study of data compression for graphs, and (more
speculatively) for discussion of unorganized versus organized complexity.