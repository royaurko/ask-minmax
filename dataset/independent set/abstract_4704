We study the quality of outcomes in games when the population of players is
dynamically changing, and where participants have to adapt to the dynamic
environment. Price of Anarchy has originally been introduced to study the Nash
equilibria of one-shot games. The Price of Total Anarchy extends this notion to
learning outcomes in a repeated setting, assuming all players use a form of
no-regret learning to choose their strategies, and the environment as well as
the player population is stable. In this paper we consider repeated games in
dynamically changing environments. We show that in large classes of games, if
players use a form of \emph{adaptive learning} to adapt their strategy choices
to the changing environment, this guarantees high social welfare, even under
very frequent changes.
  An important feature of our analysis is the connection to differential
privacy. The optimal solution to the welfare optimization problem can alter
drastically even with a single change in the parameters of one participant. To
take advantage of the adaptive learning in the analysis, we need to show that
there is benchmark solution with which we compare the regret of a player, that
is both close to optimal and relatively \emph{stable} over time. Differential
privacy offers just the right tool. It requires that the proposed solution does
not alter much with the change of one person's input. We show that the
existence of a differentially private algorithm for the welfare optimization
problem implies high welfare in the dynamic game even under very frequent
changes.
  We demonstrate our techniques by focusing on two classes of games as
examples: independent item auctions and congestion games. In both applications
we show that adaptive learning guarantees high social welfare even with
surprisingly high churn in the player population.