This work concerns a comparison of SVM kernel methods in text categorization
tasks. In particular I define a kernel function that estimates the similarity
between two objects computing by their compressed lengths. In fact, compression
algorithms can detect arbitrarily long dependencies within the text strings.
Data text vectorization looses information in feature extractions and is highly
sensitive by textual language. Furthermore, these methods are language
independent and require no text preprocessing. Moreover, the accuracy computed
on the datasets (Web-KB, 20ng and Reuters-21578), in some case, is greater than
Gaussian, linear and polynomial kernels. The method limits are represented by
computational time complexity of the Gram matrix and by very poor performance
on non-textual datasets.