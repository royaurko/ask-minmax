We consider the problem of aggregating the elements of a possibly infinite
dictionary for building a decision procedure that aims at minimizing a given
criterion. Along with the dictionary, an independent identically distributed
training sample is available, on which the performance of a given procedure can
be tested. In a fairly general set-up, we establish an oracle inequality for
the Mirror Averaging aggregate with any prior distribution. By choosing an
appropriate prior, we apply this oracle inequality in the context of prediction
under sparsity assumption for the problems of regression with random design,
density estimation and binary classification.