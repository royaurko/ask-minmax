For $k \in \mathbb{Z}_{+}$, a {\em$k$-SIIRV of order $n \in \mathbb{Z}_{+}$}
is the discrete probability distribution of the sum of $n$ mutually independent
random variables each supported on $\{0, 1, \dots, k-1\}$. We denote by ${\cal
S}_{n,k}$ the set of all $k$-SIIRV's of order $n$. In this paper we prove two
main results:
  1) We give a near-sample optimal and computationally efficient algorithm for
learning $k$-SIIRVs from independent samples under the total variation
distance. Our algorithm uses $\widetilde{O}(k/\epsilon^2)$ samples and runs in
$\widetilde{O}(k^3/\epsilon^2)$ time. This sample size is nearly optimal, as
$\Omega(k/\epsilon^2)$ samples are necessary to learn a random variable
supported on $\{0, 1, \dots, k-1\}$.
  2) We prove nearly tight bounds on the size of $\epsilon$-covers for ${\cal
S}_{n, k}$ under the total variation distance. In particular, we show that for
all $k, n \in \mathbb{Z}_{+}$ and $\epsilon \le 1/k$, ${\cal S}_{n, k}$ admits
an $\epsilon$-cover of size $n\cdot (1/\epsilon)^{O(k \cdot \log(1/\epsilon))}$
that can be constructed in polynomial time. We also prove a nearly matching
lower bound: For $k\in \mathbb{Z}_{+}$ and $n = \Omega(\log (1/\epsilon))$ any
$\epsilon$-cover for ${\cal S}_{n, k}$ has size at least $ n \cdot
(1/\epsilon)^{\Omega(k \cdot \log(1/\epsilon))}$. Using the structural
understanding obtained from our construction, we prove that the sample
complexity of learning $2$-SIIRVs is $\Omega((1/\epsilon^2)
\sqrt{\log(1/\epsilon)})$.
  The unifying idea of our upper bounds is an analysis of the structure of the
Fourier Transform of $k$-SIIRVs. Our learning algorithm relies on a structural
property of the Fourier transform of $k$-SIIRVs, namely that it has small
effective support. Our lower bounds employ a combination of geometric and
analytic arguments.