Finding interdependency relations between (possibly multivariate) time series
provides valuable knowledge about the processes that generate the signals.
Information theory sets a natural framework for non-parametric measures of
several classes of statistical dependencies. However, a reliable estimation
from information-theoretic functionals is hampered when the dependency to be
assessed is brief or evolves in time. Here, we show that these limitations can
be overcome when we have access to an ensemble of independent repetitions of
the time series. In particular, we gear a data-efficient estimator of
probability densities to make use of the full structure of trial-based
measures. By doing so, we can obtain time-resolved estimates for a family of
entropy combinations (including mutual information, transfer entropy, and their
conditional counterparts) which are more accurate than the simple average of
individual estimates over trials. We show with simulated and real data that the
proposed approach allows to recover the time-resolved dynamics of the coupling
between different subsystems.