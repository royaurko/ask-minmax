We study the computational relationship between optimization, online
learning, and learning in games: we consider models in which the online player
is given access to an optimization oracle for the corresponding offline
problem, and investigate how such oracle affects the computational complexity
of the online problem.
  First, we consider the fundamental setting of prediction with the advice of
$N$ experts, augmented with an optimization oracle that can be used to compute,
in constant time, the leading expert in retrospect at any point in time. In
this setting, we give an algorithm that attains vanishing regret in total
runtime of $\tilde{O}(\sqrt{N})$. We also give a lower bound showing that this
running time cannot be improved in the oracle model, up to logarithmic factors.
These results attest that an optimization oracle gives rise to a quadratic
speedup as compared to the standard oracle-free setting, where the required
time for vanishing regret is $\tilde{\Theta}(N)$.
  We then consider the closely related problem of learning in repeated $N
\times N$ zero-sum games, in a setting where the players have access to oracles
that compute the best-response to any mixed strategy of their opponent in
constant time. We give an efficient algorithm that converges to the value of
the game in total time $\tilde{O}(\sqrt{N})$, yielding again a quadratic
improvement upon the oracle-free setting, where $\tilde{\Theta}(N)$ is known to
be tight. We also show that this is the best possible, thereby obtaining a
tight characterization of the computational power of best-response oracles in
zero-sum games.
  Our results demonstrate an exponential gap between the power of optimization
in online learning and its power in the statistical learning setting: in the
latter, an optimization oracle, i.e., an empirical risk minimizer, allows to
learn a finite hypothesis class of size $N$ in time $O(\log{N})$.