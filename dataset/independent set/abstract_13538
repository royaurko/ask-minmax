Logistic regression models with $n$ observations and $q$ linearly-independent
covariates are shown to have Fisher information volumes which are bounded below
by $\pi^q$ and above by ${n \choose q} \pi^q$. This is proved with a novel
generalization of the classical theorems of Pythagoras and de Gua, which is of
independent interest. The finding that the volume is always finite is new, and
it implies that the volume can be directly interpreted as a measure of model
complexity. The volume is shown to be a continuous function of the design
matrix $X$ at generic $X$, but to be discontinuous in general. This means that
models with sparse design matrices can be significantly less complex than
nearby models, so the resulting model-selection criterion prefers sparse
models. This is analogous to the way that $\ell^1$-regularisation tends to
prefer sparse model fits, though in our case this behaviour arises
spontaneously from general principles. Lastly, an unusual topological duality
is shown to exist between the ideal boundaries of the natural and expectation
parameter spaces of logistic regression models.