Over the past few years, a family of interesting new inequalities for the
entropies of sums and differences of random variables has emerged, motivated by
analogous results in additive combinatorics. These inequalities were developed
by Marcus, Ruzsa, Tao, Tetali and the first-named contributor in the discrete case,
and by the contributors in the case of continuous random variables. The present work
extends these earlier results to the case of random vectors taking values in
$\mathbb{R}^n$ or, more generally, in arbitrary (locally compact and Polish)
abelian groups. We isolate and study a key quantity, the {\em Ruzsa divergence}
between two probability distributions, and we show that its properties can be
used to extend the earlier inequalities to the present general setting. The new
results established include several variations on the theme that the entropies
of the sum and the difference of two independent random variables severely
constrain each other. Although the setting is quite general, the result are
already of interest (and new) for random vectors in $\mathbb{R}^n$. In that
special case, quantitative bounds are provided for the stability of the
equality conditions in the entropy power inequality; a reverse entropy power
inequality for log-concave random vectors is proved; an information-theoretic
analog of the Rogers-Shephard inequality for convex bodies is established; and
it is observed that some of these results lead to new inequalities for the
determinants of positive-definite matrices.