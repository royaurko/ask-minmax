This paper discusses the asymptotic behavior of regression models under
general conditions. First, we give a general inequality for the difference of
the sum of square errors (SSE) of the estimated regression model and the SSE of
the theoretical best regression function in our model. A set of generalized
derivative functions is a key tool in deriving such inequality. Under suitable
Donsker condition for this set, we give the asymptotic distribution for the
difference of SSE. We show how to get this Donsker property for parametric
models even if the parameters characterizing the best regression function are
not unique. This result is applied to neural networks regression models with
redundant hidden units when loss of identifiability occurs.