Shannon's fundamental bound for perfect secrecy says that the entropy of the
secret message cannot be larger than the entropy of the secret key initially
shared by the sender and the legitimate receiver. Massey gave an information
theoretic proof of this result, however this proof does not require
independence of the key and ciphertext. By further assuming independence, we
obtain a tighter lower bound, namely that the key entropy is not less than the
logarithm of the message sample size in any cipher achieving perfect secrecy,
even if the source distribution is fixed. The same bound also applies to the
entropy of the ciphertext. The bounds still hold if the secret message has been
compressed before encryption.
  This paper also illustrates that the lower bound only gives the minimum size
of the pre-shared secret key. When a cipher system is used multiple times, this
is no longer a reasonable measure for the portion of key consumed in each
round. Instead, this paper proposes and justifies a new measure for key
consumption rate. The existence of a fundamental tradeoff between the expected
key consumption and the number of channel uses for conveying a ciphertext is
shown. Optimal and nearly optimal secure codes are designed.