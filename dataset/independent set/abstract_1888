The steep rise in availability and usage of high-throughput technologies in
biology brought with it a clear need for methods to control the False Discovery
Rate (FDR) in multiple tests. Benjamini and Hochberg (BH) introduced in 1995 a
simple procedure and proved that it provided a bound on the expected value,
$\mathit{FDR}\leq q$. Since then, many contributors tried to improve the BH bound,
with one approach being designing adaptive procedures, which aim at estimating
the number of true null hypothesis in order to get a better FDR bound. Our two
main rigorous results are the following: (i) a theorem that provides a bound on
the FDR for adaptive procedures that use any estimator for the number of true
hypotheses ($m_0$), (ii) a theorem that proves a monotonicity property of
general BH-like procedures, both for the case where the hypotheses are
independent. We also propose two improved procedures for which we prove FDR
control for the independent case, and demonstrate their advantages over several
available bounds, on simulated data and on a large number of gene expression
data sets. Both applications are simple and involve a similar amount of
computation as the original BH procedure. We compare the performance of our
proposed procedures with BH and other procedures and find that in most cases we
get more power for the same level of statistical significance.