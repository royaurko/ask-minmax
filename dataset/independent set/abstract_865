We consider the problem of learning a high-dimensional graphical model in
which certain hub nodes are highly-connected to many other nodes. Many contributors
have studied the use of an l1 penalty in order to learn a sparse graph in
high-dimensional setting. However, the l1 penalty implicitly assumes that each
edge is equally likely and independent of all other edges. We propose a general
framework to accommodate more realistic networks with hub nodes, using a convex
formulation that involves a row-column overlap norm penalty. We apply this
general framework to three widely-used probabilistic graphical models: the
Gaussian graphical model, the covariance graph model, and the binary Ising
model. An alternating direction method of multipliers algorithm is used to
solve the corresponding convex optimization problems. On synthetic data, we
demonstrate that our proposed framework outperforms competitors that do not
explicitly model hub nodes. We illustrate our proposal on a webpage data set
and a gene expression data set.