How does the human brain represent simple compositions of constituents:
actors, verbs, objects, directions, and locations? Subjects viewed videos
during neuroimaging (fMRI) sessions from which sentential descriptions of those
videos were identified by decoding the brain representations based only on
their fMRI activation patterns. Constituents (e.g., "fold" and "shirt") were
independently decoded from a single presentation. Independent constituent
classification was then compared to joint classification of aggregate concepts
(e.g., "fold-shirt"); results were similar as measured by accuracy and
correlation. The brain regions used for independent constituent classification
are largely disjoint and largely cover those used for joint classification.
This allows recovery of sentential descriptions of stimulus videos by composing
the results of the independent constituent classifiers. Furthermore,
classifiers trained on the words one set of subjects think of when watching a
video can recognise sentences a different subject thinks of when watching a
different video.