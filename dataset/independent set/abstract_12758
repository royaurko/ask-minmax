Stochastic variational inference makes it possible to approximate posterior
distributions induced by large datasets quickly using stochastic optimization.
The algorithm relies on the use of fully factorized variational distributions.
However, this "mean-field" independence approximation limits the fidelity of
the posterior approximation, and introduces local optima. We show how to relax
the mean-field approximation to allow arbitrary dependencies between global
parameters and local hidden variables, producing better parameter estimates by
reducing bias, sensitivity to local optima, and sensitivity to hyperparameters.