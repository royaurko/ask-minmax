The sumset and inverse sumset theories of Freiman, Pl\"{u}nnecke and Ruzsa,
give bounds connecting the cardinality of the sumset $A+B=\{a+b\;;\;a\in
A,\,b\in B\}$ of two discrete sets $A,B$, to the cardinalities (or the finer
structure) of the original sets $A,B$. For example, the sum-difference bound of
Ruzsa states that, $|A+B|\,|A|\,|B|\leq|A-B|^3$, where the difference set $A-B=
\{a-b\;;\;a\in A,\,b\in B\}$. Interpreting the differential entropy $h(X)$ of a
continuous random variable $X$ as (the logarithm of) the size of the effective
support of $X$, the main contribution of this paper is a series of natural
information-theoretic analogs for these results. For example, the Ruzsa
sum-difference bound becomes the new inequality, $h(X+Y)+h(X)+h(Y)\leq
3h(X-Y)$, for any pair of independent continuous random variables $X$ and $Y$.
Our results include differential-entropy versions of Ruzsa's triangle
inequality, the Pl\"{u}nnecke-Ruzsa inequality, and the
Balog-Szemer\'{e}di-Gowers lemma. Also we give a differential entropy version
of the Freiman-Green-Ruzsa inverse-sumset theorem, which can be seen as a
quantitative converse to the entropy power inequality. Versions of most of
these results for the discrete entropy $H(X)$ were recently proved by Tao,
relying heavily on a strong, functional form of the submodularity property of
$H(X)$. Since differential entropy is {\em not} functionally submodular, in the
continuous case many of the corresponding discrete proofs fail, in many cases
requiring substantially new proof strategies. We find that the basic property
that naturally replaces the discrete functional submodularity, is the data
processing property of mutual information.