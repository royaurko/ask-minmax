Lasso has been both theoretically and empirically proved a successful
variable selection approach. However, in the ultrahigh dimensional setting, the
conditions of model selection consistency for lasso could easily fail. The
independence screening framework tackles this problem by reducing the
dimensionality based on marginal correlations before performing lasso. In this
paper, we propose a two-step approach to relax the irrepresentable-type
condition (Wainwright, 2009) of lasso by using marginal information in a
different perspective from independence screening. In particular, we retain
significant variables rather than screening out irrelevant ones. The new method
is shown to be model selection consistent in the ultrahigh dimensional linear
regression model. To improve the finite sample performance, we then introduce a
three-step version and characterize its asymptotic behavior. Simulations and
real data analysis show advantages of our method over lasso and independence
screening in certain regimes.