In parametric estimation of covariance function of Gaussian processes, it is
often the case that the true covariance function does not belong to the
parametric set used for estimation. This situation is called the misspecified
case. In this case, it has been observed that, for irregular spatial sampling
of observation points, Cross Validation can yield smaller prediction errors
than Maximum Likelihood. Motivated by this comparison, we provide a general
asymptotic analysis of the misspecified case, for independent observation
points with uniform distribution. We prove that the Maximum Likelihood
estimator asymptotically minimizes a Kullback-Leibler divergence, within the
misspecified parametric set, while Cross Validation asymptotically minimizes
the integrated square prediction error. In a Monte Carlo simulation, we show
that the covariance parameters estimated by Maximum Likelihood and Cross
Validation, and the corresponding Kullback-Leibler divergences and integrated
square prediction errors, can be strongly contrasting. On a more technical
level, we provide new increasing-domain asymptotic results for the situation
where the eigenvalues of the covariance matrices involved are not upper
bounded.