We show that kernel-based quadrature rules for computing integrals are a
special case of random feature expansions for positive definite kernels for a
particular decomposition that always exists for such kernels. We provide a
theoretical analysis of the number of required samples for a given
approximation error, leading to both upper and lower bounds that are based
solely on the eigenvalues of the associated integral operator and match up to
logarithmic terms. In particular, we show that the upper bound may be obtained
from independent and identically distributed samples from a known non-uniform
distribution, while the lower bound if valid for any set of points. Applying
our results to kernel-based quadrature, while our results are fairly general,
we recover known upper and lower bounds for the special cases of Sobolev
spaces. Moreover, our results extend to the more general problem of full
function approximations (beyond simply computing an integral), with results in
L2- and L$\infty$-norm that match known results for special cases. Applying our
results to random features, we show an improvement of the number of random
features needed to preserve the generalization guarantees for learning with
Lipschitz-continuous losses.