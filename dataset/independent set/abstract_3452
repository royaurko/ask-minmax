Let $0 < \epsilon < 1/2$ be a noise parameter, and let $T_{\epsilon}$ be the
noise operator acting on functions on the boolean cube $\{0,1\}^n$. Let $f$ be
a nonnegative function on $\{0,1\}^n$. We upper bound the entropy of
$T_{\epsilon} f$ by the average entropy of conditional expectations of $f$,
given sets of roughly $(1-2\epsilon)^2 \cdot n$ variables. As an application,
we show that for a boolean function $f$, which is close to a characteristic
function $g$ of a subcube of dimension $n-1$, the entropy of $T_{\epsilon} f$
is at most that of $T_{\epsilon} g$. This, combined with a recent result of
Ordentlich, Shayevitz, and Weinstein shows that the "Most informative boolean
function" conjecture of Courtade and Kumar holds for balanced boolean functions
and high noise $\epsilon \ge 1/2 - \delta$, for some absolute constant $\delta
> 0$. Namely, if $X$ is uniformly distributed in $\{0,1\}^n$ and $Y$ is
obtained by flipping each coordinate of $X$ independently with probability
$\epsilon$, then, provided $\epsilon \ge 1/2 - \delta$, for any balanced
boolean function $f$ holds $I\Big(f(X);Y\Big) \le 1 - H(\epsilon)$.