The distributed computation of Nash equilibria is assuming growing relevance
in engineering where such problems emerge in the context of distributed
control. Accordingly, we consider static stochastic convex games complicated by
a parametric misspecification, a natural concern when competing in any
large-scale networked engineered system. In settings where the equilibrium
conditions are captured by a strongly monotone stochastic variational
inequality problem, we present two sets of distributedschemes in which agents
learn the equilibrium strategy and correct the misspecification by leveraging
noise-corrupted observations. (1) Monotone stochastic Nash games: We present a
set of coupled stochastic approximation schemes distributed across agents in
which the first scheme updates each agent's strategy via a projected
(stochastic) gradient step while the second scheme updates every agent's belief
regarding its misspecified parameter. We proceed to show that the produced
sequences converge to the true equilibrium strategy and the true parameter in
an almost sure sense. Surprisingly, convergence in the equilibrium strategy
achieves the optimal rate of convergence in a mean-squared sense with a
quantifiable degradation in the rate constant; (2) Stochastic Nash-Cournot
games with unobservable aggregate output: We refine (1) to a Cournot setting
where the tuple of strategies is not observable and assume that payoff
functions and strategy sets are public knowledge (a common knowledge
assumption). When noise-corrupted prices are observable, iterative fixed-point
schemes are developed, allowing for simultaneously learning the equilibrium
strategies and the parameter in an almost-sure sense.