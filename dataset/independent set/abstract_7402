Covariance and histogram image descriptors provide an effective way to
capture information about images. Both excel when used in combination with
special purpose distance metrics. For covariance descriptors these metrics
measure the distance along the non-Euclidean Riemannian manifold of symmetric
positive definite matrices. For histogram descriptors the Earth Mover's
distance measures the optimal transport between two histograms. Although more
precise, these distance metrics are very expensive to compute, making them
impractical in many applications, even for data sets of only a few thousand
examples. In this paper we present two methods to compress the size of
covariance and histogram datasets with only marginal increases in test error
for k-nearest neighbor classification. Specifically, we show that we can reduce
data sets to 16% and in some cases as little as 2% of their original size,
while approximately matching the test error of kNN classification on the full
training set. In fact, because the compressed set is learned in a supervised
fashion, it sometimes even outperforms the full data set, while requiring only
a fraction of the space and drastically reducing test-time computation.