To fully characterize the information that two `source' variables carry about
a third `target' variable, one must decompose the total information into
redundant, unique and synergistic components, i.e. obtain a partial information
decomposition (PID). However Shannon's theory of information does not provide
formulae to fully determine these quantities. Several recent studies have begun
addressing this. Some possible definitions for PID quantities have been
proposed, and some analyses have been carried out on systems composed of
discrete variables. Here we present the first in-depth analysis of PIDs on
Gaussian systems, both static and dynamical. We show that, for a broad class of
Gaussian systems, previously proposed PID formulae imply that: (i) redundancy
reduces to the minimum information provided by either source variable, and
hence is independent of correlation between sources; (ii) synergy is the extra
information contributed by the weaker source when the stronger source is known,
and can either increase or decrease with correlation between sources. We find
that Gaussian systems frequently exhibit net synergy, i.e. the information
carried jointly by both sources is greater than the sum of informations carried
by each source individually. Drawing from several explicit examples, we discuss
the implications of these findings for measures of information transfer and
information-based measures of complexity, both generally and within a
neuroscience setting. Importantly, by providing independent formulae for
synergy and redundancy applicable to continuous time-series data, we open up a
new approach to characterizing and quantifying information sharing amongst
complex system variables.