We focus on the supervised binary classification problem, which consists in
guessing the label $Y$ associated to a co-variate $X \in \R^d$, given a set of
$n$ independent and identically distributed co-variates and associated labels
$(X_i,Y_i)$. We assume that the law of the random vector $(X,Y)$ is unknown and
the marginal law of $X$ admits a density supported on a set $\A$. In the
particular case of plug-in classifiers, solving the classification problem
boils down to the estimation of the regression function $\eta(X) = \Exp[Y|X]$.
Assuming first $\A$ to be known, we show how it is possible to construct an
estimator of $\eta$ by localized projections onto a multi-resolution analysis
(MRA). In a second step, we show how this estimation procedure generalizes to
the case where $\A$ is unknown. Interestingly, this novel estimation procedure
presents similar theoretical performances as the celebrated local-polynomial
estimator (LPE). In addition, it benefits from the lattice structure of the
underlying MRA and thus outperforms the LPE from a computational standpoint,
which turns out to be a crucial feature in many practical applications.
Finally, we prove that the associated plug-in classifier can reach super-fast
rates under a margin assumption.