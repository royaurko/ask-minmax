We provide a condition under which a version of Shannon's Entropy Power
Inequality will hold for dependent variables. We provide information
inequalities extending those found in the independent case.