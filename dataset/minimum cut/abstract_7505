How many words (and which ones) are sufficient to define all other words?
When dictionaries are analyzed as directed graphs with links from defining
words to defined words, they turn out to have latent structure that has not
previously been noticed. Recursively removing all those words that are
reachable by definition but do not define any further words reduces the
dictionary to a Kernel of 10%, but this is still not the smallest number of
words that can define all the rest. About 75% of the Kernel is its Core, a
strongly connected subset (with a definitional path to and from any word and
any other word), but the Core cannot define all the rest. The 25% surrounding
the Core are Satellites, small strongly connected subsets. The size of the
smallest set of words that can define all the rest (a graph's "minimum feedback
vertex set" or MinSet) is about 1% of the dictionary, about 15% of the Kernel,
about half-Core and half-Satellite, but every dictionary has a huge number of
MinSets. The words in the Core turn out to be learned earlier, more frequent,
and less concrete than the Satellites, which are learned earlier and more
frequent but more concrete than the rest of the Dictionary. The findings are
related to the symbol grounding problem and the mental lexicon.