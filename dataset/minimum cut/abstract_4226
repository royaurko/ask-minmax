Software is a communication system. The usual topic of communication is
program behavior, as encoded by programs. Domain-specific libraries are
codebooks, domain-specific languages are coding schemes, and so forth. To turn
metaphor into method, we adapt toolsfrom information theory--the study of
efficient communication--to probe the efficiency with which languages and
libraries let us communicate programs. In previous work we developed an
information-theoretic analysis of software reuse in problem domains. This new
paper uses information theory to analyze tradeoffs in the design of components,
generators, and metalanguages. We seek answers to two questions: (1) How can we
judge whether a component is over- or under-generalized? Drawing on minimum
description length principles, we propose that the best component yields the
most succinct representation of the use cases. (2) If we view a programming
language as an assemblage of metalanguages, each providing a complementary
style of abstraction, how can these metalanguages aid or hinder us in
efficiently describing software? We describe a complex triangle of interactions
between the power of an abstraction mechanism, the amount of reuse it enables,
and the cognitive difficulty of its use.