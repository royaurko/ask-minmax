We continue the development, started in of the asymptotic description of
certain stochastic neural networks. We use the Large Deviation Principle (LDP)
and the good rate function H announced there to prove that H has a unique
minimum mu_e, a stationary measure on the set of trajectories. We characterize
this measure by its two marginals, at time 0, and from time 1 to T. The second
marginal is a stationary Gaussian measure. With an eye on applications, we show
that its mean and covariance operator can be inductively computed. Finally we
use the LDP to establish various convergence results, averaged and quenched.