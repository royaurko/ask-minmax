We focus on the central problem of discriminating between metallic and
insulating behaviour in amorphous alloys formed between a semiconductor and a
metal. For this, the logarithmic temperature derivative of the conductivity, w
= d ln sigma / d ln T, has proved over recent years to be very helpful in
determining the critical value x_c of the metal content x for the
metal-insulator transition (MIT). We show that, for various amorphous alloys,
recent experimental results on w(T,x) are qualitatively inconsistent with the
usual assumptions of continuity of the MIT at T = 0 and of sigma(T,x_c) being
proportional to a power of T. These results suggest that w(T,x_c) tends to 0 as
T -> 0, in which case the MIT should be discontinuous at T = 0 (but only
there), in agreement with Mott's hypothesis of a finite minimum metallic
conductivity.