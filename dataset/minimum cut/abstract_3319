Although the broad outlines of the appropriate pipeline for cosmological
likelihood analysis with CMB data has been known for several years, only
recently have we had to contend with the full, large-scale, computationally
challenging problem involving both highly-correlated noise and extremely large
datasets ($N > 1000$). In this talk we concentrate on the beginning and end of
this process. First, we discuss estimating the noise covariance from the data
itself in a rigorous and unbiased way; this is essentially an iterated
minimum-variance mapmaking approach. We also discuss the unbiased determination
of cosmological parameters from estimates of the power spectrum or experimental
bandpowers.