The application of cognitive mechanisms to support knowledge acquisition is,
from our point of view, crucial for making the resulting models coherent,
efficient, credible, easy to use and understandable. In particular, there are
two characteristic features of intelligence that are essential for knowledge
development: forgetting and consolidation. Both plays an important role in
knowledge bases and learning systems to avoid possible information overflow and
redundancy, and in order to preserve and strengthen important or frequently
used rules and remove (or forget) useless ones. We present an incremental,
long-life view of knowledge acquisition which tries to improve task after task
by determining what to keep, what to consolidate and what to forget, overcoming
The Stability-Plasticity dilemma. In order to do that, we rate rules by
introducing several metrics through the first adaptation, to our knowledge, of
the Minimum Message Length (MML) principle to a coverage graph, a hierarchical
assessment structure which treats evidence and rules in a unified way. The
metrics are not only used to forget some of the worst rules, but also to set a
consolidation process to promote those selected rules to the knowledge base,
which is also mirrored by a demotion system. We evaluate the framework with a
series of tasks in a chess rule learning domain.