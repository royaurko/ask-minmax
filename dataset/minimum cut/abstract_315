Several proofs of the monotonicity of the non-Gaussianness (divergence with
respect to a Gaussian random variable with identical second order statistics)
of the sum of n independent and identically distributed (i.i.d.) random
variables were published. We give an upper bound on the decrease rate of the
non-Gaussianness which is proportional to the inverse of n, for large n. The
proof is based on the relationship between non-Gaussianness and minimum
mean-square error (MMSE) and causal minimum mean-square error (CMMSE) in the
time-continuous Gaussian channel.