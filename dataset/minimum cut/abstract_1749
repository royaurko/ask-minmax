In this paper we experiment with a 2-player strategy board game where playing
models are evolved using reinforcement learning and neural networks. The models
are evolved to speed up automatic game development based on human involvement
at varying levels of sophistication and density when compared to fully
autonomous playing. The experimental results suggest a clear and measurable
association between the ability to win games and the ability to do that fast,
while at the same time demonstrating that there is a minimum level of human
involvement beyond which no learning really occurs.