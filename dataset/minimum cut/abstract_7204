The introduced entropy functional's (EF) information measure of random
process integrates multiple information contributions along the process
trajectories, evaluating both the states' and between states' bound information
connections. This measure reveals information that is hidden by traditional
information measures, which commonly use Shannon's entropy function for each
selected stationary states of the process. The hidden information is important
for evaluation of missing connections, disclosing the process' meaningful
information, which enables producing logic of the information. The presentation
consists of three Parts. In Part 1R-revised we analyze mechanism of arising
information regularities from a stochastic process, measured by EF,
independently of the process' specific source and origin. Uncovering the
process' regularities leads us to an information law, based on extracting
maximal information from its minimum, which could create these regularities.
The solved variation problem (VP) determines a dynamic process, measured by
information path functional (IPF), and information dynamic model, approximating
the EF measured stochastic process with a maximal functional probability on
trajectories. In Part 2, we study the cooperative processes, arising at the
consolidation, as a result of the VP-EF-IPF approach, which is able to produce
multiple cooperative structures, concurrently assembling in hierarchical
information network (IN) and generating the IN's digital genetic code. In Part
3 we study the evolutionary information processes and regularities of evolution
dynamics, evaluated by the entropy functional (EF) of random field and
informational path functional of a dynamic space-time process. The information
law and the regularities determine unified functional informational mechanisms
of evolution dynamics.