In testing of hypothesis the robustness of the tests is an important concern.
Generally, the maximum likelihood based tests are most efficient under standard
regularity conditions, but they are highly non-robust even under small
deviations from the assumed conditions. In this paper we have proposed
generalized Wald-type tests based on minimum density power divergence
estimators for parametric hypotheses. This method avoids the use of
nonparametric density estimation and the bandwidth selection. The trade-off
between efficiency and robustness is controlled by a tuning parameter $\beta$.
The asymptotic distributions of the test statistics are chi-square with
appropriate degrees of freedom. The performance of the proposed tests are
explored through simulations and real data analysis.