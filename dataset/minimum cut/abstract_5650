We consider the problem of inferring the conditional independence graph (CIG)
of a multivariate stationary dicrete-time Gaussian random process based on a
finite length observation. Using information-theoretic methods, we derive a
lower bound on the error probability of any learning scheme for the underlying
process CIG. This bound, in turn, yields a minimum required sample-size which
is necessary for any algorithm regardless of its computational complexity, to
reliably select the true underlying CIG. Furthermore, by analysis of a simple
selection scheme, we show that the information-theoretic limits can be achieved
for a subclass of processes having sparse CIG. We do not assume a parametric
model for the observed process, but require it to have a sufficiently smooth
spectral density matrix (SDM).