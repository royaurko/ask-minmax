This paper develops a methodology for robust Bayesian inference through the
use of disparities. Metrics such as Hellinger distance and negative exponential
disparity have a long history in robust estimation in frequentist inference. We
demonstrate that an equivalent robustification may be made in Bayesian
inference by substituting an appropriately scaled disparity for the log
likelihood to which standard Monte Carlo Markov Chain methods may be applied. A
particularly appealing property of minimum-disparity methods is that while they
yield robustness with a breakdown point of 1/2, the resulting parameter
estimates are also efficient when the posited probabilistic model is correct.
We demonstrate that a similar property holds for disparity-based Bayesian
inference. We further show that in the Bayesian setting, it is also possible to
extend these methods to robustify regression models, random effects
distributions and other hierarchical models. The methods are demonstrated on
real world data.