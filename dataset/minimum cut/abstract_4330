The asymptotic behaviour of a family of gradient algorithms (including the
methods of steepest descent and minimum residues) for the optimisation of
bounded quadratic operators in R^d and Hilbert spaces is analyzed. The results
obtained generalize those of Akaike (1959) in several directions. First, all
algorithms in the family are shown to have the same asymptotic behaviour
(convergence to a two-point attractor), which implies in particular that they
have similar asymptotic convergence rates. Second, the analysis also covers the
Hilbert space case. A detailed analysis of the stability property of the
attractor is provided.