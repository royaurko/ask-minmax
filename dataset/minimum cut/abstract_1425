The nonparametric problem of detecting existence of an anomalous interval
over a one dimensional line network is studied. Nodes corresponding to an
anomalous interval (if exists) receive samples generated by a distribution q,
which is different from the distribution p that generates samples for other
nodes. If anomalous interval does not exist, then all nodes receive samples
generated by p. It is assumed that the distributions p and q are arbitrary, and
are unknown. In order to detect whether an anomalous interval exists, a test is
built based on mean embeddings of distributions into a reproducing kernel
Hilbert space (RKHS) and the metric of maximummean discrepancy (MMD). It is
shown that as the network size n goes to infinity, if the minimum length of
candidate anomalous intervals is larger than a threshold which has the order
O(log n), the proposed test is asymptotically successful, i.e., the probability
of detection error approaches zero asymptotically. An efficient algorithm to
perform the test with substantial computational complexity reduction is
proposed, and is shown to be asymptotically successful if the condition on the
minimum length of candidate anomalous interval is satisfied. Numerical results
are provided, which are consistent with the theoretical results.