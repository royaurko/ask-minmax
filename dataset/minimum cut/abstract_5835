Information divergence functions play a critical role in statistics and
information theory. In this paper we show that a non-parametric f-divergence
measure can be used to provide improved bounds on the minimum binary
classification probability of error for the case when the training and test
data are drawn from the same distribution and for the case where there exists
some mismatch between training and test distributions. We confirm the
theoretical results by designing feature selection algorithms using the
criteria from these bounds and by evaluating the algorithms on a series of
pathological speech classification tasks.