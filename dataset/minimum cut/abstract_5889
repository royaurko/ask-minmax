We introduce and develop a novel approach to outlier detection based on
adaptation of random subspace learning. Our proposed method handles both
high-dimension low-sample size and traditional low-dimensional high-sample size
datasets. Essentially, we avoid the computational bottleneck of techniques like
minimum covariance determinant (MCD) by computing the needed determinants and
associated measures in much lower dimensional subspaces. Both theoretical and
computational development of our approach reveal that it is computationally
more efficient than the regularized methods in high-dimensional low-sample
size, and often competes favorably with existing methods as far as the
percentage of correct outlier detection is concerned.