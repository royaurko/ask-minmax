A simple characterization of uniformly minimum variance unbiased estimators
(UMVUEs) is provided (in the case when the sample space is finite) in terms of
a linear independence condition on the likelihood functions corresponding to
the possible samples. The crucial observation in the proof is that, if a UMVUE
exists, then, after an appropriate cleaning of the parameter space, the nonzero
likelihood functions are eigenvectors of an "artificial" matrix of Lagrange
multipliers, and the values of the UMVUE are eigenvalues of that matrix. The
characterization is then extended to best unbiased estimators with respect to
arbitrary convex loss functions.