Mean-field models are a popular tool in a variety of fields. They provide an
understanding of the impact of interactions among a large number of particles
or people or other "self-interested agents", and are an increasingly popular
tool in distributed control.
  This paper considers a particular randomized distributed control architecture
introduced in our own recent work. In numerical results it was found that the
associated mean-field model had attractive properties for purposes of control.
In particular, when viewed as an input-output system, its linearization was
found to be minimum phase.
  In this paper we take a closer look at the control model. The results are
summarized as follows:
  (i) The Markov Decision Process framework of Todorov is extended to
continuous time models, in which the "control cost" is based on relative
entropy. This is the basis of the construction of a family of controlled
Markovian generators.
  (ii) A decentralized control architecture is proposed in which each agent
evolves as a controlled Markov process. A central contributority broadcasts a common
control signal to each agent. The central contributority chooses this signal based
on an aggregate scalar output of the Markovian agents.
  (iii) Provided the control-free system is a reversible Markov process, the
following identity holds for the linearization, \[ \text{Real} (G(j\omega)) =
\text{PSD}_Y(\omega)\ge 0, \quad \omega\in\Re, \] where the right hand side
denotes the power spectral density for the output of any one of the individual
(control-free) Markov processes.