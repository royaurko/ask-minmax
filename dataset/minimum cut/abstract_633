We investigate a robust penalized logistic regression algorithm based on a
minimum distance criterion. Influential outliers are often associated with the
explosion of parameter vector estimates, but in the context of standard
logistic regression, the bias due to outliers always causes the parameter
vector to implode, that is shrink towards the zero vector. Thus, using
LASSO-like penalties to perform variable selection in the presence of outliers
can result in missed detections of relevant covariates. We show that by
choosing a minimum distance criterion together with an Elastic Net penalty, we
can simultaneously find a parsimonious model and avoid estimation implosion
even in the presence of many outliers in the important small $n$ large $p$
situation. Implementation using an MM algorithm is described and performance
evaluated.