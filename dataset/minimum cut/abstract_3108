Cosmological simulations make use of sub-grid recipes for the implementation
of galactic winds driven by massive stars because direct injection of supernova
energy in thermal form leads to strong radiative losses, rendering the feedback
inefficient. We argue that the main cause of the catastrophic cooling is a
mismatch between the mass of the gas in which the energy is injected and the
mass of the parent stellar population. Because too much mass is heated, the
temperatures are too low and the cooling times too short. We use analytic
arguments to estimate, as a function of the gas density and the numerical
resolution, the minimum heating temperature that is required for the injected
thermal energy to be efficiently converted into kinetic energy. We then propose
and test a stochastic implementation of thermal feedback that uses this minimum
temperature increase as an input parameter and that can be employed in both
particle- and grid-based codes. We use smoothed particle hydrodynamics
simulations to test the method on models of isolated disc galaxies in dark
matter haloes with total mass 10^10 and 10^12 h^-1 solar masses. The thermal
feedback strongly suppresses the star formation rate and can drive massive,
large-scale outflows without the need to turn off radiative cooling
temporarily. In accord with expectations derived from analytic arguments, for
sufficiently high resolution the results become insensitive to the imposed
temperature jump and also agree with high-resolution simulations employing
kinetic feedback.