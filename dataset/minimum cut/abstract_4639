In this paper, source coding or data compression is viewed as a measurement
problem. Given a measurement device with fewer states than the observable of a
stochastic source, how can one capture the essential information? We propose
modeling stochastic sources as piecewise linear discrete chaotic dynamical
systems known as Generalized Lur\"{o}th Series (GLS) which dates back to Georg
Cantor's work in 1869. The Lyapunov exponent of GLS is equal to the Shannon's
entropy of the source (up to a constant of proportionality). By successively
approximating the source with GLS having fewer states (with the closest
Lyapunov exponent), we derive a binary coding algorithm which exhibits minimum
redundancy (the least average codeword length with integer codeword lengths).
This turns out to be a re-discovery of Huffman coding, the popular lossless
compression algorithm used in the JPEG international standard for still image
compression.