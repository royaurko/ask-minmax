We introduce a novel parametric family of symmetric information-theoretic
distances based on Jensen's inequality for a convex functional generator. In
particular, this family unifies the celebrated Jeffreys divergence with the
Jensen-Shannon divergence when the Shannon entropy generator is chosen. We then
design a generic algorithm to compute the unique centroid defined as the
minimum average divergence. This yields a smooth family of centroids linking
the Jeffreys to the Jensen-Shannon centroid. Finally, we report on our
experimental results.