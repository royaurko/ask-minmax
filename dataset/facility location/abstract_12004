Knowing where people look is a useful tool in many various image and video
applications. However, traditional gaze tracking hardware is expensive and
requires local study participants, so acquiring gaze location data from a large
number of participants is very problematic. In this work we propose a
crowdsourced method for acquisition of gaze direction data from a virtually
unlimited number of participants, using a robust self-reporting mechanism (see
Figure 1). Our system collects temporally sparse but spatially dense
points-of-attention in any visual information. We apply our approach to an
existing video data set and demonstrate that we obtain results similar to
traditional gaze tracking. We also explore the parameter ranges of our method,
and collect gaze tracking data for a large set of YouTube videos.