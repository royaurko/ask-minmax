We propose a model where a supernova explodes in some vicinity of our solar
system (some tens of parsecs) in the recent past (some tens of thousands years)
with the energy release in cosmic rays of order of $ 10 ^ {51} $ erg. The flux
from this supernova is added to an isotropic flux from other sources. We
consider the case where the Sun's location is not in some typical for Our
Galaxy average environment, but in the Local Superbubble about 100 pc across,
in which the diffusion coefficient $D (E) = D_0 \times E ^ {0.6} $, with the
value of $ D_0 \sim 10 ^ {25} cm^ 2 s^ {-1} $. We describe the energy
dependence of the anisotropy of cosmic rays in the TeV region, together with
the observed features of the energy spectrum of protons found in direct
measurements. Our model provides a natural explanation to the hardening of the
proton spectrum at 200 GeV, together with the observed steepening of the
spectrum above 50 TeV.