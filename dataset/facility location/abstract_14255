Automated estimation of the allocation of a driver's visual attention may be
a critical component of future Advanced Driver Assistance Systems. In theory,
vision-based tracking of the eye can provide a good estimate of gaze location.
In practice, eye tracking from video is challenging because of sunglasses,
eyeglass reflections, lighting conditions, occlusions, motion blur, and other
factors. Estimation of head pose, on the other hand, is robust to many of these
effects, but cannot provide as fine-grained of a resolution in localizing the
gaze. However, for the purpose of keeping the driver safe, it is sufficient to
partition gaze into regions. In this effort, we propose a system that extracts
facial features and classifies their spatial configuration into six regions in
real-time. Our proposed method achieves an average accuracy of 91.4% at an
average decision rate of 11 Hz on a dataset of 50 drivers from an on-road
study.