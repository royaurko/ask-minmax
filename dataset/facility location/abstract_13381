Good robust estimators can be tuned to combine a high breakdown point and a
specified asymptotic efficiency at a central model. This happens in regression
with MM- and tau-estimators among others. However, the finite-sample efficiency
of these estimators can be much lower than the asymptotic one. To overcome this
drawback, an approach is proposed for parametric models, which is based on a
distance between parameters. Given a robust estimator, the proposed one is
obtained by maximizing the likelihood under the constraint that the distance is
less than a given threshold. For the linear model with normal errors and using
the MM estimator and the distance induced by the Kullback-Leibler divergence,
simulations show that the proposed estimator attains a finite-sample efficiency
close to one, while its maximum mean squared error under pointwise outlier
contamination is smaller than that of the MM estimator. The same approach also
shows good results in the estimation of multivariate location and scatter.