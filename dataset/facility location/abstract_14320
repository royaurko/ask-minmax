We develop the information-theoretical concepts required to study the
statistical dependencies among three variables. Some of such dependencies are
pure triple interactions, in the sense that they cannot be explained in terms
of a combination of pairwise correlations. We derive bounds for triple
dependencies, and characterize the shape of the joint probability distribution
of three binary variables with high triple interaction. The analysis also
allows us to quantify the amount of redundancy in the mutual information
between pairs of variables, and to assess whether the information between two
variables is or is not mediated by a third variable. These concepts are applied
to the analysis of written texts. We find that the probability that a given
word is found in a particular location within the text is not only modulated by
the presence or absence of other nearby words, but also, on the presence or
absence of nearby pairs of words. We identify the words enclosing the key
semantic concepts of the text, the triplets of words with high pairwise and
triple interactions, and the words that mediate the pairwise interactions
between other words.