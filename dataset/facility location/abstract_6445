We propose an algorithm for simultaneously detecting and locating
changepoints in a time series, and a framework for predicting the distribution
of the next point in the series. The kernel of the algorithm is a system of
equations that computes, for each index i, the probability that the last (most
recent) change point occurred at i. We evaluate this algorithm by applying it
to the change point detection problem and comparing it to the generalized
likelihood ratio (GLR) algorithm. We find that our algorithm is as good as GLR,
or better, over a wide range of scenarios, and that the advantage increases as
the signal-to-noise ratio decreases.