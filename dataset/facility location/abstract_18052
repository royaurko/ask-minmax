Egocentric images offer a hands-free way to record daily experiences and
special events, where social interactions are of special interest. A natural
question that arises is how to extract and track the appearance of multiple
persons in a social event captured by a wearable camera. In this paper, we
propose a novel method to find correspondences of multiple-faces in low
temporal resolution egocentric sequences acquired through a wearable camera.
This kind of sequences imposes additional challenges to the multitracking
problem with respect to conventional videos. Due to the free motion of the
camera and to its low temporal resolution (2 fpm), abrupt changes in the field
of view, in illumination conditions and in the target location are very
frequent. To overcome such a difficulty, we propose to generate, for each
detected face, a set of correspondences along the whole sequence that we call
tracklet and to take advantage of their redundancy to deal with both false
positive face detections and unreliable tracklets. Similar tracklets are
grouped into the so called extended bag-of-tracklets (eBoT), which are aimed to
correspond to specific persons. Finally, a prototype tracklet is extracted for
each eBoT. We validated our method over a dataset of 18.000 images from 38
egocentric sequences with 52 trackable persons and compared to the
state-of-the-art methods, demonstrating its effectiveness and robustness.