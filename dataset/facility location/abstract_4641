Given a random sample from a distribution with density function that depends
on an unknown parameter $\theta$, we are interested in accurately estimating
the true parametric density function at a future observation from the same
distribution. The asymptotic risk of Bayes predictive density estimates with
Kullback--Leibler loss function $D(f_{\theta}||{\hat{f}})=\int{f_{\theta}
\log{(f_{\theta}/ hat{f})}}$ is used to examine various ways of choosing prior
distributions; the principal type of choice studied is minimax. We seek
asymptotically least favorable predictive densities for which the corresponding
asymptotic risk is minimax. A result resembling Stein's paradox for estimating
normal means by the maximum likelihood holds for the uniform prior in the
multivariate location family case: when the dimensionality of the model is at
least three, the Jeffreys prior is minimax, though inadmissible. The Jeffreys
prior is both admissible and minimax for one- and two-dimensional location
problems.