We describe a methodology for designing efficient parallel and distributed
scientific software. This methodology utilizes sequences of mechanizable
algebra--based optimizing transformations. In this study, we apply our
methodology to the FFT, starting from a high--level algebraic algorithm
description. Abstract multiprocessor plans are developed and refined to specify
which computations are to be done by each processor. Templates are then created
that specify the locations of computations and data on the processors, as well
as data flow among processors. Templates are developed in both the MPI and
OpenMP programming styles.
  Preliminary experiments comparing code constructed using our methodology with
code from several standard scientific libraries show that our code is often
competitive and sometimes performs better. Interestingly, our code handled a
larger range of problem sizes on one target architecture.