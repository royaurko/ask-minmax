The use of High Performance Computing (HPC) in commercial and consumer IT
applications is becoming popular. They need the ability to gain rapid and
scalable access to high-end computing capabilities. Cloud computing promises to
deliver such a computing infrastructure using data centers so that HPC users
can access applications and data from a Cloud anywhere in the world on demand
and pay based on what they use. However, the growing demand drastically
increases the energy consumption of data centers, which has become a critical
issue. High energy consumption not only translates to high energy cost, which
will reduce the profit margin of Cloud providers, but also high carbon
emissions which is not environmentally sustainable. Hence, energy-efficient
solutions are required that can address the high increase in the energy
consumption from the perspective of not only Cloud provider but also from the
environment. To address this issue we propose near-optimal scheduling policies
that exploits heterogeneity across multiple data centers for a Cloud provider.
We consider a number of energy efficiency factors such as energy cost, carbon
emission rate, workload, and CPU power efficiency which changes across
different data center depending on their location, architectural design, and
management system. Our carbon/energy based scheduling policies are able to
achieve on average up to 30% of energy savings in comparison to profit based
scheduling policies leading to higher profit and less carbon emissions.