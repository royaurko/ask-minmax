We consider the problem of detecting multiple changepoints in large data
sets. Our focus is on applications where the number of changepoints will
increase as we collect more data: for example in genetics as we analyse larger
regions of the genome, or in finance as we observe time-series over longer
periods. We consider the common approach of detecting changepoints through
minimising a cost function over possible numbers and locations of changepoints.
This includes several established procedures for detecting changing points,
such as penalised likelihood and minimum description length. We introduce a new
method for finding the minimum of such cost functions and hence the optimal
number and location of changepoints that has a computational cost which, under
mild conditions, is linear in the number of observations. This compares
favourably with existing methods for the same problem whose computational cost
can be quadratic or even cubic. In simulation studies we show that our new
method can be orders of magnitude faster than these alternative exact methods.
We also compare with the Binary Segmentation algorithm for identifying
changepoints, showing that the exactness of our approach can lead to
substantial improvements in the accuracy of the inferred segmentation of the
data.