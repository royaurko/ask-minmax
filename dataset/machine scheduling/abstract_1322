In supervised learning, the redundancy contained in random examples can be
avoided by learning from queries. Using statistical mechanics, we study
learning from minimum entropy queries in a large tree-committee machine. The
generalization error decreases exponentially with the number of training
examples, providing a significant improvement over the algebraic decay for
random examples. The connection between entropy and generalization error in
multi-layer networks is discussed, and a computationally cheap algorithm for
constructing queries is suggested and analysed.