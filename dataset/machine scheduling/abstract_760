Most input buffered packet switches internally segment variable-length
packets into fixed-length cells. The last cell in a segmented packet will
contain overhead bytes if the packet length is not evenly divisible by the cell
length. Switch speed-up is used to compensate for this overhead. In this paper,
we develop an analytical model of a single-server queue where an input stream
of packets is segmented into cells for service. Analytical models are developed
for M/M/1, M/H2/1, and M/E2/1 queues with a discretized (or quantized) service
time. These models and simulation using real packet traces are used to evaluate
the effect of speed-up on mean queue length. We propose and evaluate a new
method of segmenting a packet trailer and subsequent packet header into a
single cell. This cell merging method reduces the required speed-up. No changes
to switch-matrix scheduling algorithms are needed. Simulation with a packet
trace shows a reduction in the needed speed-up for an iSLIP scheduled input
buffered switch.