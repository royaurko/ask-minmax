In this paper we borrow concepts from Information Theory and Statistical
Mechanics to perform a pattern recognition procedure on a set of x-ray hazelnut
images. We identify two relevant statistical scales, whose ratio affects the
performance of a machine learning algorithm based on statistical observables,
and discuss the dependence of such scales on the image resolution. Finally, by
averaging the performance of a Support Vector Machines algorithm over a set of
training samples, we numerically verify the predicted onset of an optimal scale
of resolution, at which the pattern recognition is favoured.