In logic programming, dynamic scheduling refers to a situation where the
selection of the atom in each resolution (computation) step is determined at
runtime, as opposed to a fixed selection rule such as the left-to-right one of
Prolog. This has applications e.g. in parallel programming. A mechanism to
control dynamic scheduling is provided in existing languages in the form of
delay declarations.
  Input-consuming derivations were introduced to describe dynamic scheduling
while abstracting from the technical details. In this paper, we first formalise
the relationship between delay declarations and input-consuming derivations,
showing in many cases a one-to-one correspondence. Then, we define a
model-theoretic semantics for input-consuming derivations of simply-moded
programs. Finally, for this class of programs, we provide a necessary and
sufficient criterion for termination.