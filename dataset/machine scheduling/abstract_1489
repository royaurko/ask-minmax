From the Bayesian perspective, the category of conditional probabilities (a
variant of the Kleisli category of the Giry monad, whose objects are measurable
spaces and arrows are Markov kernels) gives a nice framework for
conceptualization and analysis of many aspects of machine learning. Using
categorical methods, we construct models for parametric and nonparametric
Bayesian reasoning on function spaces, thus providing a basis for the
supervised learning problem. In particular, stochastic processes are arrows to
these function spaces which serve as prior probabilities. The resulting
inference maps can often be analytically constructed in this symmetric monoidal
weakly closed category. We also show how to view general stochastic processes
using functor categories and demonstrate the Kalman filter as an archetype for
the hidden Markov model.