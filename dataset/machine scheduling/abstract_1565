We are working to develop automated intelligent agents, which can act and
react as learning machines with minimal human intervention. To accomplish this,
an intelligent agent is viewed as a question-asking machine, which is designed
by coupling the processes of inference and inquiry to form a model-based
learning unit. In order to select maximally-informative queries, the
intelligent agent needs to be able to compute the relevance of a question. This
is accomplished by employing the inquiry calculus, which is dual to the
probability calculus, and extends information theory by explicitly requiring
context. Here, we consider the interaction between two question-asking
intelligent agents, and note that there is a potential information redundancy
with respect to the two questions that the agents may choose to pose. We show
that the information redundancy is minimized by maximizing the joint entropy of
the questions, which simultaneously maximizes the relevance of each question
while minimizing the mutual information between them. Maximum joint entropy is
therefore an important principle of information-based collaboration, which
enables intelligent agents to efficiently learn together.