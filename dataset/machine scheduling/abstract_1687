We introduce a principal support vector machine (PSVM) approach that can be
used for both linear and nonlinear sufficient dimension reduction. The basic
idea is to divide the response variables into slices and use a modified form of
support vector machine to find the optimal hyperplanes that separate them.
These optimal hyperplanes are then aligned by the principal components of their
normal vectors. It is proved that the aligned normal vectors provide an
unbiased, $\sqrt{n}$-consistent, and asymptotically normal estimator of the
sufficient dimension reduction space. The method is then generalized to
nonlinear sufficient dimension reduction using the reproducing kernel Hilbert
space. In that context, the aligned normal vectors become functions and it is
proved that they are unbiased in the sense that they are functions of the true
nonlinear sufficient predictors. We compare PSVM with other sufficient
dimension reduction methods by simulation and in real data analysis, and
through both comparisons firmly establish its practical advantages.