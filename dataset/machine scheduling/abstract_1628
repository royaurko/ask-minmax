In the framework of on-line learning, a learning machine might move around a
teacher due to the differences in structures or output functions between the
teacher and the learning machine. In this paper we analyze the generalization
performance of a new student supervised by a moving machine. A model composed
of a fixed true teacher, a moving teacher, and a student is treated
theoretically using statistical mechanics, where the true teacher is a
nonmonotonic perceptron and the others are simple perceptrons. Calculating the
generalization errors numerically, we show that the generalization errors of a
student can temporarily become smaller than that of a moving teacher, even if
the student only uses examples from the moving teacher. However, the
generalization error of the student eventually becomes the same value with that
of the moving teacher. This behavior is qualitatively different from that of a
linear model.