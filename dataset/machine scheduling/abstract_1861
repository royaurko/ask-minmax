Tackling problems in areas such as computer vision, bioinformatics, speech or
text recognition is often done best by taking into account task-specific
statistical relations between output variables. In structured prediction, this
internal structure is levered to predict multiple outputs simultaneously,
leading to more accurate and coherent predictions. Structural support vector
machines (SSVMs) are nonprobabilistic models that optimize a joint input-output
function through margin-based learning. Because SSVMs generally disregard the
interplay between unary and interaction factors during the training phase,
final parameters are suboptimal. Moreover, its factors are often restricted to
linear combinations of input features, limiting its generalization power. To
improve prediction accuracy, this paper proposes: (i) Joint inference and
learning by integration of back-propagation and loss-augmented inference in
SSVM subgradient descent; (ii) Extending SSVM factors to neural networks that
form highly nonlinear functions of input features. Image segmentation benchmark
results demonstrate improvements over conventional SSVM training methods in
terms of accuracy, highlighting the feasibility of end-to-end SSVM training
with neural factors.