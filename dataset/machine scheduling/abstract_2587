We derive the Baum-Welch algorithm for hidden Markov models (HMMs) through an
information-theoretical approach using cross-entropy instead of the Lagrange
multiplier approach which is universal in machine learning literature. The
proposed approach provides a more concise derivation of the Baum-Welch method
and naturally generalizes to multiple observations.