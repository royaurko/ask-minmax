We investigate the issue of model selection and the use of the nonconformity
(strangeness) measure in batch learning. Using the nonconformity measure we
propose a new training algorithm that helps avoid the need for Cross-Validation
or Leave-One-Out model selection strategies. We provide a new generalisation
error bound using the notion of nonconformity to upper bound the loss of each
test example and show that our proposed approach is comparable to standard
model selection methods, but with theoretical guarantees of success and faster
convergence. We demonstrate our novel model selection technique using the
Support Vector Machine.