The choice titration procedure presents a subject with a repeated choice
between a standard option that always provides the same reward and an adjusting
option for which the reward schedule is adjusted based on the subjects previous
choices. The procedure is designed to determine the point of indifference
between the two schedules which is then used to estimate a utility equivalence
point between the two options. Analyzing the titration procedure as a Markov
birth death process, we show that a large class of reinforcement learning
models invariably generates a titration bias, and that the bias varies
non-linearly with the reward value. We treat several titration procedures,
presenting analytic results for some simple learning models and simulation
results for more complex models. These results suggest that results from
titration experiments are likely to be biased and that inferences based on the
titration experiments may need to be reconsidered.