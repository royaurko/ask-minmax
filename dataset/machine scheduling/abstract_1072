The concept of boosting emerged from the field of machine learning. The basic
idea is to boost the accuracy of a weak classifying tool by combining various
instances into a more accurate prediction. This general concept was later
adapted to the field of statistical modelling. This review article attempts to
highlight this evolution of boosting algorithms from machine learning to
statistical modelling. We describe the AdaBoost algorithm for classification as
well as the two most prominent statistical boosting approaches, gradient
boosting and likelihood-based boosting. Although both appraoches are typically
treated separately in the literature, they share the same methodological roots
and follow the same fundamental concepts. Compared to the initial machine
learning algorithms, which must be seen as black-box prediction schemes,
statistical boosting result in statistical models which offer a
straight-forward interpretation. We highlight the methodological background and
present the most common software implementations. Worked out examples and
corresponding R code can be found in the Appendix.