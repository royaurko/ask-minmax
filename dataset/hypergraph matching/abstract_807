In the field of ontology matching, the most systematic evaluation of matching
systems is established by the Ontology Alignment Evaluation Initiative (OAEI),
which is an annual campaign for evaluating ontology matching systems organized
by different groups of researchers. In this paper, we report on the results of
an intermediary OAEI campaign called OAEI 2011.5. The evaluations of this
campaign are divided in five tracks. Three of these tracks are new or have been
improved compared to previous OAEI campaigns. Overall, we evaluated 18 matching
systems. We discuss lessons learned, in terms of scalability, multilingual
issues and the ability do deal with real world cases from different domains.