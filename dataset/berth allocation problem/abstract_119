With the increasing popularity of Internet-based services and applications,
power efficiency is becoming a major concern for data center operators, as high
electricity consumption not only increases greenhouse gas emissions, but also
increases the cost of running the server farm itself. In this paper we address
the problem of maximizing the revenue of a service provider by means of dynamic
allocation policies that run the minimum amount of servers necessary to meet
user's requirements in terms of performance. The results of several experiments
executed using Wikipedia traces are described, showing that the proposed
schemes work well, even if the workload is non-stationary. Since any resource
allocation policy requires the use of forecasting mechanisms, various schemes
allowing compensating errors in the load forecasts are presented and evaluated.