Maximizing robustness and minimizing cost are common objectives in the design
of infrastructure networks. However, most infrastructure networks evolve and
operate in a highly decentralized fashion, which may significantly impact the
allocation of resources across the system. Here, we investigate this question
by focusing on the relation between capacity and load in different types of
real-world communication and transportation networks. We find strong empirical
evidence that the actual capacity of the network elements tends to be similar
to the maximum available capacity, if the cost is not strongly constraining. As
more weight is given to the cost, however, the capacity approaches the load
nonlinearly. In particular, all systems analyzed show larger unoccupied
portions of the capacities on network elements subjected to smaller loads,
which is in sharp contrast with the assumptions involved in (linear) models
proposed in previous theoretical studies. We describe the observed behavior of
the capacity-load relation as a function of the relative importance of the cost
by using a model that optimizes capacities to cope with network traffic
fluctuations. These results suggest that infrastructure systems have evolved
under pressure to minimize local failures, but not necessarily global failures
that can be caused by the spread of local damage through cascading processes.