In this paper we study the convergence rate of proximal-gradient homotopy
algorithm for norm-regularized linear least squares problems. Homotopy
algorithm reduces regularization parameter in a series of steps, and uses
proximal-gradient algorithm to solve the problem at each step.
Proximal-gradient algorithm has a linear rate of convergence given that the
objective function is strongly convex and the gradient of the smooth component
of the objective function is Lipschitz continuous. In general, the objective
function in this type of problems is not strongly convex, especially when the
problem is high-dimensional. We will show that if the linear sampling matrix
and the regularizing norm satisfy certain assumptions, proximal-gradient
homotopy algorithm converges with a linear rate even though the objective
function is not strongly convex. Our result generalizes results on the linear
convergence of homotopy algorithm for $l_1$-regularized least squares problems.