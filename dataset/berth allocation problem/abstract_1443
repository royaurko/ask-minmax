Latent Dirichlet Allocation (LDA) mining thematic structure of documents
plays an important role in nature language processing and machine learning
areas. However, the probability distribution from LDA only describes the
statistical relationship of occurrences in the corpus and usually in practice,
probability is not the best choice for feature representations. Recently,
embedding methods have been proposed to represent words and documents by
learning essential concepts and representations, such as Word2Vec and Doc2Vec.
The embedded representations have shown more effectiveness than LDA-style
representations in many tasks. In this paper, we propose the Topic2Vec approach
which can learn topic representations in the same semantic vector space with
words, as an alternative to probability. The experimental results show that
Topic2Vec achieves interesting and meaningful results.