We consider the problem of recovering the sparsest vector in a subspace
$\mathcal{S} \subseteq \mathbb{R}^p$ with $\mathrm{dim}(\mathcal{S}) = n < p$.
This problem can be considered a homogeneous variant of the sparse recovery
problem, and finds applications in sparse dictionary learning, sparse PCA, and
other problems in signal processing and machine learning. Simple convex
heuristics for this problem provably break down when the fraction of nonzero
entries in the target sparse vector substantially exceeds $1/\sqrt{n}$. In
contrast, we exhibit a relatively simple nonconvex approach based on
alternating directions, which provably succeeds even when the fraction of
nonzero entries is $\Omega(1)$. To our knowledge, this is the first practical
algorithm to achieve this linear scaling. This result assumes a planted sparse
model, in which the target sparse vector is embedded in an otherwise random
subspace. Empirically, our proposed algorithm also succeeds in more challenging
data models arising, e.g., from sparse dictionary learning.