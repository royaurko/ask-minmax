The systems that statisticians are asked to assess, such as nuclear weapons,
infrastructure networks, supercomputer codes and munitions, have become
increasingly complex. It is often costly to conduct full system tests. As such,
we present a review of methodology that has been proposed for addressing system
reliability with limited full system testing. The first approaches presented in
this paper are concerned with the combination of multiple sources of
information to assess the reliability of a single component. The second general
set of methodology addresses the combination of multiple levels of data to
determine system reliability. We then present developments for complex systems
beyond traditional series/parallel representations through the use of Bayesian
networks and flowgraph models. We also include methodological contributions to
resource allocation considerations for system relability assessment. We
illustrate each method with applications primarily encountered at Los Alamos
National Laboratory.