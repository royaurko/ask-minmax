The "Disaggregated Server" concept has been proposed for datacenters where
the same type server resources are aggregated in their respective pools, for
example a compute pool, memory pool, network pool, and a storage pool. Each
server is constructed dynamically by allocating the right amount of resources
from these pools according to the workload's requirements. Modularity, higher
packaging and cooling efficiencies, and higher resource utilization are among
the suggested benefits. With the emergence of very large datacenters, "clouds"
containing tens of thousands of servers, datacenter efficiency has become an
important topic. Few computer chip and systems vendors are working on and
making frequent announcements on silicon photonics and disaggregated memory
systems.
  In this paper we study the trade-off between cost and performance of building
a disaggregated memory system where DRAM modules in the datacenter are pooled,
for example in memory-only chassis and racks. The compute pool and the memory
pool are interconnected by an optical interconnect to overcome the distance and
bandwidth issues of electrical fabrics. We construct a simple cost model that
includes the cost of latency, cost of bandwidth and the savings expected from a
disaggregated memory system. We then identify the level at which a
disaggregated memory system becomes cost competitive with a traditional direct
attached memory system.
  Our analysis shows that a rack-scale disaggregated memory system will have a
non-trivial performance penalty, and at the datacenter scale the penalty is
impractically high, and the optical interconnect costs are at least a factor of
10 more expensive than where they should be when compared to the traditional
direct attached memory systems.