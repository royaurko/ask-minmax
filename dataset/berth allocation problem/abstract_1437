Latent dirichlet allocation (LDA) is a model widely used for unsupervised
probabilistic modeling of text and images. MCMC sampling from the posterior
distribution is typically performed using a collapsed Gibbs sampler that
integrates out all model parameters except the topic indicators for each word.
The topic indicators are Gibbs sampled iteratively by drawing each topic from
its conditional posterior. The popularity of this sampler stems from its
balanced combination of simplicity and efficiency, but its inherently
sequential nature is an obstacle for parallel implementations. Growing corpus
sizes and increasing model complexity are making inference in LDA models
computationally infeasible without parallel sampling. We propose a parallel
implementation of LDA that only collapses over the topic proportions in each
document and therefore allows independent sampling of the topic indicators in
different documents. We develop several modifications of the basic algorithm
that exploits sparsity and structure to further improve the performance of the
partially collapsed sampler. Contrary to other parallel LDA implementations,
the partially collapsed sampler guarantees convergence to the true posterior.
We show on several well-known corpora that the expected increase in statistical
inefficiency from only partial collapsing is smaller than commonly assumed, and
can be more than compensated by the speed-up from parallelization for larger
corpora.