Given a training sample of size $m$ from a $d$-dimensional population, we
wish to allocate a new observation $Z\in \R^d$ to this population or to the
noise. We suppose that the difference between the distribution of the
population and that of the noise is only in a shift, which is a sparse vector.
For the Gaussian noise, fixed sample size $m$, and the dimension $d$ that tends
to infinity, we obtain the sharp classification boundary and we propose
classifiers attaining this boundary. We also give extensions of this result to
the case where the sample size $m$ depends on $d$ and satisfies the condition
$(\log m)/\log d \to \gamma$, $0\le \gamma<1$, and to the case of non-Gaussian
noise satisfying the Cram\'er condition.