Flow analysis is a ubiquitous and much-studied component of compiler
technology---and its variations abound. Amongst the most well known is Shivers'
0CFA; however, the best known algorithm for 0CFA requires time cubic in the
size of the analyzed program and is unlikely to be improved. Consequently,
several analyses have been designed to approximate 0CFA by trading precision
for faster computation. Henglein's simple closure analysis, for example,
forfeits the notion of directionality in flows and enjoys an "almost linear"
time algorithm. But in making trade-offs between precision and complexity, what
has been given up and what has been gained? Where do these analyses differ and
where do they coincide?
  We identify a core language---the linear $\lambda$-calculus---where 0CFA,
simple closure analysis, and many other known approximations or restrictions to
0CFA are rendered identical. Moreover, for this core language, analysis
corresponds with (instrumented) evaluation. Because analysis faithfully
captures evaluation, and because the linear $\lambda$-calculus is complete for
PTIME, we derive PTIME-completeness results for all of these analyses.