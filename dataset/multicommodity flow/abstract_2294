The concepts of information transfer and causal effect have received much
recent attention, yet often the two are not appropriately distinguished and
certain measures have been suggested to be suitable for both. We discuss two
existing measures, transfer entropy and information flow, which can be used
separately to quantify information transfer and causal information flow
respectively. We apply these measures to cellular automata on a local scale in
space and time, in order to explicitly contrast them and emphasize the
differences between information transfer and causality. We also describe the
manner in which the measures are complementary, including the circumstances
under which the transfer entropy is the best available choice to infer a causal
effect. We show that causal information flow is a primary tool to describe the
causal structure of a system, while information transfer can then be used to
describe the emergent computation in the system.