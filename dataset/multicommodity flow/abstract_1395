In many clusters of galaxies, the cooling time at the core of the
intracluster medium is much less than the age of the system, suggesting that
the the gas should continually lose energy by radiation. Simple thermodynamic
arguments then require that the expected "cooling flow" should exhibit a
specific spectroscopic signature, characterized by a differential emission
measure distribution that is inversely proportional to the cooling function of
the plasma. That prediction can be quantitatively tested for the first time by
the Reflection Grating Spectrometer (RGS) experiment on XMM-Newton, which
provides high resolution X-ray spectra, even for moderately extended sources
like clusters. We present RGS data on 14 separate cooling flow clusters,
sampling a very wide range in mass deposition rate. Surprisingly, in all cases
we find a systematic deficit of low temperature emission relative to the
predictions of the cooling flow models. However, we do see evidence for cooling
flow gas at temperatures just below the cluster background temperature, $T_0$,
roughly down to $T_0/2$. These results are difficult to reconcile with most of
the possible explanations for the cooling flow problem that have been proposed
to date. We also present RGS data on the massive elliptical galaxy NGC 4636. In
this case, we detect evidence for resonance emission line scattering of high
oscillator strength Fe L-shell emission lines within the gaseous halo of the
galaxy. The detection of that effect leads to very tight constraints on
physical conditions within the halo. However, here again, the expected
signature of a cooling flow is not detected, perhaps suggesting some
fundamental uncertainty in our understanding of radiative cooling in low
density cosmic plasmas.