The performance of PCFGs estimated from tree banks is sensitive to the
particular way in which linguistic constructions are represented as trees in
the tree bank. This paper presents a theoretical analysis of the effect of
different tree representations for PP attachment on PCFG models, and introduces
a new methodology for empirically examining such effects using tree
transformations. It shows that one transformation, which copies the label of a
parent node onto the labels of its children, can improve the performance of a
PCFG model in terms of labelled precision and recall on held out data from 73%
(precision) and 69% (recall) to 80% and 79% respectively. It also points out
that if only maximum likelihood parses are of interest then many productions
can be ignored, since they are subsumed by combinations of other productions in
the grammar. In the Penn II tree bank grammar, almost 9% of productions are
subsumed in this way.