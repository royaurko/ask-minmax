The cellular tree classifier model addresses a fundamental problem in the
design of classifiers for a parallel or distributed computing world: Given a
data set, is it sufficient to apply a majority rule for classification, or
shall one split the data into two or more parts and send each part to a
potentially different computer (or cell) for further processing? At first
sight, it seems impossible to define with this paradigm a consistent classifier
as no cell knows the "original data size", $n$. However, we show that this is
not so by exhibiting two different consistent classifiers. The consistency is
universal but is only shown for distributions with nonatomic marginals.