We examine the performance of four different methods which are used to
measure mass segregation in star-forming regions: the radial variation of the
mass function $\mathcal{M}_{\rm MF}$; the minimum spanning tree-based
$\Lambda_{\rm MSR}$ method; the local surface density $\Sigma_{\rm LDR}$
method; and the $\Omega_{\rm GSR}$ technique, which isolates groups of stars
and determines whether the most massive star in each group is more centrally
concentrated than the average star. All four methods have been proposed in the
literature as techniques for quantifying mass segregation, yet they routinely
produce contradictory results as they do not all measure the same thing. We
apply each method to synthetic star-forming regions to determine when and why
they have shortcomings. When a star-forming region is smooth and centrally
concentrated, all four methods correctly identify mass segregation when it is
present. However, if the region is spatially substructured, the $\Omega_{\rm
GSR}$ method fails because it arbitrarily defines groups in the hierarchical
distribution, and usually discards positional information for many of the most
massive stars in the region. We also show that the $\Lambda_{\rm MSR}$ and
$\Sigma_{\rm LDR}$ methods can sometimes produce apparently contradictory
results, because they use different definitions of mass segregation. We
conclude that only $\Lambda_{\rm MSR}$ measures mass segregation in the
classical sense (without the need for defining the centre of the region),
although $\Sigma_{\rm LDR}$ does place limits on the amount of previous
dynamical evolution in a star-forming region.