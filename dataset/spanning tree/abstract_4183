We find upper bounds for the probability of underestimation and
overestimation errors in penalized likelihood context tree estimation. The
bounds are explicit and applies to processes of not necessarily finite memory.
We allow for general penalizing terms and we give conditions over the maximal
depth of the estimated trees in order to get strongly consistent estimates.
This generalizes previous results obtained in the case of estimation of the
order of a Markov chain.