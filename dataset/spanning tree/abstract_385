Although regression trees were originally designed for large datasets, they
can profitably be used on small datasets as well, including those from
replicated or unreplicated complete factorial experiments. We show that in the
latter situations, regression tree models can provide simpler and more
intuitive interpretations of interaction effects as differences between
conditional main effects. We present simulation results to verify that the
models can yield lower prediction mean squared errors than the traditional
techniques. The tree models span a wide range of sophistication, from piecewise
constant to piecewise simple and multiple linear, and from least squares to
Poisson and logistic regression.