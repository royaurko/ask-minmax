Mapping the Internet generally consists in sampling the network from a
limited set of sources by using "traceroute"-like probes. This methodology,
akin to the merging of different spanning trees to a set of destinations, has
been argued to introduce uncontrolled sampling biases that might produce
statistical properties of the sampled graph which sharply differ from the
original ones. Here we explore these biases and provide a statistical analysis
of their origin. We derive a mean-field analytical approximation for the
probability of edge and vertex detection that exploits the role of the number
of sources and targets and allows us to relate the global topological
properties of the underlying network with the statistical accuracy of the
sampled graph. In particular we find that the edge and vertex detection
probability is depending on the betweenness centrality of each element. This
allows us to show that shortest path routed sampling provides a better
characterization of underlying graphs with scale-free topology. We complement
the analytical discussion with a throughout numerical investigation of
simulated mapping strategies in different network models. We show that sampled
graphs provide a fair qualitative characterization of the statistical
properties of the original networks in a fair range of different strategies and
exploration parameters. The numerical study also allows the identification of
intervals of the exploration parameters that optimize the fraction of nodes and
edges discovered in the sampled graph. This finding might hint the steps toward
more efficient mapping strategies.