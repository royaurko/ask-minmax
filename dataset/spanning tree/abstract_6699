Hierarchical latent class (HLC) models are tree-structured Bayesian networks
where leaf nodes are observed while internal nodes are latent. There are no
theoretically well justified model selection criteria for HLC models in
particular and Bayesian networks with latent nodes in general. Nonetheless,
empirical studies suggest that the BIC score is a reasonable criterion to use
in practice for learning HLC models. Empirical studies also suggest that
sometimes model selection can be improved if standard model dimension is
replaced with effective model dimension in the penalty term of the BIC score.
Effective dimensions are difficult to compute. In this paper, we prove a
theorem that relates the effective dimension of an HLC model to the effective
dimensions of a number of latent class models. The theorem makes it
computationally feasible to compute the effective dimensions of large HLC
models. The theorem can also be used to compute the effective dimensions of
general tree models.