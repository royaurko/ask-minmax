Traditional natural language parsers are based on rewrite rule systems
developed in an arduous, time-consuming manner by grammarians.  A majority
of the grammarian's efforts are devoted to the disambiguation process,
first hypothesizing rules which dictate constituent categories and
relationships among words in ambiguous sentences, and then seeking
exceptions and corrections to these rules.
  In this work, I propose an automatic method for acquiring a statistical
parser from a set of parsed sentences which takes advantage of some initial
linguistic input, but avoids the pitfalls of the iterative and seemingly
endless grammar development process.  Based on distributionally-derived and
linguistically-based features of language, this parser acquires a set of
statistical decision trees which assign a probability distribution on the
space of parse trees given the input sentence.  These decision trees take
advantage of significant amount of contextual information, potentially
including all of the lexical information in the sentence, to produce highly
accurate statistical models of the disambiguation process.  By basing the
disambiguation criteria selection on entropy reduction rather than human
intuition, this parser development method is able to consider more sentences
than a human grammarian can when making individual disambiguation rules.
  In experiments between a parser, acquired using this statistical
framework, and a grammarian's rule-based parser, developed over a ten-year
period, both using the same training material and test sentences, the
decision tree parser significantly outperformed the grammar-based parser on
the accuracy measure which the grammarian was trying to maximize, achieving
an accuracy of 78% compared to the grammar-based parser's 69%.