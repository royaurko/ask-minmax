A standard goal of model evaluation and selection is to find a model that
approximates the truth well while at the same time is as parsimonious as
possible. In this paper we emphasize the point of view that the models under
consideration are almost always false, if viewed realistically, and so we
should analyze model adequacy from that point of view. We investigate this
issue in large samples by looking at a model credibility index, which is
designed to serve as a one-number summary measure of model adequacy. We define
the index to be the maximum sample size at which samples from the model and
those from the true data generating mechanism are nearly indistinguishable. We
use standard notions from hypothesis testing to make this definition precise.
We use data subsampling to estimate the index. We show that the definition
leads us to some new ways of viewing models as flawed but useful. The concept
is an extension of the work of Davies [Statist. Neerlandica 49 (1995)
185--245].