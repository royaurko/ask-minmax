Perceptual similarity is a cognitive judgment that represents the end-stage
of a complex cascade of hierarchical processing throughout visual cortex.
Although it is intuitive that visual objects that appear similar may share
similar underlying patterns of neural activation, a direct mapping between
perceptual similarity and representational distance has not been demonstrated.
Here we explore the relationship between the human brain's time-varying
representation of visual patterns and behavioral judgments of perceptual
similarity. The visual stimuli were abstract patterns constructed from
identical perceptual units (oriented Gabor patches) so that each pattern had a
unique global form or perceptual 'Gestalt'. The visual stimuli were decodable
from evoked neural activation patterns measured with magnetoencephalography
(MEG), however, stimuli differed in the similarity of their neural
representation as estimated by differences in decodability. Early after
stimulus onset (from 50ms), a model based on retinotopic organization predicted
the representational similarity of the visual stimuli. Following the peak
correlation between the retinotopic model and neural data at 80ms, the neural
representations quickly evolved so that retinotopy no longer provided a
sufficient account of the brain's time-varying representation of the stimuli.
Overall the strongest predictor of the brain's representation was a model based
on human judgments of perceptual similarity, which reached the theoretical
limits of the maximum correlation with the neural data defined by the 'noise
ceiling'. Our results show that large-scale brain activation patterns contain a
neural signature for the perceptual Gestalt of composite visual features, and
demonstrate a strong correspondence between perception and complex patterns of
brain activity.