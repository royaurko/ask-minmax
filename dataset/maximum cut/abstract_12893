We consider a linear regression model, with the parameter of interest a
specified linear combination of the regression parameter vector. We suppose
that, as a first step, a data-based model selection (e.g. by preliminary
hypothesis tests or minimizing AIC) is used to select a model. It is common
statistical practice to then construct a confidence interval for the parameter
of interest based on the assumption that the selected model had been given to
us a priori. This assumption is false and it can lead to a confidence interval
with poor coverage properties. We provide an easily-computed finite sample
upper bound (calculated by repeated numerical evaluation of a double integral)
to the minimum coverage probability of this confidence interval. This bound
applies for model selection by any of the following methods: minimum AIC,
minimum BIC, maximum adjusted R-squared, minimum Mallows' Cp and t-tests. The
importance of this upper bound is that it delineates general categories of
design matrices and model selection procedures for which this confidence
interval has poor coverage properties. This upper bound is shown to be a finite
sample analogue of an earlier large sample upper bound due to Kabaila and Leeb.