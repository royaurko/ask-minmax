When analyzing incomplete data, is it better to use multiple imputation (MI)
or full information maximum likelihood (ML)? In large samples ML is clearly
better, but in small samples ML's usefulness has been limited because ML
commonly uses normal test statistics and confidence intervals that require
large samples. We propose small-sample t-based ML confidence intervals that
have good coverage and are shorter than t-based confidence intervals under MI.
We also show that ML point estimates are less biased and more efficient than MI
point estimates in small samples of bivariate normal data. With our new
confidence intervals, ML should be preferred over MI, even in small samples,
whenever both options are available.