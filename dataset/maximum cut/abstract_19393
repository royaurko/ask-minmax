In a coeval group of low-mass stars, the luminosity of the sharp transition
between stars that retain their initial lithium and those at slightly higher
masses in which Li has been depleted by nuclear reactions, the lithium
depletion boundary (LDB), has been advanced as an almost model-independent
means of establishing an age scale for young stars. Here we construct
polytropic models of contracting pre-main sequence stars (PMS) that have cool,
magnetic starspots blocking a fraction $\beta$ of their photospheric flux.
Starspots slow the descent along Hayashi tracks, leading to lower core
temperatures and less Li destruction at a given mass and age. The age,
$\tau_{\rm LDB}$, determined from the luminosity of the LDB, $L_{\rm LDB}$, is
increased by a factor $(1-\beta)^{-E}$ compared to that inferred from unspotted
models, where $E \simeq 1 + d\log \tau_{\rm LDB}/d \log L_{\rm LDB}$ and has a
value $\sim 0.5$ at ages $< 80$ Myr, decreasing to $\sim 0.3$ for older stars.
Spotted stars have virtually the same relationship between K-band bolometric
correction and colour as unspotted stars, so this relationship applies equally
to ages inferred from the absolute K magnitude of the LDB. Low-mass PMS stars
do have starspots, but the appropriate value of $\beta$ is highly uncertain
with a probable range of $0.1 < \beta < 0.4$. For the smaller $\beta$ values
our result suggests a modest systematic increase in LDB ages that is comparable
with the maximum levels of theoretical uncertainty previously claimed for the
technique. The largest $\beta$ values would however increase LDB ages by 20-30
per cent and demand a re-evaluation of other age estimation techniques
calibrated using LDB ages.