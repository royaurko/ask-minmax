In this paper we combine the advantages of a model using global source
sentence contexts, the Discriminative Word Lexicon, and neural networks. By
using deep neural networks instead of the linear maximum entropy model in the
Discriminative Word Lexicon models, we are able to leverage dependencies
between different source words due to the non-linearity. Furthermore, the
models for different target words can share parameters and therefore data
sparsity problems are effectively reduced.
  By using this approach in a state-of-the-art translation system, we can
improve the performance by up to 0.5 BLEU points for three different language
pairs on the TED translation task.