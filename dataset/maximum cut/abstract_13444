In a distributed clustering algorithm introduced by Coffman, Courtois,
Gilbert and Piret \cite{coffman91}, each vertex of $\mathbb{Z}^d$ receives an
initial amount of a resource, and, at each iteration, transfers all of its
resource to the neighboring vertex which currently holds the maximum amount of
resource. In \cite{hlrnss} it was shown that, if the distribution of the
initial quantities of resource is invariant under lattice translations, then
the flow of resource at each vertex eventually stops almost surely, thus
solving a problem posed in \cite{berg91}. In this article we prove the
existence of translation-invariant initial distributions for which resources
nevertheless escape to infinity, in the sense that the the final amount of
resource at a given vertex is strictly smaller in expectation than the initial
amount. This answers a question posed in \cite{hlrnss}.