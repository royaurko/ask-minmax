We introduce a simply stated conjecture regarding the maximum mutual
information a Boolean function can reveal about noisy inputs. Specifically, let
$X^n$ be i.i.d. Bernoulli(1/2), and let $Y^n$ be the result of passing $X^n$
through a memoryless binary symmetric channel with crossover probability
$\alpha$. For any Boolean function $b:\{0,1\}^n\rightarrow \{0,1\}$, we
conjecture that $I(b(X^n);Y^n)\leq 1-H(\alpha)$. While the conjecture remains
open, we provide substantial evidence supporting its validity.