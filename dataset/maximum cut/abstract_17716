The estimation of a log-concave density on $\mathbb{R}^d$ represents a
central problem in the area of nonparametric inference under shape constraints.
In this paper, we study the performance of log-concave density estimators with
respect to global (e.g. squared Hellinger) loss functions, and adopt a minimax
approach. We first show that no statistical procedure based on a sample of size
$n$ can estimate a log-concave density with supremum risk smaller than order
$n^{-4/5}$, when $d=1$, and order $n^{-2/(d+1)}$ when $d \geq 2$. In
particular, this reveals a sense in which, when $d \geq 3$, log-concave density
estimation is fundamentally more challenging than the estimation of a density
with two bounded derivatives (a problem to which it has been compared). Second,
we show that the Hellinger $\epsilon$-bracketing entropy of a class of
log-concave densities with small mean and covariance matrix close to the
identity grows like $\max\{\epsilon^{-d/2},\epsilon^{-(d-1)}\}$ as $\epsilon
\searrow 0$. This enables us to obtain rates of convergence for the supremum
squared Hellinger risk of the log-concave maximum likelihood estimator of
$O(n^{-4/(d+4)})$ when $d \leq 2$ (the minimax optimal rate), $O(n^{-1/2}\log
n)$ when $d = 3$, and $O(n^{-1/(d-1)})$ when $d \geq 4$.