I consider the effect of a finite sample size on the entropy of a sample of
independent events. I propose formula for entropy which satisfies Shannon's
axioms, and which reduces to Shannon's entropy when sample size is infinite. I
discuss the physical meaning of the difference between two formulas, including
some practical implications, such as maximum achievable channel utilization,
and minimum achievable communication protocol overhead, for a given message
size.