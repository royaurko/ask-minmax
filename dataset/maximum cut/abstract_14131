Stellar coronae have been invoked to explain the apparently extragalactic
dispersion measures observed in fast radio bursts. This paper demonstrates that
the suggested plasma densities would lead to deviations from the standard
dispersion curve that are inconsistent with the data. The problem is then
turned around and higher-order dispersion terms are connected to the moments of
the density distribution along the line of sight. The deviations quantified in
three observed bursts are analysed and a lower limit on the maximum electron
density is obtained in one case, although with considerable uncertainty.
Selection effects are then discussed and shown to be non-restrictive in
relation to plasma density, except at the lowest frequencies and highest
temperatures.