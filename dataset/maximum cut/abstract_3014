Zipf's law is a fundamental paradigm in the statistics of written and spoken
natural language as well as in other communication systems. We raise the
question of the elementary units for which Zipf's law should hold in the most
natural way, studying its validity for plain word forms and for the
corresponding lemma forms. In order to have as homogeneous sources as possible,
we analyze some of the longest literary texts ever written, comprising four
different languages, with different levels of morphological complexity. In all
cases Zipf's law is fulfilled, in the sense that a power-law distribution of
word or lemma frequencies is valid for several orders of magnitude. We
investigate the extent to which the word-lemma transformation preserves two
parameters of Zipf's law: the exponent and the low-frequency cut-off. We are
not able to demonstrate a strict invariance of the tail, as for a few texts
both exponents deviate significantly, but we conclude that the exponents are
very similar, despite the remarkable transformation that going from words to
lemmas represents, considerably affecting all ranges of frequencies. In
contrast, the low-frequency cut-offs are less stable.