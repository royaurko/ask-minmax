Blind deconvolution has made significant progress in the past decade. Most
successful algorithms are classified either as Variational or Maximum
a-Posteriori ($MAP$). In spite of the superior theoretical justification of
variational techniques, carefully constructed $MAP$ algorithms have proven
equally effective in practice. In this paper, we show that all successful $MAP$
and variational algorithms share a common framework, relying on the following
key principles: sparsity promotion in the gradient domain, $l_2$ regularization
for kernel estimation, and the use of convex (often quadratic) cost functions.
Our observations lead to a unified understanding of the principles required for
successful blind deconvolution. We incorporate these principles into a novel
algorithm that improves significantly upon the state of the art.