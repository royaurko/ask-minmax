The Expectation-Maximization (EM) algorithm is a commonly used method for
finding the maximum likelihood estimates of the parameters in a mixture model
via coordinate ascent. A serious pitfall with the algorithm is that in the case
of multimodal likelihood functions, it can get trapped at a local maximum. This
problem often occurs when sub-optimal starting values are used to initialize
the algorithm.
  Bayesian initialization averaging (BIA) is proposed as a method to generate
high quality starting values for the EM algorithm. Competing sets of starting
values are combined as a weighted average, which is then used as the starting
position for one full EM run. The convergent log-likelihoods and associated
clustering solutions of observations using the BIA method for the EM and an
EM-type algorithm are presented for a range of continuous, categorical and
network data sets. These compare favorably with the output produced using
competing initialization methods such as random starts, hierarchical clustering
and deterministic annealing. The global maximum likelihood is recovered in a
higher percentage of cases and the different clustering solutions are examined.