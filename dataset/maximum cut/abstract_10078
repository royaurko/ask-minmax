In this paper we consider the problem of estimating a parameter of a
probability distribution when we have some prior information on a nuisance
parameter. We start by the very simple case where we know perfectly the value
of the nuisance parameter. The complete likelihood is the classical tool in
this case. Then, progressively, we consider the case where we are given a prior
probability distribution on this nuisance parameter. The marginal likelihood is
then the classical tool in this case. Then, we consider the case where we only
have a fixed number of its moments. Here, we may use the maximum entropy (ME)
principle to assign a prior law and thus go back to the previous case. Finally,
we consider the case where we know only its median. In our knowledge, there is
not any classical tool for this case. We propose then a new tool for this case
based on a recently proposed alternative distribution to the marginal
probability distribution. This new criterion is obtained by first remarking
that the marginal distribution can be considered as the mean value of the
original distribution over the prior probability law of the nuisance parameter,
and then, by using the median in place of the mean. In this paper, we first
summarize the classical tools used for the three first cases, then we give the
precise definition of this new criterion and its properties and, finally,
present a few examples to show the differences of these cases.
  Key Words: Nuisance parameter, Bayesian inference, Maximum Entropy,
Marginalization, Incomplete knowledge, Mean and Median of the Likelihood over
the prior distribution