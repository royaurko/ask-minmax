We discuss how the method of maximum entropy, MaxEnt, can be extended beyond
its original scope, as a rule to assign a probability distribution, to a
full-fledged method for inductive inference. The main concept is the (relative)
entropy S[p|q] which is designed as a tool to update from a prior probability
distribution q to a posterior probability distribution p when new information
in the form of a constraint becomes available. The extended method goes beyond
the mere selection of a single posterior p, but also addresses the question of
how much less probable other distributions might be. Our approach clarifies how
the entropy S[p|q] is used while avoiding the question of its meaning.
Ultimately, entropy is a tool for induction which needs no interpretation.
Finally, being a tool for generalization from special examples, we ask whether
the functional form of the entropy depends on the choice of the examples and we
find that it does. The conclusion is that there is no single general theory of
inductive inference and that alternative expressions for the entropy are
possible.