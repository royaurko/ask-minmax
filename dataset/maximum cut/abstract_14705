It has been shown \citep{broeck90:physicalreview,patarnello87:europhys} that
feedforward Boolean networks can learn to perform specific simple tasks and
generalize well if only a subset of the learning examples is provided for
learning. Here, we extend this body of work and show experimentally that random
Boolean networks (RBNs), where both the interconnections and the Boolean
transfer functions are chosen at random initially, can be evolved by using a
state-topology evolution to solve simple tasks. We measure the learning and
generalization performance, investigate the influence of the average node
connectivity $K$, the system size $N$, and introduce a new measure that allows
to better describe the network's learning and generalization behavior. We show
that the connectivity of the maximum entropy networks scales as a power-law of
the system size $N$. Our results show that networks with higher average
connectivity $K$ (supercritical) achieve higher memorization and partial
generalization. However, near critical connectivity, the networks show a higher
perfect generalization on the even-odd task.