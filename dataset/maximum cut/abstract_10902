The recently proposed discrete scale invariance and its associated
log-periodicity are an elaboration of the concept of scale invariance in which
the system is scale invariant only under powers of specific values of the
magnification factor. We report on the discovery of a novel mechanism for such
log-periodicity relying solely on the manipulation of data. This ``synthetic''
scenario for log-periodicity relies on two steps: (1) the fact that
approximately logarithmic sampling in time corresponds to uniform sampling in
the logarithm of time; and (2) a low-pass-filtering step, as occurs in
constructing cumulative functions, in maximum likelihood estimations, and in
de-trending, reddens the noise and, in a finite sample, creates a maximum in
the spectrum leading to a most probable frequency in the logarithm of time. We
explore in detail this mechanism and present extensive numerical simulations.
We use this insight to analyze the 27 best aftershock sequences studied by
Kisslinger and Jones [1991] to search for traces of genuine log-periodic
corrections to Omori's law, which states that the earthquake rate decays
approximately as the inverse of the time since the last main shock. The
observed log-periodicity is shown to almost entirely result from the
``synthetic scenario'' owing to the data analysis. From a statistical point of
view, resolving the issue of the possible existence of log-periodicity in
aftershocks will be very difficult as Omori's law describes a point process
with a uniform sampling in the logarithm of the time. By construction, strong
log-periodic fluctuations are thus created by this logarithmic sampling.