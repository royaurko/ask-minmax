The {\em repair locality} of a distributed storage code is the maximum number
of nodes that ever needs to be contacted during the repair of a failed node.
Having small repair locality is desirable, since it is proportional to the
number of disk accesses during repair. However, recent publications show that
small repair locality comes with a penalty in terms of code distance or storage
overhead if exact repair is required.
  Here, we first review some of the main results on storage codes under various
repair regimes and discuss the recent work on possible
(information-theoretical) trade-offs between repair locality and other code
parameters like storage overhead and code distance, under the exact repair
regime.
  Then we present some new information theoretical lower bounds on the storage
overhead as a function of the repair locality, valid for all common coding and
repair models. In particular, we show that if each of the $n$ nodes in a
distributed storage system has storage capacity $\ga$ and if, at any time, a
failed node can be {\em functionally} repaired by contacting {\em some} set of
$r$ nodes (which may depend on the actual state of the system) and downloading
an amount $\gb$ of data from each, then in the extreme cases where $\ga=\gb$ or
$\ga = r\gb$, the maximal coding rate is at most $r/(r+1)$ or 1/2, respectively
(that is, the excess storage overhead is at least $1/r$ or 1, respectively).