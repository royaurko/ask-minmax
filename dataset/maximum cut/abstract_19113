We review current methods for building PSF-matching kernels for the purposes
of image subtraction or coaddition. Such methods use a linear decomposition of
the kernel on a series of basis functions. The correct choice of these basis
functions is fundamental to the efficiency and effectiveness of the matching -
the chosen bases should represent the underlying signal using a reasonably
small number of shapes, and/or have a minimum number of user-adjustable tuning
parameters. We examine methods whose bases comprise multiple Gauss-Hermite
polynomials, as well as a form free basis composed of delta-functions. Kernels
derived from delta-functions are unsurprisingly shown to be more expressive;
they are able to take more general shapes and perform better in situations
where sum-of-Gaussian methods are known to fail. However, due to its many
degrees of freedom (the maximum number allowed by the kernel size) this basis
tends to overfit the problem, and yields noisy kernels having large variance.
We introduce a new technique to regularize these delta-function kernel
solutions, which bridges the gap between the generality of delta-function
kernels, and the compactness of sum-of-Gaussian kernels. Through this
regularization we are able to create general kernel solutions that represent
the intrinsic shape of the PSF-matching kernel with only one degree of freedom,
the strength of the regularization lambda. The role of lambda is effectively to
exchange variance in the resulting difference image with variance in the kernel
itself. We examine considerations in choosing the value of lambda, including
statistical risk estimators and the ability of the solution to predict
solutions for adjacent areas. Both of these suggest moderate strengths of
lambda between 0.1 and 1.0, although this optimization is likely dataset
dependent.