Maximum entropy method is a constructive criterion for setting up a
probability distribution maximally non-committal to missing information on the
basis of partial knowledge, usually stated as constrains on expectation values
of some functions. In connection with experiments sample average of those
functions are used as surrogate of the expectation values. We address sampling
bias in maximum entropy approaches with finite data sets without forcedly
equating expectation values to corresponding experimental average values.
Though we rise the approach in a general formulation, the equations are
unfortunately complicated. We bring simple case examples, hopping clear but
sufficient illustration of the concepts.