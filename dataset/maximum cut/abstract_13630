G. Edelman, O. Sporns, and G. Tononi introduced in theoretical biology the
neural complexity of a family of random variables. This functional is a special
case of intricacy, i.e., an average of the mutual information of subsystems
whose weights have good mathematical properties. Moreover, its maximum value
grows at a definite speed with the size of the system.
  In this work, we compute exactly this speed of growth by building
"approximate maximizers" subject to an entropy condition. These approximate
maximizers work simultaneously for all intricacies. We also establish some
properties of arbitrary approximate maximizers, in particular the existence of
a threshold in the size of subsystems of approximate maximizers: most smaller
subsystems are almost equidistributed, most larger subsystems determine the
full system.
  The main ideas are a random construction of almost maximizers with a high
statistical symmetry and the consideration of entropy profiles, i.e., the
average entropies of sub-systems of a given size. The latter gives rise to
interesting questions of probability and information theory.