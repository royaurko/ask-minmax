We present 33\,GHz photometry of 103 galaxy nuclei and extranuclear
star-forming complexes taken with the Green Bank Telescope (GBT) as part of the
Star Formation in Radio Survey (SFRS). Among the sources without evidence for
an AGN, and also having lower frequency radio data, we find a median thermal
fraction at 33GHz of ~76% with a dispersion of ~24%. For all sources resolved
on scales <0.5kpc, the thermal fraction is even larger, being >90%. This
suggests that the rest-frame 33GHz emission provides a sensitive measure of the
ionizing photon rate from young star-forming regions, thus making it a robust
star formation rate indicator. Taking the 33GHz star formation rates as a
reference, we investigate other empirical calibrations relying on different
combinations of warm 24\mu m dust, total infrared (IR; 8-1000\mu m), H\alpha\
line, and far-UV continuum emission. The recipes derived here generally agree
with others found in the literature, albeit with a large dispersion that most
likely stems from a combination of effects. Comparing the 33GHz to total IR
flux ratios as a function of the radio spectral index, measured between 1.7 and
33GHz, we find that the ratio increases as the radio spectral index flattens
which does not appear to be a distance effect. Consequently, the ratio of
non-thermal to total IR emission appears relatively constant, suggesting only
moderate variations in the cosmic-ray electron injection spectrum and ratio of
synchrotron to total cooling processes among star-forming complexes. Assuming
that this trend solely arises from an increase in the thermal fraction sets a
maximum on the scatter of the non-thermal spectral indices among the
star-forming regions of \sigma_\alpha^{NT} < 0.13.