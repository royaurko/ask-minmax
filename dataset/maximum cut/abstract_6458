Though Shannon entropy of a probability measure $P$, defined as $- \int_{X}
\frac{\ud P}{\ud \mu} \ln \frac{\ud P}{\ud\mu} \ud \mu$ on a measure space $(X,
\mathfrak{M},\mu)$, does not qualify itself as an information measure (it is
not a natural extension of the discrete case), maximum entropy (ME)
prescriptions in the measure-theoretic case are consistent with that of
discrete case. In this paper, we study the measure-theoretic definitions of
generalized information measures and discuss the ME prescriptions. We present
two results in this regard: (i) we prove that, as in the case of classical
relative-entropy, the measure-theoretic definitions of generalized
relative-entropies, R\'{e}nyi and Tsallis, are natural extensions of their
respective discrete cases, (ii) we show that, ME prescriptions of
measure-theoretic Tsallis entropy are consistent with the discrete case.