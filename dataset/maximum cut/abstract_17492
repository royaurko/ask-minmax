We determine the limit of the lowest achievable photoemitted electron
temperature, and therefore the maximum achievable electron brightness, due to
heating just after emission into vacuum, applicable to dense relativistic or
nonrelativistic photoelectron beams. This heating is due to poorly screened
Coulomb interactions equivalent to disorder induced heating seen in ultracold
neutral plasmas. We first show that traditional analytic methods of Coulomb
collisions fail for the calculation of this strongly coupled heating. Instead,
we employ an N-body tree algorithm to compute the universal scaling of the
disorder induced heating in fully contained bunches, and show it to agree well
with a simple model utilizing the tabulated correlated energy of one component
plasmas. We also present simulations for beams undergoing Coulomb explosion at
the photocathode, and demonstrate that both the temperature growth and
subsequent cooling must be characterized by correlated effects, as well as
correlation-frozen dynamics. In either case, the induced temperature is found
to be of several meV for typical photoinjector beam densities, a significant
fraction of the intrinsic beam temperature of the coldest semiconductor
photocathodes. Thus, we expect disorder induced heating to become a major
limiting factor in the next generation of photoemission sources delivering
dense bunches and employing ultra-cold photoemitters.