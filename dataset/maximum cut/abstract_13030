We introduce a useful tool for analyzing boosting algorithms called the
``smooth margin function,'' a differentiable approximation of the usual margin
for boosting algorithms. We present two boosting algorithms based on this
smooth margin, ``coordinate ascent boosting'' and ``approximate coordinate
ascent boosting,'' which are similar to Freund and Schapire's AdaBoost
algorithm and Breiman's arc-gv algorithm. We give convergence rates to the
maximum margin solution for both of our algorithms and for arc-gv. We then
study AdaBoost's convergence properties using the smooth margin function. We
precisely bound the margin attained by AdaBoost when the edges of the weak
classifiers fall within a specified range. This shows that a previous bound
proved by R\"{a}tsch and Warmuth is exactly tight. Furthermore, we use the
smooth margin to capture explicit properties of AdaBoost in cases where cyclic
behavior occurs.