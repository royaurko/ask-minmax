We consider model selection in generalized linear models (GLM) for
high-dimensional data and propose a wide class of model selection criteria
based on penalized maximum likelihood with a complexity penalty on the model
size. We derive a general nonasymptotic upper bound for the expected
Kullback-Leibler divergence between the true distribution of the data and that
generated by a selected model, and establish the corresponding minimax lower
bounds for sparse GLM. For the properly chosen (nonlinear) penalty, the
resulting penalized maximum likelihood estimator is shown to be asymptotically
minimax and adaptive to the unknown sparsity. We discuss also possible
extensions of the proposed approach to model selection in GLM under additional
structural constraints and aggregation.