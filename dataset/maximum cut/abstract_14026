In this article we focus on Maximum Likelihood estimation (MLE) for the
static parameters of hidden Markov models (HMMs). We will consider the case
where one cannot or does not want to compute the conditional likelihood density
of the observation given the hidden state because of increased computational
complexity or analytical intractability. Instead we will assume that one may
obtain samples from this conditional likelihood and hence use approximate
Bayesian computation (ABC) approximations of the original HMM. ABC
approximations are biased, but the bias can be controlled to arbitrary
precision via a parameter \epsilon>0; the bias typically goes to zero as
\epsilon \searrow 0. We first establish that the bias in the log-likelihood and
gradient of the log-likelihood of the ABC approximation, for a fixed batch of
data, is no worse than \mathcal{O}(n\epsilon), n being the number of data;
hence, for computational reasons, one might expect reasonable parameter
estimates using such an ABC approximation. Turning to the computational problem
of estimating $\theta$, we propose, using the ABC-sequential Monte Carlo (SMC)
algorithm in Jasra et al. (2012), an approach based upon simultaneous
perturbation stochastic approximation (SPSA). Our method is investigated on two
numerical examples