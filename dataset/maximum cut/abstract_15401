The goal of this paper is to compare several widely used Bayesian model
selection methods in practical model selection problems, highlight their
differences and give recommendations about the preferred approaches. We focus
on the variable subset selection for regression and classification and perform
several numerical experiments using both simulated and real world data. The
results show that the optimization of a utility estimate such as the
cross-validation score is liable to finding overfitted models due to relatively
high variance in the utility estimates when the data is scarce. Better and much
less varying results are obtained by incorporating all the uncertainties into a
full encompassing model and projecting this information onto the submodels. The
reference model projection appears to outperform also the maximum a posteriori
model and the selection of the most probable variables. The study also
demonstrates that the model selection can greatly benefit from using
cross-validation outside the searching process both for guiding the model size
selection and assessing the predictive performance of the finally selected
model.