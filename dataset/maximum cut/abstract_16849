Despite its obvious relevance, meaning has been outside most theoretical
approaches to information in biology. As a consequence, functional responses
based on an appropriate interpretation of signals has been replaced by a
probabilistic description of correlations between emitted and received symbols.
This assumption leads to potential paradoxes, such as the presence of a maximum
information associated to a channel that would actually create completely wrong
interpretations of the signals. Game-theoretic models of language evolution use
this view of Shannon's theory, but other approaches considering embodied
communicating agents show that the correct (meaningful) match resulting from
agent-agent exchanges is always achieved and natural systems obviously solve
the problem correctly. How can Shannon's theory be expanded in such a way that
meaning -at least, in its minimal referential form- is properly incorporated?
Inspired by the concept of {\em duality of the communicative sign} stated by
the swiss linguist Ferdinand de Saussure, here we present a complete
description of the minimal system necessary to measure the amount of
information that is consistently decoded. Several consequences of our
developments are investigated, such the uselessness of an amount of information
properly transmitted for communication among autonomous agents.