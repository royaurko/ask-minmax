Thermal transpiration effects are commonly encountered in low pressure
measurements with capacitance diaphragm gauges. They arise from the temperature
difference between the measurement volume and the temperature stabilised
manometer. Several approaches have been proposed to correct for the pressure
difference, but surface and geometric effects usually require that the
correction is determined for each gas type and gauge individually. Common
(semi) empirical corrections are based on studies of atoms or small molecules.
We present a simple calibration method for diaphragm gauges and compare
transpiration corrections for argon and styrene at pressures above 1 Pa. We
find that characteristic pressures at which the pressure difference reaches
half its maximum value, are compatible with the universal scaling p_{1/2} = 2
\{\eta} \cdot \{v_{th}} / d, thus essentially depending on gas viscosity \eta,
thermal molecular speed v_{th} and gauge tubing diameter d. This contradicts
current recommendations based on the Takaishi and Sensui formula, which show an
unphysical scaling with molecular size. Our results support the Miller or
\v{S}etina equations where the pressure dependency is basically determined by
the Knudsen number. The use of these two schemes is therefore recommended,
especially when thermal transpiration has to be predicted for new molecules.
Implications for investigations on large polyatomics are discussed.