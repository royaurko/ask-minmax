We investigate the difference between the host galaxy properties of
core-collapse supernovae and long-duration gamma-ray bursts (LGRBs), and
quantify a possible metallicity dependence of the efficiency of producing
LGRBs. We use a sample of 16 CC SNe and 16 LGRBs from Fruchter et al. (2006)
which have similar redshift distributions to eliminate galaxy evolution biases.
We make a forward prediction of their host galaxy luminosity distributions from
the overall cosmic metallicity distribution of star formation. This appoach is
supported by the finding that LGRB hosts follow the L-Z relations of
star-forming galaxies. We then compare predictions for metallicity-dependent
event efficiencies with the observed host data. We find that UV-based SFR
estimates predict the hosts distribution of CC SNe perfectly well in a
metallicity-independent form. In contrast, LGRB hosts are fainter on average by
one magnitude, almost as faint as the Large Magellanic Cloud. Assuming this is
a metallicity effect, the present data are insufficient to discriminate between
a sharp metallicity cutoff and a soft decrease in efficiency towards higher
metallicity. For a sharp cut-off, however, we find a best value for the cutoff
metallicity, as reflected in the oxygen abundance, 12+log (O/H)_lim ~ 8.7+/-0.3
at 95% confidence including systematic uncertainties on the calibration of
Kobulnicky & Kewley (2004). This value is somewhat lower than the traditionally
quoted value for the Sun, but is comparable to the revised solar oxygen
abundance (Asplund, Grevesse & Sauval 2005). LGRB models that require sharp
metallicity cutoffs well below ~1/2 the revised solar metallicity appear to be
effectively ruled out (abridged).