[Abridged] We investigate the use of a wide variety of spectroscopic
measurements to determine distances to low-redshift Type Ia supernovae (SN Ia).
We consider linear models for predicting distances to SN Ia using light-curve
width and color parameters (determined using the SALT2 light-curve fitter) and
a spectroscopic indicator, and evaluate the resulting Hubble diagram scatter
using a cross-validation procedure. We confirm the ability of spectral flux
ratios alone at maximum light to reduce the scatter of Hubble residuals by ~10%
with respect to the standard combination of light-curve width and color. When
used in combination with the SALT2 color parameter, the color-corrected flux
ratio R^c(6420/5290) at maximum light leads to an even lower scatter, although
the improvement has low statistical significance (<2 sigma) given the size of
our sample (26 SN Ia). We highlight the importance of an accurate relative flux
calibration and the failure of this method for highly-reddened objects.
Comparison with synthetic spectra from 2D delayed-detonation explosion models
shows that the correlation of R(6630/4400) with SN Ia absolute magnitudes can
be largely attributed to intrinsic color variations and not to reddening by
dust in the host galaxy. We consider flux ratios at other ages, as well as the
use of pairs of flux ratios, revealing the presence of small-scale intrinsic
spectroscopic variations in the iron-group dominated absorption features around
~4300 A and ~4800 A. The best flux ratio overall is the color-corrected
R^c(4610/4260) at t=-2.5d from maximum light, which leads to ~30% lower scatter
with respect to the standard combination of light-curve width and color. We
examine other spectroscopic indicators related to line-profile morphology, but
none appear to lead to a significant improvement over the standard light-curve
width and color parameters.