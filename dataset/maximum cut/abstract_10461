We study a classical iterative algorithm for balancing matrices in the
$L_\infty$ norm via a scaling transformation. This algorithm, which goes back
to Osborne and Parlett \& Reinsch in the 1960s, is implemented as a standard
preconditioner in many numerical linear algebra packages. Surprisingly, despite
its widespread use over several decades, no bounds were known on its rate of
convergence. In this paper we prove that, for any irreducible $n\times n$ (real
or complex) input matrix~$A$, a natural variant of the algorithm converges in
$O(n^3\log(n\rho/\varepsilon))$ elementary balancing operations, where $\rho$
measures the initial imbalance of~$A$ and $\varepsilon$ is the target imbalance
of the output matrix. (The imbalance of~$A$ is $\max_i
|\log(a_i^{\text{out}}/a_i^{\text{in}})|$, where
$a_i^{\text{out}},a_i^{\text{in}}$ are the maximum entries in magnitude in the
$i$th row and column respectively.) This bound is tight up to the $\log n$
factor. A balancing operation scales the $i$th row and column so that their
maximum entries are equal, and requires $O(m/n)$ arithmetic operations on
average, where $m$ is the number of non-zero elements in~$A$. Thus the running
time of the iterative algorithm is $\tilde{O}(n^2m)$. This is the first time
bound of any kind on any variant of the Osborne-Parlett-Reinsch algorithm. We
also prove a conjecture of Chen that characterizes those matrices for which the
limit of the balancing process is independent of the order in which balancing
operations are performed.