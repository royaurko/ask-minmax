To characterize strongly interacting statistical systems within a
thermodynamical framework - complex systems in particular - it might be
necessary to introduce generalized entropies, $S_g$. A series of such entropies
have been proposed in the past, mainly to accommodate important empirical
distribution functions to a maximum ignorance principle. Until now the
understanding of the fundamental origin of these entropies and its deeper
relations to complex systems is limited. Here we explore this questions from
first principles. We start by observing that the 4th Khinchin axiom
(separability axiom) is violated by strongly interacting systems in general and
ask about the consequences of violating the 4th axiom while assuming the first
three Khinchin axioms (K1-K3) to hold and $S_g=\sum_ig(p_i)$. We prove by
simple scaling arguments that under these requirements {\em each} statistical
system is uniquely characterized by a distinct pair of scaling exponents
$(c,d)$ in the large size limit. The exponents define equivalence classes for
all interacting and non interacting systems. This allows to derive a unique
entropy, $S_{c,d}\propto \sum_i \Gamma(d+1, 1- c \ln p_i)$, which covers all
entropies which respect K1-K3 and can be written as $S_g=\sum_ig(p_i)$. Known
entropies can now be classified within these equivalence classes. The
corresponding distribution functions are special forms of Lambert-$W$
exponentials containing as special cases Boltzmann, stretched exponential and
Tsallis distributions (power-laws) -- all widely abundant in nature. This is,
to our knowledge, the first {\em ab initio} justification for the existence of
generalized entropies. Even though here we assume $S_g=\sum_ig(p_i)$, we show
that more general entropic forms can be classified along the same lines.