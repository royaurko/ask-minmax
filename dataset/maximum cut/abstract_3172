We give a characterization of Maximum Entropy/Minimum Relative Entropy
inference by providing two `strong entropy concentration' theorems. These
theorems unify and generalize Jaynes' `concentration phenomenon' and Van
Campenhout and Cover's `conditional limit theorem'. The theorems characterize
exactly in what sense a prior distribution Q conditioned on a given constraint,
and the distribution P, minimizing the relative entropy D(P ||Q) over all
distributions satisfying the constraint, are `close' to each other. We then
apply our theorems to establish the relationship between entropy concentration
and a game-theoretic characterization of Maximum Entropy Inference due to
Topsoe and others.