We propose a general maximum likelihood empirical Bayes (GMLEB) method for
the estimation of a mean vector based on observations with i.i.d. normal
errors. We prove that under mild moment conditions on the unknown means, the
average mean squared error (MSE) of the GMLEB is within an infinitesimal
fraction of the minimum average MSE among all separable estimators which use a
single deterministic estimating function on individual observations, provided
that the risk is of greater order than $(\log n)^5/n$. We also prove that the
GMLEB is uniformly approximately minimax in regular and weak $\ell_p$ balls
when the order of the length-normalized norm of the unknown means is between
$(\log n)^{\kappa_1}/n^{1/(p\wedge2)}$ and $n/(\log n)^{\kappa_2}$. Simulation
experiments demonstrate that the GMLEB outperforms the James--Stein and several
state-of-the-art threshold estimators in a wide range of settings without much
down side.