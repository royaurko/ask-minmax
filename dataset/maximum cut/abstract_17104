A rapidly growing amount of evidences, mostly coming from the recent
gamma-ray observations of Galactic supernova remnants (SNRs), is seriously
challenging our understanding of how particles are accelerated at fast shocks.
The cosmic-ray (CR) spectra required to account for the observed phenomenology
are in fact as steep as $E^{-2.2}--E^{-2.4}$, i.e., steeper than the
test-particle prediction of first-order Fermi acceleration, and significantly
steeper than what expected in a more refined non-linear theory of diffusive
shock acceleration. By accounting for the dynamical back-reaction of the
non-thermal particles, such a theory in fact predicts that the more efficient
the particle acceleration, the flatter the CR spectrum. In this work we put
forward a self-consistent scenario in which the account for the magnetic field
amplification induced by CR streaming produces the conditions for reversing
such a trend, allowing --- at the same time --- for rather steep spectra and CR
acceleration efficiencies (about 20%) consistent with the hypothesis that SNRs
are the sources of Galactic CRs. In particular, we quantitatively work out the
details of instantaneous and cumulative CR spectra during the evolution of a
typical SNR, also stressing the implications of the observed levels of
magnetization on both the expected maximum energy and the predicted CR
acceleration efficiency. The latter naturally turns out to saturate around
10-30%, almost independently of the fraction of particles injected into the
acceleration process as long as this fraction is larger than about $10^{-4}$.