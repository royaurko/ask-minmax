We find the value of constants related to constraints in characterization of
some known statistical distributions and then we proceed to use the idea behind
maximum entropy principle to derive generalized version of this distributions
using the Tsallis' and Renyi information measures instead of the well-known
Bolztmann-Gibbs-Shannon. These generalized distributions will depend on q (real
number) and in the limit (q=1) we obtain the "classical" ones. We found that
apart from a constant, generalized versions of statistical distributions
following Tsallis' or Renyi are undistinguishable.