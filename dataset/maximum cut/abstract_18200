We present results from a statistical study of clouds in two-dimensional
numerical simulations of the interstellar medium. The clouds in the simulations
exhibit a differential mass spectrum $dN(M)/dM \sim M^{-1.44 \pm 0.1}$ and a
velocity dispersion-size relation $\Delta v \sim R^{0.41 \pm 0.08}$. However,
the clouds do {\it not} exhibit a clear density-size relation. At a given mean
density, clouds span a range of sizes from the smallest resolved scales up to a
maximum given by a Larson-type relation $R_{\rm max} \sim \rho^\alpha$, with
$\alpha = -0.81 \pm .15$, although numerical effects cannot be ruled out as
responsible for the latter correlation. Clouds span a range of column densities
$N$ of two orders of magnitude, supporting the suggestion that the
observational density-size relation may be an artifact of survey limitations.
In this case, the $\Delta v$--$R$ relation can be interpreted as a direct
consequence of a $k^{-2}$ turbulent spectrum, characteristic of a field of
shocks, verified in the simulations, rather than of virial equilibrium of
clouds with a $\rho \propto R^{-1}$ law. However, we also discuss the
possibility that the clouds are in balance between self-gravity and turbulence,
but with a scatter of at least a factor of 10 in the $Delta v$--$R$ relation,
and of 100 in the density-size relation, according to the equilibrium relation
$\Delta v \sim (NR)^{1/2}$. Finally, we compare these results with
observational data. We propose a simple model suggesting that recent results
finding nearly constant column densities for dark IRAS clouds may be an
artifact of a temperature gradient within the clouds induced by external
radiative heating. As a consequence, we emphasize that IRAS surface brightness
maps are not appropriate for measuring column densities.