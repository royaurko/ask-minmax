Linear-scaling electronic-structure techniques, also called O(N) techniques,
rely heavily on the multiplication of sparse matrices, where the sparsity
arises from spatial cut-offs. In order to treat very large systems, the
calculations must be run on parallel computers. We analyse the problem of
parallelising the multiplication of sparse matrices with the sparsity pattern
required by linear-scaling techniques. We show that the management of
inter-node communications and the effective use of on-node cache are helped by
organising the atoms into compact groups. We also discuss how to identify a key
part of the code called the `multiplication kernel', which is repeatedly
invoked to build up the matrix product, and explicit code is presented for this
kernel. Numerical tests of the resulting multiplication code are reported for
cases of practical interest, and it is shown that their scaling properties for
systems containing up to 20,000 atoms on machines having up to 512 processors
are excellent. The tests also show that the cpu efficiency of our code is
satisfactory.