In this paper, we propose a general framework to learn a robust large-margin
classifier, when corrupt measurements caused by sensor failure might be present
in the training set. The goal is to minimize the generalization error of the
classifier under the null hypothesis that the underlying distribution is
nominal, with the constraint that the given hypothesis test has a false alarm
rate less than a fixed threshold. By incorporating a non-parametric regularizer
based on an empirical entropy estimator, we propose a
Geometric-Entropy-Minimization regularized Maximum Entropy Discrimination
(GEM-MED) method to perform classification and anomaly detection in a joint
manner. We demonstrate that our proposed method can yield improved performance
over previous robust classification methods in terms of both classification
accuracy and anomaly detection rate using simulated data and real footstep
data.