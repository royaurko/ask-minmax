We show that maximum entropy (maxent) models can be modeled with certain
kinds of HMMs, allowing us to construct maxent models with hidden variables,
hidden state sequences, or other characteristics. The models can be trained
using the forward-backward algorithm. While the results are primarily of
theoretical interest, unifying apparently unrelated concepts, we also give
experimental results for a maxent model with a hidden variable on a word
disambiguation task; the model outperforms standard techniques.