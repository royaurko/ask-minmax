We derive an exact representation of the topological effect on the dynamics
of sequence processing neural networks within signal-to-noise analysis. A new
network structure parameter, loopiness coefficient, is introduced to
quantitatively study the loop effect on network dynamics. The large loopiness
coefficient means the large probability of finding loops in the networks. We
develop the recursive equations for the overlap parameters of neural networks
in the term of the loopiness. It was found that the large loopiness increases
the correlations among the network states at different times, and eventually it
reduces the performance of neural networks. The theory is applied to several
network topological structures, including fully-connected, densely-connected
random, densely-connected regular, and densely-connected small-world, where
encouraging results are obtained.