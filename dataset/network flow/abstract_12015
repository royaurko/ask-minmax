We consider the dynamics of diluted neural networks with clipped and adapting
synapses. Unlike previous studies, the learning rate is kept constant as the
connectivity tends to infinity: the synapses evolve on a time scale
intermediate between the quenched and annealing limits and all orders of
synaptic correlations must be taken into account. The dynamics is solved by
mean-field theory, the order parameter for synapses being a function. We
describe the effects, in the double dynamics, due to synaptic correlations.