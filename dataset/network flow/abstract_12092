Feedforward neural networks with error backpropagation (FFBP) are widely
applied to pattern recognition. One general problem encountered with this type
of neural networks is the uncertainty, whether the minimization procedure has
converged to a global minimum of the cost function. To overcome this problem a
novel approach to minimize the error function is presented. It allows to
monitor the approach to the global minimum and as an outcome several
ambiguities related to the choice of free parameters of the minimization
procedure are removed.