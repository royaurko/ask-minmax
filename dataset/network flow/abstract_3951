This paper examines the memory capacity of generalized neural networks.
Hopfield networks trained with a variety of learning techniques are
investigated for their capacity both for binary and non-binary alphabets. It is
shown that the capacity can be much increased when multilevel inputs are used.
New learning strategies are proposed to increase Hopfield network capacity, and
the scalability of these methods is also examined in respect to size of the
network. The ability to recall entire patterns from stimulation of a single
neuron is examined for the increased capacity networks.