We consider the problem of distributed dictionary learning, where a set of
nodes is required to collectively learn a common dictionary from noisy
measurements. This approach may be useful in several contexts including sensor
networks. Diffusion cooperation schemes have been proposed to solve the
distributed linear regression problem. In this work we focus on a
diffusion-based adaptive dictionary learning strategy: each node records
observations and cooperates with its neighbors by sharing its local dictionary.
The resulting algorithm corresponds to a distributed block coordinate descent
(alternate optimization). Beyond dictionary learning, this strategy could be
adapted to many matrix factorization problems and generalized to various
settings. This article presents our approach and illustrates its efficiency on
some numerical examples.