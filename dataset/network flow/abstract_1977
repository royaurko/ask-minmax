In a physical neural system, where storage and processing are intimately
intertwined, the rules for adjusting the synaptic weights can only depend on
variables that are available locally, such as the activity of the pre- and
post-synaptic neurons, resulting in local learning rules. A systematic
framework for studying the space of local learning rules must first define the
nature of the local variables, and then the functional form that ties them
together into each learning rule. We consider polynomial local learning rules
and analyze their behavior and capabilities in both linear and non-linear
networks. As a byproduct, this framework enables also the discovery of new
learning rules as well as important relationships between learning rules and
group symmetries. Stacking local learning rules in deep feedforward networks
leads to deep local learning. While deep local learning can learn interesting
representations, it cannot learn complex input-output functions, even when
targets are available for the top layer. Learning complex input-output
functions requires local deep learning where target information is propagated
to the deep layers through a backward channel. The nature of the propagated
information about the targets, and the backward channel through which this
information is propagated, partition the space of learning algorithms. For any
learning algorithm, the capacity of the backward channel can be defined as the
number of bits provided about the gradient per weight, divided by the number of
required operations per weight. We estimate the capacity associated with
several learning algorithms and show that backpropagation outperforms them and
achieves the maximum possible capacity. The theory clarifies the concept of
Hebbian learning, what is learnable by Hebbian learning, and explains the
sparsity of the space of learning rules discovered so far.