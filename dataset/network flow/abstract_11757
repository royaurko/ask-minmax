The activity of a neural network is defined by patterns of spiking and
silence from the individual neurons. Because spikes are (relatively) sparse,
patterns of activity with increasing numbers of spikes are less probable, but
with more spikes the number of possible patterns increases. This tradeoff
between probability and numerosity is mathematically equivalent to the
relationship between entropy and energy in statistical physics. We construct
this relationship for populations of up to N=160 neurons in a small patch of
the vertebrate retina, using a combination of direct and model-based analyses
of experiments on the response of this network to naturalistic movies. We see
signs of a thermodynamic limit, where the entropy per neuron approaches a
smooth function of the energy per neuron as N increases. The form of this
function corresponds to the distribution of activity being poised near an
unusual kind of critical point. Networks with more or less correlation among
neurons would not reach this critical state. We suggest further tests of
criticality, and give a brief discussion of its functional significance.