The PC algorithm learns maximally oriented causal Bayesian networks. However,
there is no equivalent complete algorithm for learning the structure of
relational models, a more expressive generalization of Bayesian networks.
Recent developments in the theory and representation of relational models
support lifted reasoning about conditional independence. This enables a
powerful constraint for orienting bivariate dependencies and forms the basis of
a new algorithm for learning structure. We present the relational causal
discovery (RCD) algorithm that learns causal relational models. We prove that
RCD is sound and complete, and we present empirical results that demonstrate
effectiveness.