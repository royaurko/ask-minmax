Characterizing the capacity region for a network can be extremely difficult.
Even with independent sources, determining the capacity region can be as hard
as the open problem of characterizing all information inequalities. The
majority of computable outer bounds in the literature are relaxations of the
Linear Programming bound which involves entropy functions of random variables
related to the sources and link messages. When sources are not independent, the
problem is even more complicated. Extension of linear programming bounds to
networks with correlated sources is largely open. Source dependence is usually
specified via a joint probability distribution, and one of the main challenges
in extending linear programming bounds is the difficulty (or impossibility) of
characterizing arbitrary dependencies via entropy functions. This paper tackles
the problem by answering the question of how well entropy functions can
characterize correlation among sources. We show that by using carefully chosen
auxiliary random variables, the characterization can be fairly "accurate".