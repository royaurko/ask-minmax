We introduce a graceful approach to probabilistic inference called bounded
conditioning. Bounded conditioning monotonically refines the bounds on
posterior probabilities in a belief network with computation, and converges on
final probabilities of interest with the allocation of a complete resource
fraction. The approach allows a reasoner to exchange arbitrary quantities of
computational resource for incremental gains in inference quality. As such,
bounded conditioning holds promise as a useful inference technique for
reasoning under the general conditions of uncertain and varying reasoning
resources. The algorithm solves a probabilistic bounding problem in complex
belief networks by breaking the problem into a set of mutually exclusive,
tractable subproblems and ordering their solution by the expected effect that
each subproblem will have on the final answer. We introduce the algorithm,
discuss its characterization, and present its performance on several belief
networks, including a complex model for reasoning about problems in
intensive-care medicine.