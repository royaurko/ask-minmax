Bayesian network structures are usually built using only the data and
starting from an empty network or from a naive Bayes structure. Very often, in
some domains, like medicine, a prior structure knowledge is already known. This
structure can be automatically or manually refined in search for better
performance models. In this work, we take Bayesian networks built by
specialists and show that minor perturbations to this original network can
yield better classifiers with a very small computational cost, while
maintaining most of the intended meaning of the original model.