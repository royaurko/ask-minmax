We introduce and analyze a rigorous formulation of the dynamics of a signal
processing scheme that aims at dense scanning of large input signals. Recently
proposed methodologies lack a satisfactory discussion of whether they actually
produce the correct results according to their definition, especially in the
context of Convolutional Neural Networks. We improve on this through an exact
characterization of the requirements for a sound sliding window approach. The
tools developed in this paper are especially beneficial if Convolutional Neural
Networks are employed, but can also be used as a more general framework to
validate related approaches to signal scanning. The contributed theory helps to
eliminate redundant computations and renders special case treatment
unnecessary, resulting in a dramatic boost in efficiency particularly on
massively parallel processors.