The state space of a conventional Hopfield network typically exhibits many
different attractors of which only a small subset satisfy constraints between
neurons in a globally optimal fashion. It has recently been demonstrated that
combining Hebbian learning with occasional alterations of normal neural states
avoids this problem by means of self-organized enlargement of the best basins
of attraction. However, so far it is not clear to what extent this process of
self-optimization is also operative in real brains. Here we demonstrate that it
can be transferred to more biologically plausible neural networks by
implementing a self-optimizing spiking neural network model. In addition, by
using this spiking neural network to emulate a Hopfield network with Hebbian
learning, we attempt to make a connection between rate-based and temporal
coding based neural systems. Although further work is required to make this
model more realistic, it already suggests that the efficacy of the
self-optimizing process is independent from the simplifying assumptions of a
conventional Hopfield network. We also discuss natural and cultural processes
that could be responsible for occasional alteration of neural firing patterns
in actual brains