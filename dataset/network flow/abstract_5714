This paper focuses on structured-output learning using deep neural networks
for 3D human pose estimation from monocular images. Our network takes an image
and 3D pose as inputs and outputs a score value, which is high when the
image-pose pair matches and low otherwise. The network structure consists of a
convolutional neural network for image feature extraction, followed by two
sub-networks for transforming the image features and pose into a joint
embedding. The score function is then the dot-product between the image and
pose embeddings. The image-pose embedding and score function are jointly
trained using a maximum-margin cost function. Our proposed framework can be
interpreted as a special form of structured support vector machines where the
joint feature space is discriminatively learned using deep neural networks. We
test our framework on the Human3.6m dataset and obtain state-of-the-art results
compared to other recent methods. Finally, we present visualizations of the
image-pose embedding space, demonstrating the network has learned a high-level
embedding of body-orientation and pose-configuration.