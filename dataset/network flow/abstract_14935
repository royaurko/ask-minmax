Several scenarios of interacting neural networks which are trained either in
an identical or in a competitive way are solved analytically. In the case of
identical training each perceptron receives the output of its neighbour. The
symmetry of the stationary state as well as the sensitivity to the used
training algorithm are investigated. Two competitive perceptrons trained on
mutually exclusive learning aims and a perceptron which is trained on the
opposite of its own output are examined analytically. An ensemble of
competitive perceptrons is used as decision-making algorithms in a model of a
closed market (El Farol Bar problem or Minority Game); each network is trained
on the history of minority decisions. This ensemble of perceptrons relaxes to a
stationary state whose performance can be better than random.