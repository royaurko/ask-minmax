We study the limits of communication efficiency for function computation in
collocated networks within the framework of multi-terminal block source coding
theory. With the goal of computing a desired function of sources at a sink,
nodes interact with each other through a sequence of error-free, network-wide
broadcasts of finite-rate messages. For any function of independent sources, we
derive a computable characterization of the set of all feasible message coding
rates - the rate region - in terms of single-letter information measures. We
show that when computing symmetric functions of binary sources, the sink will
inevitably learn certain additional information which is not demanded in
computing the function. This conceptual understanding leads to new improved
bounds for the minimum sum-rate. The new bounds are shown to be orderwise
better than those based on cut-sets as the network scales. The scaling law of
the minimum sum-rate is explored for different classes of symmetric functions
and source parameters.