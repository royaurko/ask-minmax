We present a general information theoretic approach for identifying
functional subgraphs in complex networks where the dynamics of each node are
observable. We show that the uncertainty in the state of each node can be
expressed as a sum of information quantities involving a growing number of
correlated variables at other nodes. We demonstrate that each term in this sum
is generated by successively conditioning mutual informations on new measured
variables, in a way analogous to a discrete differential calculus. The analogy
to a Taylor series suggests efficient search algorithms for determining the
state of a target variable in terms of functional groups of other degrees of
freedom. We apply this methodology to electrophysiological recordings of
networks of cortical neurons grown it in vitro. Despite strong stochasticity,
we show that each cell's patterns of firing are generally explained by the
activity of a small number of other neurons. We identify these neuronal
subgraphs in terms of their mutually redundant or synergetic character and
reconstruct neuronal circuits that account for the state of each target cell.