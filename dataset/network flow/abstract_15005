Different scaling properties for the complexity of bidirectional
synchronization and unidirectional learning are essential for the security of
neural cryptography. Incrementing the synaptic depth of the networks increases
the synchronization time only polynomially, but the success of the geometric
attack is reduced exponentially and it clearly fails in the limit of infinite
synaptic depth. This method is improved by adding a genetic algorithm, which
selects the fittest neural networks. The probability of a successful genetic
attack is calculated for different model parameters using numerical
simulations. The results show that scaling laws observed in the case of other
attacks hold for the improved algorithm, too. The number of networks needed for
an effective attack grows exponentially with increasing synaptic depth. In
addition, finite-size effects caused by Hebbian and anti-Hebbian learning are
analyzed. These learning rules converge to the random walk rule if the synaptic
depth is small compared to the square root of the system size.