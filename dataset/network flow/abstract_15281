We study the problem of communicating a distributed correlated memoryless
source over a memoryless network, from source nodes to destination nodes, under
quadratic distortion constraints. We establish the following two complementary
results: (a) for an arbitrary memoryless network, among all distributed
memoryless sources of a given correlation, Gaussian sources are least
compressible, that is, they admit the smallest set of achievable distortion
tuples, and (b) for any memoryless source to be communicated over a memoryless
additive-noise network, among all noise processes of a given correlation,
Gaussian noise admits the smallest achievable set of distortion tuples. We
establish these results constructively by showing how schemes for the
corresponding Gaussian problems can be applied to achieve similar performance
for (source or noise) distributions that are not necessarily Gaussian but have
the same covariance.