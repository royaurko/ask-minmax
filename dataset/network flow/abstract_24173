In this paper, we compare the performance of two methods for estimating
Bayesian networks from data containing exogenous variables and random effects.
The first method is fully Bayesian in which a prior distribution is placed on
the exogenous variables, whereas the second method, which we call the residual
approach, accounts for the effects of exogenous variables by using the notion
of restricted maximum likelihood. We review the two score-based metrics, then
study their performance by measuring the Kullback Leibler divergence, or
distance, between the two resulting posterior density functions. The Kullback
Leibler divergence provides a natural framework for comparing distributions.
The residual approach is considerably simpler to apply in practice and we
demonstrate its utility both theoretically and via simulations. In particular,
in applications where the exogenous variables are not of primary interest, we
show that the potential loss of information about parameters and induced
components of correlation, is generally small.