We continue to explore the hypothesis that neuronal populations represent and
process analog variables in terms of probability density functions (PDFs). A
neural assembly encoding the joint probability density over relevant analog
variables can in principle answer any meaningful question about these variables
by implementing the Bayesian rules of inference. Aided by an intermediate
representation of the probability density based on orthogonal functions
spanning an underlying low-dimensional function space, we show how neural
circuits may be generated from Bayesian belief networks. The ideas and the
formalism of this PDF approach are illustrated and tested with several
elementary examples, and in particular through a problem in which model-driven
top-down information flow influences the processing of bottom-up sensory input.