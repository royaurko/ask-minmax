Adaptivity, both of the individual agents and of the interaction structure
among the agents, seems indispensable for scaling up multi-agent systems
(MAS's) in noisy environments. One important consideration in designing
adaptive agents is choosing their action spaces to be as amenable as possible
to machine learning techniques, especially to reinforcement learning (RL)
techniques. One important way to have the interaction structure connecting
agents itself be adaptive is to have the intentions and/or actions of the
agents be in the input spaces of the other agents, much as in Stackelberg
games. We consider both kinds of adaptivity in the design of a MAS to control
network packet routing.
  We demonstrate on the OPNET event-driven network simulator the perhaps
surprising fact that simply changing the action space of the agents to be
better suited to RL can result in very large improvements in their potential
performance: at their best settings, our learning-amenable router agents
achieve throughputs up to three and one half times better than that of the
standard Bellman-Ford routing algorithm, even when the Bellman-Ford protocol
traffic is maintained. We then demonstrate that much of that potential
improvement can be realized by having the agents learn their settings when the
agent interaction structure is itself adaptive.