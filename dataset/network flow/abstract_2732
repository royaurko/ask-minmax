We introduce a class of neural networks derived from probabilistic models in
the form of Bayesian belief networks. By imposing additional assumptions about
the nature of the probabilistic models represented in the belief networks, we
derive neural networks with standard dynamics that require no training to
determine the synaptic weights, that can pool multiple sources of evidence, and
that deal cleanly and consistently with inconsistent or contradictory evidence.
The presented neural networks capture many properties of Bayesian belief
networks, providing distributed versions of probabilistic models.