Countless learning tasks require dealing with sequential data. Image
captioning, speech synthesis, music generation, and video game playing all
require that a model generate sequences of outputs. In other domains, such as
time series prediction, video analysis, and music information retrieval, a
model must learn from sequences of inputs. Significantly more interactive
tasks, such as natural language translation, engaging in dialogue, and robotic
control, often demand both.
  Recurrent neural networks (RNNs) are a powerful family of connectionist
models that capture time dynamics via cycles in the graph. Unlike feedforward
neural networks, recurrent networks can process examples one at a time,
retaining a state, or memory, that reflects an arbitrarily long context window.
While these networks have long been difficult to train and often contain
millions of parameters, recent advances in network architectures, optimization
techniques, and parallel computation have enabled large-scale learning with
recurrent nets.
  Over the past few years, systems based on state of the art long short-term
memory (LSTM) and bidirectional recurrent neural network (BRNN) architectures
have demonstrated record-setting performance on tasks as varied as image
captioning, language translation, and handwriting recognition. In this review
of the literature we synthesize the body of research that over the past three
decades has yielded and reduced to practice these powerful models. When
appropriate, we reconcile conflicting notation and nomenclature. Our goal is to
provide a mostly self-contained explication of state of the art systems,
together with a historical perspective and ample references to the primary
research.