During a disaster scenario, situational awareness information, such as
location, physical status and images of the surrounding area, is essential for
minimizing loss of life, injury, and property damage. Today's handhelds make it
easy for people to gather data from within the disaster area in many formats,
including text, images and video. Studies show that the extreme anxiety induced
by disasters causes humans to create a substantial amount of repetitive and
redundant content. Transporting this content outside the disaster zone can be
problematic when the network infrastructure is disrupted by the disaster.
  This paper presents the design of a novel architecture called CARE
(Content-Aware Redundancy Elimination) for better utilizing network resources
in disaster-affected regions. Motivated by measurement-driven insights on
redundancy patterns found in real-world disaster area photos, we demonstrate
that CARE can detect the semantic similarity between photos in the networking
layer, thus reducing redundant transfers and improving buffer utilization.
Using DTN simulations, we explore the boundaries of the usefulness of deploying
CARE on a damaged network, and show that CARE can reduce packet delivery times
and drops, and enables 20-40% more unique information to reach the rescue teams
outside the disaster area than when CARE is not deployed.