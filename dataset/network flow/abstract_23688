A recent experiment suggests that neural circuits may alternatively implement
continuous or discrete attractors, depending on the training set up. In
recurrent neural network models, continuous and discrete attractors are
separately modeled by distinct forms of synaptic prescriptions (learning
rules). Here, we report a solvable network model, endowed with Hebbian synaptic
plasticity, which is able to learn either discrete or continuous attractors,
depending on the frequency of presentation of stimuli and on the structure of
sensory coding. A continuous attractor is learned when experience matches
sensory coding, i.e. when the distribution of experienced stimuli matches the
distribution of preferred stimuli of neurons. In that case, there is no
processing of sensory information and neural activity displays maximal entropy.
If experience goes beyond sensory coding, processing is initiated and the
continuous attractor is destabilized into a set of discrete attractors.