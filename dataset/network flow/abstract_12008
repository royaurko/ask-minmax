The information processing abilities of a multilayer neural network with a
number of hidden units scaling as the input dimension are studied using
statistical mechanics methods. The mapping from the input layer to the hidden
units is performed by general symmetric Boolean functions whereas the hidden
layer is connected to the output by either discrete or continuous couplings.
Introducing an overlap in the space of Boolean functions as order parameter the
storage capacity if found to scale with the logarithm of the number of
implementable Boolean functions. The generalization behaviour is smooth for
continuous couplings and shows a discontinuous transition to perfect
generalization for discrete ones.