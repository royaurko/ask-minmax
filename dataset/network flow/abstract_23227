We examine the long-standing cooling flow problem in galaxy clusters with 3D
MHD simulations of isolated clusters including radiative cooling and
anisotropic thermal conduction along magnetic field lines. The central regions
of the intracluster medium (ICM) can have cooling timescales of ~200 Myr or
shorter--in order to prevent a cooling catastrophe the ICM must be heated by
some mechanism such as AGN feedback or thermal conduction from the thermal
reservoir at large radii. The cores of galaxy clusters are linearly unstable to
the heat-flux-driven buoyancy instability (HBI), which significantly changes
the thermodynamics of the cluster core. The HBI is a convective,
buoyancy-driven instability that rearranges the magnetic field to be
preferentially perpendicular to the temperature gradient. For a wide range of
parameters, our simulations demonstrate that in the presence of the HBI, the
effective radial thermal conductivity is reduced to less than 10% of the full
Spitzer conductivity. With this suppression of conductive heating, the cooling
catastrophe occurs on a timescale comparable to the central cooling time of the
cluster. Thermal conduction alone is thus unlikely to stabilize clusters with
low central entropies and short central cooling timescales. High central
entropy clusters have sufficiently long cooling times that conduction can help
stave off the cooling catastrophe for cosmologically interesting timescales.