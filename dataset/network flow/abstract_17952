We present a statistic for the detection of stochastic gravitational wave
backgrounds (SGWBs) using radiometry with a network of multiple baselines. We
also quantitatively compare the sensitivities of existing baselines and their
network to SGWBs. We assess how the measurement accuracy of signal parameters,
e.g., the sky position of a localized source, can improve when using a network
of baselines, as compared to any of the single participating baselines. The
search statistic itself is derived from the likelihood ratio of the cross
correlation of the data across all possible baselines in a detector network and
is optimal in Gaussian noise. Specifically, it is the likelihood ratio
maximized over the strength of the SGWB, and is called the maximized-likelihood
ratio (MLR). One of the main advantages of using the MLR over past search
strategies for inferring the presence or absence of a signal is that the former
does not require the deconvolution of the cross correlation statistic.
Therefore, it does not suffer from errors inherent to the deconvolution
procedure and is especially useful for detecting weak sources. In the limit of
a single baseline, it reduces to the detection statistic studied by Ballmer
[Class. Quant. Grav. 23, S179 (2006)] and Mitra et al. [Phys. Rev. D 77, 042002
(2008)]. Unlike past studies, here the MLR statistic enables us to compare
quantitatively the performances of a variety of baselines searching for a SGWB
signal in (simulated) data. Although we use simulated noise and SGWB signals
for making these comparisons, our method can be straightforwardly applied on
real data.