Tree-structured neural networks encode a particular tree geometry for a
sentence in the network design. However, these models have at best only
slightly outperformed simpler sequence-based models. We hypothesize that neural
sequence models like LSTMs are in fact able to discover and implicitly use
recursive compositional structure, at least for tasks with clear cues to that
structure in the data. We demonstrate this possibility using an artificial data
task for which recursive compositional structure is crucial, and find that the
sequence model can learn the underlying patterning. The sequence model is
better in that it learns the value of tree structure from the data in an
emergent way, while the tree-structured model is better in being able to learn
with greater statistical efficiency due to its informative prior model
structure.