Approximate Bayesian inference is NP-hard. Dagum and Luby defined the Local
Variance Bound (LVB) to measure the approximation hardness of Bayesian
inference on Bayesian networks, assuming the networks model strictly positive
joint probability distributions, i.e. zero probabilities are not permitted.
This paper introduces the k-test to measure the approximation hardness of
inference on Bayesian networks with deterministic causalities in the
probability distribution, i.e. when zero conditional probabilities are
permitted. Approximation by stochastic sampling is a widely-used inference
method that is known to suffer from inefficiencies due to sample rejection. The
k-test predicts when rejection rates of stochastic sampling a Bayesian network
will be low, modest, high, or when sampling is intractable.