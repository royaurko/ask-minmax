Natural logic offers a powerful relational conception of meaning that is a
natural counterpart to distributed semantic representations, which have proven
valuable in a wide range of sophisticated language tasks. However, it remains
an open question whether it is possible to train distributed representations to
support the rich, diverse logical reasoning captured by natural logic. We
address this question using two neural network-based models for learning
embeddings: plain neural networks and neural tensor networks. Our experiments
evaluate the models' ability to learn the basic algebra of natural logic
relations from simulated data and from the WordNet noun graph. The overall
positive results are promising for the future of learned distributed
representations in the applied modeling of logical semantics.