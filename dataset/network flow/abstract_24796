Common Representation Learning (CRL), wherein different descriptions (or
views) of the data are embedded in a common subspace, is receiving a lot of
attention recently. Two popular paradigms here are Canonical Correlation
Analysis (CCA) based approaches and Autoencoder (AE) based approaches. CCA
based approaches learn a joint representation by maximizing correlation of the
views when projected to the common subspace. AE based methods learn a common
representation by minimizing the error of reconstructing the two views. Each of
these approaches has its own advantages and disadvantages. For example, while
CCA based approaches outperform AE based approaches for the task of transfer
learning, they are not as scalable as the latter. In this work we propose an AE
based approach called Correlational Neural Network (CorrNet), that explicitly
maximizes correlation among the views when projected to the common subspace.
Through a series of experiments, we demonstrate that the proposed CorrNet is
better than the above mentioned approaches with respect to its ability to learn
correlated common representations. Further, we employ CorrNet for two cross
language tasks and show that the representations learned using CorrNet perform
better than the ones learned using other state of the art approaches.