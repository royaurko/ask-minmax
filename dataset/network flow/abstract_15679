Training neural networks is a challenging non-convex optimization problem,
and backpropagation or gradient descent can get stuck in spurious local optima.
We propose a novel algorithm based on tensor decomposition for training a
two-layer neural network. We prove efficient risk bounds for our proposed
method, with a polynomial sample complexity in the relevant parameters, such as
input dimension and number of neurons. While learning arbitrary target
functions is NP-hard, we provide transparent conditions on the function and the
input for generalizability. Our training method is based on tensor
decomposition, which provably converges to the global optimum, under a set of
mild non-degeneracy conditions. It consists of simple embarrassingly parallel
linear and multi-linear operations, and is competitive with standard stochastic
gradient descent (SGD), in terms of computational complexity. Thus, we have a
computationally efficient method with guaranteed risk bounds for training
neural networks with general non-linear activations.