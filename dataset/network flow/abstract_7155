We formulate the data analysis problem for the detection of the Newtonian
coalescing-binary signal by a network of laser interferometric gravitational
wave detectors that have arbitrary orientations, but are located at the same
site. We use the maximum likelihood method for optimizing the detection
problem. We show that for networks comprising of up to three detectors, the
optimal statistic is essentially the magnitude of the network correlation
vector constructed from the matched network-filter. Alternatively, it is simply
a linear combination of the signal-to-noise ratios of the individual detectors.
This statistic, therefore, can be interpreted as the signal-to-noise ratio of
the network. The overall sensitivity of the network is shown to increase
roughly as the square-root of the number of detectors in the network. We
further show that these results continue to hold even for the restricted
post-Newtonian filters. Finally, our formalism is general enough to be extended
to address the problem of detection of such waves from other sources by some
other types of detectors, e.g., bars or spheres, or even by networks of
spatially well-separated detectors.