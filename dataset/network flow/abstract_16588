A toy model of a neural network in which both Hebbian learning and
reinforcement learning occur is studied. The problem of `path interference',
which makes that the neural net quickly forgets previously learned input-output
relations is tackled by adding a Hebbian term (proportional to the learning
rate $\eta$) to the reinforcement term (proportional to $\rho$) in the learning
rule. It is shown that the number of learning steps is reduced considerably if
$1/4 < \eta/\rho < 1/2$, i.e., if the Hebbian term is neither too small nor too
large compared to the reinforcement term.