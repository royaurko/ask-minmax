In this paper, we propose an extremely simple deep model for the unsupervised
nonlinear dimensionality reduction -- deep distributed random samplings, which
performs like a stack of unsupervised bootstrap aggregating. First, its network
structure is novel: each layer of the network is a group of mutually
independent $k$-centers clusterings. Second, its learning method is extremely
simple: the $k$ centers of each clustering are only $k$ randomly selected
examples from the training data; for small-scale data sets, the $k$ centers are
further randomly reconstructed by a simple cyclic-shift operation. Experimental
results on nonlinear dimensionality reduction show that the proposed method can
learn abstract representations on both large-scale and small-scale problems,
and meanwhile is much faster than deep neural networks on large-scale problems.