In this work we determine a Large Deviation Principle (LDP) for a model of
neurons interacting on a lattice Z^d. The neurons are subject to correlated
external noise, which is modelled as an infinite-dimensional stochastic
integral. The probability law governing the noise is strictly stationary, and
we are therefore able to find a LDP for the probability laws Pi^n governing the
ergodic empirical measure mu^n generated by the neurons in a cube of length
(2n+1) as n asymptotes to infinity. We use this LDP to determine an LDP for the
neural network model. The connection weights between the neurons evolve
according to a learning rule / neuronal plasticity, and these results are
adaptable to a large variety of specific types of neural network. This LDP is
of great use in the mathematical modelling of neural networks, because it
allows a quantification of the likelihood of the system deviating from its
limit, and also a determination of which direction the system is likely to
deviate. The work is also of interest because there are nontrivial correlations
between the neurons even in the asymptotic limit, thereby presenting itself as
a generalisation of traditional mean-field models.