The broad abundance of time series data, which is in sharp contrast to
limited knowledge of the underlying network dynamic processes that produce such
observations, calls for a rigorous and efficient method of causal network
inference. Here we develop mathematical theory of causation entropy, an
information-theoretic statistic designed for model-free causality inference.
For stationary Markov processes, we prove that for a given node in the network,
its causal parents forms the minimal set of nodes that maximizes causation
entropy, a result we refer to as the optimal causation entropy principle.
Furthermore, this principle guides us to develop computational and data
efficient algorithms for causal network inference based on a two-step discovery
and removal algorithm for time series data for a network-couple dynamical
system. Validation in terms of analytical and numerical results for Gaussian
processes on large random networks highlight that inference by our algorithm
outperforms previous leading methods including conditioned Granger causality
and transfer entropy. Interestingly, our numerical results suggest that the
number of samples required for accurate inference depends strongly on network
characteristics such as the density of links and information diffusion rate and
not necessarily on the number of nodes.