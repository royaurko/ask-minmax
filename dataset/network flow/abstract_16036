Information encoding in the nervous system is supported through the precise
spike-timings of neurons; however, an understanding of the underlying processes
by which such representations are formed in the first place remains unclear.
Here we examine how networks of spiking neurons can learn to encode for input
patterns using a fully temporal coding scheme. To this end, we introduce a
learning rule for spiking networks containing hidden neurons which optimizes
the likelihood of generating desired output spiking patterns. We show the
proposed learning rule allows for a large number of accurate input-output spike
pattern mappings to be learnt, which outperforms other existing learning rules
for spiking neural networks: both in the number of mappings that can be learnt
as well as the complexity of spike train encodings that can be utilised. The
learning rule is successful even in the presence of input noise, is
demonstrated to solve the linearly non-separable XOR computation and
generalizes well on an example dataset. We further present a biologically
plausible implementation of backpropagated learning in multilayer spiking
networks, and discuss the neural mechanisms that might underlie its function.
Our approach contributes both to a systematic understanding of how pattern
encodings might take place in the nervous system, and a learning rule that
displays strong technical capability.