Convolutional neural networks (convnets) work well on large datasets. But
labelled data is hard to collect, and in some applications larger amounts of
data are not available. The problem then is how to use convnets with small data
-- as convnets overfit quickly. We present an efficient Bayesian convnet,
offering better robustness to over-fitting on small data than traditional
approaches. This is by placing a probability distribution over the convnet's
kernels.
  To make this possible we present new theoretical results casting dropout
network training as approximate inference in Bayesian neural networks. This
allows us to implement our model using existing tools in the field with no
increase in time complexity. We approximate our model's intractable posterior
with Bernoulli variational distributions, requiring no additional model
parameters. We show a considerable improvement in classification accuracy
compared to standard techniques with state-of-the-art results on CIFAR-10.