Most infrastructure networks evolve and operate in a decentralized fashion,
which may adversely impact the allocation of resources across the system. Here
we investigate this question by focusing on the relation between capacity and
load in various such networks. We find that, due to network traffic
fluctuations, real systems tend to have larger unoccupied portions of the
capacities--smaller load-to-capacity ratios--on network elements with smaller
capacities, which contrasts with key assumptions involved in previous studies.
This finding suggests that infrastructure networks have evolved to minimize
local failures but not necessarily large-scale failures that can be caused by
the cascading spread of local damage.