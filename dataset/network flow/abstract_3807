A stationary state replica analysis for a dual neural network model that
interpolates between a fully recurrent symmetric attractor network and a
strictly feed-forward layered network, studied by Coolen and Viana, is extended
in this work to account for finite dilution of the recurrent Hebbian
interactions between binary Ising units within each layer. Gradual dilution is
found to suppress part of the phase transitions that arise from the competition
between recurrent and feed-forward operation modes of the network. Despite
that, a long chain of layers still exhibits a relatively good performance under
finite dilution for a balanced ratio between inter-layer and intra-layer
interactions.