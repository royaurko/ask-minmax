We consider the fundamental problem of managing a bounded size queue buffer
where traffic consists of packets of varying size, where each packet requires
several rounds of processing before it can be transmitted from the queue
buffer. The goal in such an environment is to maximize the overall size of
packets that are successfully transmitted. This model is motivated by the
ever-growing ubiquity of network processors architectures, which must deal with
heterogeneously-sized traffic, with heterogeneous processing requirements. Our
work addresses the tension between two conflicting algorithmic approaches in
such settings: the tendency to favor packets with fewer processing
requirements, thus leading to fast contributions to the accumulated throughput,
as opposed to preferring packets of larger size, which imply a large increase
in throughput at each step. We present a model for studying such systems, and
present competitive algorithms whose performance depend on the maximum size a
packet may have, and maximum amount of processing a packet may require. We
further provide lower bounds on algorithms performance in such settings.