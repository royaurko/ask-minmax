Network traffic is difficult to monitor and analyze, especially in
high-bandwidth networks. Performance analysis, in particular, presents extreme
complexity and scalability challenges. GPU (Graphics Processing Unit)
technology has been utilized recently to accelerate general purpose scientific
and engineering computing. GPUs offer extreme thread-level parallelism with
hundreds of simple cores. Their data-parallel execution model can rapidly solve
large problems with inherent data parallelism. At Fermilab, we have prototyped
a GPU-accelerated network performance monitoring system, called G-NetMon, to
support large-scale scientific collaborations. In this work, we explore new
opportunities in network traffic monitoring and analysis with GPUs. Our system
exploits the data parallelism that exists within network flow data to provide
fast analysis of bulk data movement between Fermilab and collaboration sites.
Experiments demonstrate that our G-NetMon can rapidly detect sub-optimal bulk
data movements.