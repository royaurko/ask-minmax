We derive information-theoretic lower bounds on the minimum time required by
any scheme for distributed function computation over a network of
point-to-point channels with finite capacity to achieve a given accuracy with a
given probability. The main contributions include: 1) A lower bound on
conditional mutual information via so-called small ball probabilities, which
captures the influence on the computation time of the joint distribution of the
initial observations at the nodes, the structure of the function, and the
accuracy requirement. For linear functions, the small ball probability can be
expressed in terms of L\'evy concentration functions of sums of independent
random variables, for which tight estimates are available that lead to strict
improvements over existing results. 2) An upper bound on conditional mutual
information via strong data processing inequalities, which complements and
strengthens the cutset-capacity upper bounds from the literature. For discrete
observations, it can lead to much tighter lower bounds on computation time with
respect to the confidence of the computation results. 3) A multi-cutset
analysis that quantifies the dissipation of information as it flows across a
succession of cutsets in the network. This analysis is based on reducing a
general network to a bidirected chain, and the results highlight the dependence
of the computation time on the diameter of the network, a fundamental parameter
that is missing from most of the existing results.