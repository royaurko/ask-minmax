The inherent intractability of probabilistic inference has hindered the
application of belief networks to large domains. Noisy OR-gates [30] and
probabilistic similarity networks [18, 17] escape the complexity of inference
by restricting model expressiveness. Recent work in the application of
belief-network models to time-series analysis and forecasting [9, 10] has given
rise to the additive belief network model (ABNM). We (1) discuss the nature and
implications of the approximations made by an additive decomposition of a
belief network, (2) show greater efficiency in the induction of additive models
when available data are scarce, (3) generalize probabilistic inference
algorithms to exploit the additive decomposition of ABNMs, (4) show greater
efficiency of inference, and (5) compare results on inference with a simple
additive belief network.