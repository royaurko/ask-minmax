In decentralized consensus optimization, a connected network of agents
collaboratively minimize the sum of their local objective functions over a
common decision variable, where their information exchange is restricted
between the neighbors. To this end, one can first obtain a problem
reformulation and then apply the alternating direction method of multipliers
(ADMM). The method applies iterative computation at the individual agents and
information exchange between the neighbors. This approach has been observed to
converge quickly and deemed powerful. This paper establishes its linear
convergence rate for decentralized consensus optimization problem with strongly
convex local objective functions. The theoretical convergence rate is
explicitly given in terms of the network topology, the properties of local
objective functions, and the algorithm parameter. This result is not only a
performance guarantee but also a guideline toward accelerating the ADMM
convergence.