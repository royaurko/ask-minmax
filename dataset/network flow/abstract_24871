The data-processing inequality, that is, $I(U;Y) \le I(U;X)$ for a Markov
chain $U \to X \to Y$, has been the method of choice for proving impossibility
(converse) results in information theory and many other disciplines. Various
channel-dependent strengthenings of this inequality have been proposed both
classically and more recently. This note considers the basic
information-theoretic question: given strong data-processing inequality (SDPI)
for each constituent channel in a Bayesian network, how does one produce an
end-to-end SDPI?
  Our approach is based on the (extract of the) Evans-Schulman method, which is
demonstrated for three different kinds of SDPIs, namely, the usual
Ahslwede-G\'acs type contraction coefficients (mutual information), Dobrushin's
contraction coefficients (total variation), and finally the $F_I$-curve (the
best possible SDPI for a given channel). As an example, it is shown how to
obtain SDPI for an $n$-letter memoryless channel with feedback given an SDPI
for $n=1$.