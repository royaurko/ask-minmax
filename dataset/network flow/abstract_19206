Deep learning has gained much success in sentence-level relation
classification. For example, convolutional neural networks (CNN) have delivered
state-of-the-art performance without much effort on feature engineering as the
conventional pattern-based methods. A key issue that has not been well
addressed by the existing research is the lack of capability to learn %in
modeling temporal features, especially long-distance dependency between nominal
pairs. In this paper, we propose a novel framework based on recurrent neural
networks (RNN) to tackle the problem, and present several modifications to
enhance the model, including a max-pooling approach and a bi-directional
architecture. Our experiment on the SemEval-2010 Task-8 dataset shows that the
RNN model can deliver state-of-the-art performance on relation classification,
and it is particularly capable of learning long-distance relation patterns.
This makes it suitable for real-world applications where complicated
expressions are often involved.