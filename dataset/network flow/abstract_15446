We introduce concepts and tools at the intersection of information theory and
network biology. We show that Shannon's information entropy, compressibility
and algorithmic probability quantify different aspects of synthetic and
biological networks at the intersection of local and global pattern detection,
including, the detection of the connectivity phase transition leading to the
emergence of giant components in Erd\"os-R\'enyi random graphs, and the
recovery of topological properties from numerical kinetic properties simulating
gene expression arrays. We provide exact theoretical calculations, numerical
approximations and error estimations of entropy, algorithmic probability and
Kolmogorov complexity for different types of graphs characterizing their
variant and invariant properties. We introduce formal definitions of complexity
for both labelled and unlabelled graphs and prove that the Kolmogorov
complexity of a labelled graph is a good approximation of the Kolmogorov
complexity of the unlabelled graph.