Nodes in real-world networks are repeatedly observed to form dense clusters,
often referred to as communities. Methods to detect these groups of nodes
usually maximize an objective function, which implicitly contains the
definition of a community. We here analyze a recently proposed measure called
Surprise, which assesses the quality of the partition of a network into
communities. Given that, in its current form, its formulation is rather
difficult to analyze, we develop an accurate asymptotic approximation. This
allows for the development of an efficient algorithm for optimizing Surprise.
Incidentally, this leads to a straightforward extension of Surprise to weighted
graphs. Finally, we analytically compare it to previous methods, which makes
clear that Surprise is more discriminative than ER Modularity. Furthermore, we
show that it is especially suited for detecting relatively small communities in
large graphs, an area where some earlier methods fail.