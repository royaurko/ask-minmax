Recent experimental and computational evidence suggests that several
dynamical properties may characterize the operating point of functioning neural
networks: critical branching, neutral stability, and production of a wide range
of firing patterns. We seek the simplest setting in which these properties
emerge, clarifying their origin and relationship in random, feedforward
networks of McCullochs-Pitts neurons. Two key parameters are the thresholds at
which neurons fire spikes, and the overall level of feedforward connectivity.
When neurons have low thresholds, we show that there is always a connectivity
for which the properties in question all occur: that is, these networks
preserve overall firing rates from layer to layer and produce broad
distributions of activity in each layer. This fails to occur, however, when
neurons have high thresholds. A key tool in explaining this difference is
eigenstructure of the resulting mean-field Markov chain, as this reveals which
activity modes will be preserved from layer to layer. We extend our analysis
from purely excitatory networks to more complex models that include inhibition
and 'local' noise, and find that both of these features extend the parameter
ranges over which networks produce the properties of interest.