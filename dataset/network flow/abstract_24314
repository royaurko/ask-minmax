Synaptic plasticity seems to be a capital aspect of the dynamics of neural
networks. It is about the physiological modifications of the synapse, which
have like consequence a variation of the value of the synaptic weight. The
information encoding is based on the precise timing of single spike events that
is based on the relative timing of the pre- and post-synaptic spikes, local
synapse competitions within a single neuron and global competition via lateral
connections. In order to classify temporal sequences, we present in this paper
how to use a local hebbian learning, spike-timing dependent plasticity for
unsupervised competitive learning, preserving self-organizing maps of spiking
neurons. In fact we present three variants of self-organizing maps (SOM) with
spike-timing dependent Hebbian learning rule, the Leaky Integrators Neurons
(LIN), the Spiking_SOM and the recurrent Spiking_SOM (RSSOM) models. The case
study of the proposed SOM variants is phoneme classification and word
recognition in continuous speech and speaker independent.