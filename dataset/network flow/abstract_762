The time evolution of an exactly solvable layered feedforward neural network
with three-state neurons and optimizing the mutual information is studied for
arbitrary synaptic noise (temperature). Detailed stationary
temperature-capacity and capacity-activity phase diagrams are obtained. The
model exhibits pattern retrieval, pattern-fluctuation retrieval and spin-glass
phases. It is found that there is an improved performance in the form of both a
larger critical capacity and information content compared with three-state
Ising-type layered network models. Flow diagrams reveal that saddle-point
solutions associated with fluctuation overlaps slow down considerably the flow
of the network states towards the stable fixed-points.