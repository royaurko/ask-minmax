In the past few years, lossy compression has been widely applied in the field
of wireless sensor networks (WSN), where energy efficiency is a crucial concern
due to the constrained nature of the transmission devices. Often, the common
thinking among researchers and implementers is that compression is always a
good choice, because the major source of energy consumption in a sensor node
comes from the transmission of the data. Lossy compression is deemed a viable
solution as the imperfect reconstruction of the signal is often acceptable in
WSN. In this paper, we thoroughly review a number of lossy compression methods
from the literature, and analyze their performance in terms of compression
efficiency, computational complexity and energy consumption. We consider two
different scenarios, namely, wireless and underwater communications, and show
that signal compression may or may not help in the reduction of the overall
energy consumption, depending on factors such as the compression algorithm, the
signal statistics and the hardware characteristics, i.e., micro-controller and
transmission technology. The lesson that we have learned, is that signal
compression may in fact provide some energy savings. However, its usage should
be carefully evaluated, as in quite a few cases processing and transmission
costs are of the same order of magnitude, whereas, in some other cases, the
former may even dominate the latter. In this paper, we show quantitative
comparisons to assess these tradeoffs in the above mentioned scenarios.
Finally, we provide formulas, obtained through numerical fittings, to gauge
computational complexity, overall energy consumption and signal representation
accuracy for the best performing algorithms as a function of the most relevant
system parameters.