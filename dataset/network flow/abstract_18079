In this paper we try to model certain features of human language complexity
by means of advanced concepts borrowed from statistical mechanics. We use a
time series approach, the diffusion entropy method (DE), to compute the
complexity of an italian corpus of newspapers and magazines. We find that the
anomalous scaling index is compatible with a simple dynamical model, a random
walk on a complex scale-free network, which is linguistically related to
Saussurre's paradigms. The network complexity is independently measured on the
same corpus, looking at the co-occurrence of nouns and verbs. This connection
of cognitive complexity with long-range time correlations also provides an
explanation for the famous Zipf's law in terms of the generalized central limit
theorem.