We describe a mechanism for biological learning and adaptation based on two
simple principles: (I) Neuronal activity propagates only through the network's
strongest synaptic connections (extremal dynamics), and (II) The strengths of
active synapses are reduced if mistakes are made, otherwise no changes occur
(negative feedback). The balancing of those two tendencies typically shapes a
synaptic landscape with configurations which are barely stable, and therefore
highly flexible. This allows for swift adaptation to new situations.
Recollection of past successes is achieved by punishing synapses which have
once participated in activity associated with successful outputs much less than
neurons that have never been successful. Despite its simplicity, the model can
readily learn to solve complicated nonlinear tasks, even in the presence of
noise. In particular, the learning time for the benchmark parity problem scales
algebraically with the problem size N, with an exponent $k\sim 1.4$.