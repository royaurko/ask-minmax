In this paper, we study optimal radar deployment for intrusion detection,
with focus on network coverage. In contrast to the disk-based sensing model in
a traditional sensor network, the detection range of a bistatic radar depends
on the locations of both the radar transmitter and radar receiver, and is
characterized by Cassini ovals. Furthermore, in a network with multiple radar
transmitters and receivers, since any pair of transmitter and receiver can
potentially form a bistatic radar, the detection ranges of different bistatic
radars are coupled and the corresponding network coverage is intimately related
to the locations of all transmitters and receivers, making the optimal
deployment design highly non-trivial. Clearly, the detectability of an intruder
depends on the highest SNR received by all possible bistatic radars. We focus
on the worst-case intrusion detectability, i.e., the minimum possible
detectability along all possible intrusion paths. Although it is plausible to
deploy radars on a shortest line segment across the field, it is not always
optimal in general, which we illustrate via counter-examples. We then present a
sufficient condition on the field geometry for the optimality of shortest line
deployment to hold. Further, we quantify the local structure of detectability
corresponding to a given deployment order and spacings of radar transmitters
and receivers, building on which we characterize the optimal deployment to
maximize the worst-case intrusion detectability. Our results show that the
optimal deployment locations exhibit a balanced structure. We also develop a
polynomial-time approximation algorithm for characterizing the worse-case
intrusion path for any given locations of radars under random deployment.