Consider the problem of minimizing the expected value of a cost function
parameterized by a random variable. The classical sample average approximation
(SAA) method for solving this problem requires minimization of an ensemble
average of the objective at each step, which can be expensive. In this paper,
we propose a stochastic successive upper-bound minimization method (SSUM) which
minimizes an approximate ensemble average at each iteration. To ensure
convergence and to facilitate computation, we require the approximate ensemble
average to be a locally tight upper-bound of the expected cost function and be
easily optimized. The main contributions of this work include the development
and analysis of the SSUM method as well as its applications in linear
transceiver design for wireless communication networks and online dictionary
learning. Moreover, using the SSUM framework, we extend the classical
stochastic (sub-)gradient (SG) method to the case of minimizing a nonsmooth
nonconvex objective function and establish its convergence.