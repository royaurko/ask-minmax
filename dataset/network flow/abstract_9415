We present asymptotic and finite-sample results on the use of stochastic
blockmodels for the analysis of network data. We show that the fraction of
misclassified network nodes converges in probability to zero under maximum
likelihood fitting when the number of classes is allowed to grow as the root of
the network size and the average network degree grows at least
poly-logarithmically in this size. We also establish finite-sample confidence
bounds on maximum-likelihood blockmodel parameter estimates from data
comprising independent Bernoulli random variates; these results hold uniformly
over class assignment. We provide simulations verifying the conditions
sufficient for our results, and conclude by fitting a logit parameterization of
a stochastic blockmodel with covariates to a network data example comprising a
collection of Facebook profiles, resulting in block estimates that reveal
residual structure.