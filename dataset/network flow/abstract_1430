In an unreliable single-hop broadcast network setting, we investigate the
throughput and decoding-delay performance of random linear network coding as a
function of the coding window size and the network size. Our model consists of
a source transmitting packets of a single flow to a set of $n$ users over
independent erasure channels. The source performs random linear network coding
(RLNC) over $k$ (coding window size) packets and broadcasts them to the users.
We note that the broadcast throughput of RLNC must vanish with increasing $n$,
for any fixed $k.$ Hence, in contrast to other works in the literature, we
investigate how the coding window size $k$ must scale for increasing $n$. Our
analysis reveals that the coding window size of $\Theta(\ln(n))$ represents a
phase transition rate, below which the throughput converges to zero, and above
which it converges to the broadcast capacity. Further, we characterize the
asymptotic distribution of decoding delay and provide approximate expressions
for the mean and variance of decoding delay for the scaling regime of
$k=\omega(\ln(n)).$ These asymptotic expressions reveal the impact of channel
correlations on the throughput and delay performance of RLNC. We also show how
our analysis can be extended to other rateless block coding schemes such as the
LT codes. Finally, we comment on the extension of our results to the cases of
dependent channels across users and asymmetric channel model.