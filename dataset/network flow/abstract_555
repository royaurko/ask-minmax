The Hopfield recurrent neural network is a classical auto-associative model
of memory, in which collections of symmetrically-coupled McCulloch-Pitts
neurons interact to perform emergent computation. Although previous researchers
have explored the potential of this network to solve combinatorial optimization
problems and store memories as attractors of its deterministic dynamics, a
basic open problem is to design a family of Hopfield networks with a number of
noise-tolerant memories that grows exponentially with neural population size.
Here, we discover such networks by minimizing probability flow, a recently
proposed objective for estimating parameters in discrete maximum entropy
models. By descending the gradient of the convex probability flow, our networks
adapt synaptic weights to achieve robust exponential storage, even when
presented with vanishingly small numbers of training patterns. In addition to
providing a new set of error-correcting codes that achieve Shannon's channel
capacity bound, these networks also efficiently solve a variant of the hidden
clique problem in computer science, opening new avenues for real-world
applications of computational models originating from biology.