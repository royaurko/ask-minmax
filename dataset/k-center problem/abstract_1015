A classical condition for fast learning rates is the margin condition, first
introduced by Mammen and Tsybakov. We tackle in this paper the problem of
adaptivity to this condition in the context of model selection, in a general
learning framework. Actually, we consider a weaker version of this condition
that allows one to take into account that learning within a small model can be
much easier than within a large one. Requiring this "strong margin adaptivity"
makes the model selection problem more challenging. We first prove, in a
general framework, that some penalization procedures (including local
Rademacher complexities) exhibit this adaptivity when the models are nested.
Contrary to previous results, this holds with penalties that only depend on the
data. Our second main result is that strong margin adaptivity is not always
possible when the models are not nested: for every model selection procedure
(even a randomized one), there is a problem for which it does not demonstrate
strong margin adaptivity.