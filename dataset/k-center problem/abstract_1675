We consider a general model for representing and manipulating parametric
curves, in which a curve is specified by a black box mapping a parameter value
between 0 and 1 to a point in Euclidean d-space. In this model, we consider the
nearest-point-on-curve and farthest-point-on-curve problems: given a curve C
and a point p, find a point on C nearest to p or farthest from p. In the
general black-box model, no algorithm can solve these problems. Assuming a
known bound on the speed of the curve (a Lipschitz condition), the answer can
be estimated up to an additive error of epsilon using O(1/epsilon) samples, and
this bound is tight in the worst case. However, many instances can be solved
with substantially fewer samples, and we give algorithms that adapt to the
inherent difficulty of the particular instance, up to a logarithmic factor.
More precisely, if OPT(C,p,epsilon) is the minimum number of samples of C that
every correct algorithm must perform to achieve tolerance epsilon, then our
algorithm performs O(OPT(C,p,epsilon) log (epsilon^(-1)/OPT(C,p,epsilon)))
samples. Furthermore, any algorithm requires Omega(k log (epsilon^(-1)/k))
samples for some instance C' with OPT(C',p,epsilon) = k; except that, for the
nearest-point-on-curve problem when the distance between C and p is less than
epsilon, OPT is 1 but the upper and lower bounds on the number of samples are
both Theta(1/epsilon). When bounds on relative error are desired, we give
algorithms that perform O(OPT log (2+(1+epsilon^(-1)) m^(-1)/OPT)) samples
(where m is the exact minimum or maximum distance from p to C) and prove that
Omega(OPT log (1/epsilon)) samples are necessary on some problem instances.