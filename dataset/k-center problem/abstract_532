We consider the group lasso penalty for the linear model. We note that the
standard algorithm for solving the problem assumes that the model matrices in
each group are orthonormal. Here we consider a more general penalty that blends
the lasso (L1) with the group lasso ("two-norm"). This penalty yields solutions
that are sparse at both the group and individual feature levels. We derive an
efficient algorithm for the resulting convex problem based on coordinate
descent. This algorithm can also be used to solve the general form of the group
lasso, with non-orthonormal model matrices.