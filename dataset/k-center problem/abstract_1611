A large class of problems in statistical genetics amounts to finding a sparse
linear effect in a binary classification setup, such as finding a small set of
genes that most strongly predict a disease. Very often, these signals are
spurious and obfuscated by confounders such as age, ethnicity or population
structure. In the probit regression model, such confounding can be modeled in
terms of correlated label noise, but poses mathematical challenges for learning
algorithms. In this paper we propose a learning algorithm to overcome these
problems. We manage to learn sparse signals that are less influenced by the
correlated noise. This problem setup generalizes to fields outside statistical
genetics.
  Our method can be understood as a hybrid between an $\ell_1$ regularized
probit classifier and a Gaussian Process (GP) classifier. In addition to a
latent GP to capture correlated noise, the model captures sparse signals in a
linear effect. Because the observed labels can be explained in part by the
correlated noise, the linear effect will try to learn signals that capture
information beyond just correlated noise. As we show on real-world data,
signals found by our model are less correlated with the top confounders. Hence,
we can find effects closer to the unconfounded sparse effects we are aiming to
detect. Besides that, we show that our method outperforms Gaussian process
classification and uncorrelated probit regression in terms of prediction
accuracy.