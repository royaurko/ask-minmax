The solution of linear inverse problems when the unknown parameters outnumber
data requires addressing the problem of a nontrivial null space. After
restating the problem within the Bayesian framework, a priori information about
the unknown can be utilized for determining the null space contribution to the
solution. More specifically, if the solution of the associated linear system is
computed by the Conjugate Gradient for Least Squares (CGLS) method, the
additional information can be encoded in the form of a right preconditioner. In
this paper we study how the right preconditioned changes the Krylov subspaces
where the CGLS iterates live, and draw a tighter connection between Bayesian
inference and Krylov subspace methods. The advantages of a Krylov-meet-Bayes
approach to the solution of underdetermined linear inverse problems is
illustrated with computed examples.