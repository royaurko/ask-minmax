Via a simulation study we compare the finite sample performance of the
deconvolution kernel density estimator in the supersmooth deconvolution problem
to its asymptotic behaviour predicted by two asymptotic normality theorems. Our
results indicate that for lower noise levels and moderate sample sizes the
match between the asymptotic theory and the finite sample performance of the
estimator is not satisfactory. On the other hand we show that the two
approaches produce reasonably close results for higher noise levels. These
observations in turn provide additional motivation for the study of
deconvolution problems under the assumption that the error term variance
$\sigma^2\to 0$ as the sample size $n\to\infty.$