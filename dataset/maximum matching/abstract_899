In this work, we propose a theory for information matching. It is motivated
by the observation that retrieval is about the relevance matching between two
sets of properties (features), namely, the information need representation and
information item representation. However, many probabilistic retrieval models
rely on fixing one representation and optimizing the other (e.g. fixing the
single information need and tuning the document) but not both. Therefore, it is
difficult to use the available related information on both the document and the
query at the same time in calculating the probability of relevance. In this
paper, we address the problem by hypothesizing the relevance as a logical
relationship between the two sets of properties; the relationship is defined on
two separate mappings between these properties. By using the hypothesis we
develop a unified probabilistic relevance model which is capable of using all
the available information. We validate the proposed theory by formulating and
developing probabilistic relevance ranking functions for both ad-hoc text
retrieval and collaborative filtering. Our derivation in text retrieval
illustrates the use of the theory in the situation where no relevance
information is available. In collaborative filtering, we show that the
resulting recommender model unifies the user and item information into a
relevance ranking function without applying any dimensionality reduction
techniques or computing explicit similarity between two different users (or
items), in contrast to the state-of-the-art recommender models.