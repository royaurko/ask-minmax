The Lasso is an attractive technique for regularization and variable
selection for high-dimensional data, where the number of predictor variables
$p_n$ is potentially much larger than the number of samples $n$. However, it
was recently discovered that the sparsity pattern of the Lasso estimator can
only be asymptotically identical to the true sparsity pattern if the design
matrix satisfies the so-called irrepresentable condition. The latter condition
can easily be violated in the presence of highly correlated variables. Here we
examine the behavior of the Lasso estimators if the irrepresentable condition
is relaxed. Even though the Lasso cannot recover the correct sparsity pattern,
we show that the estimator is still consistent in the $\ell_2$-norm sense for
fixed designs under conditions on (a) the number $s_n$ of nonzero components of
the vector $\beta_n$ and (b) the minimal singular values of design matrices
that are induced by selecting small subsets of variables. Furthermore, a rate
of convergence result is obtained on the $\ell_2$ error with an appropriate
choice of the smoothing parameter. The rate is shown to be optimal under the
condition of bounded maximal and minimal sparse eigenvalues. Our results imply
that, with high probability, all important variables are selected. The set of
selected variables is a meaningful reduction on the original set of variables.
Finally, our results are illustrated with the detection of closely adjacent
frequencies, a problem encountered in astrophysics.