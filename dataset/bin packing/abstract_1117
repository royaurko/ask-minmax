The effect of our Galaxy's motion through the Cosmic Microwave Background
rest frame, which aberrates and Doppler shifts incoming photons measured by
current CMB experiments, has been shown to produce mode-mixing in the multipole
space temperature coefficients. However, multipole space determinations are
subject to many difficulties, and a real-space analysis can provide a
straightforward alternative. In this work we describe a numerical method for
removing Lorentz- boost effects from real-space temperature maps. We show that
to deboost a map so that one can accurately extract the temperature power
spectrum requires calculating the boost kernel at a finer pixelization than one
might naively expect. In idealized cases that allow for easy comparison to
analytic results, we have confirmed that there is indeed mode mixing among the
spherical harmonic coefficients of the temperature. We find that using a boost
kernel calculated at Nside=8192 leads to a 1% bias in the binned boosted power
spectrum at l~2000, while individual Cls exhibit ~5% fluctuations around the
binned average. However, this bias is dominated by pixelization effects and not
the aberration and Doppler shift of CMB photons that causes the fluctuations.
Performing analysis on maps with galactic cuts does not induce any additional
error in the boosted, binned power spectra over the full sky analysis. For
multipoles that are free of resolution effects, there is no detectable
deviation between the binned boosted and unboosted spectra. This result arises
because the power spectrum is a slowly varying function of and does not show
that, in general, Lorentz boosts can be neglected for other cosmological
quantities such as polarization maps or higher-point functions.