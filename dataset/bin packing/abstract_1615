Distributed source coding is traditionally viewed in the block coding context
-- all the source symbols are known in advance at the encoders. This paper
instead considers a streaming setting in which iid source symbol pairs are
revealed to the separate encoders in real time and need to be reconstructed at
the decoder with some tolerable end-to-end delay using finite rate noiseless
channels. A sequential random binning argument is used to derive a lower bound
on the error exponent with delay and show that both ML decoding and universal
decoding achieve the same positive error exponents inside the traditional
Slepian-Wolf rate region. The error events are different from the block-coding
error events and give rise to slightly different exponents. Because the
sequential random binning scheme is also universal over delays, the resulting
code eventually reconstructs every source symbol correctly with probability 1.