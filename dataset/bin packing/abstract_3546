The deterministic notions of capacity and entropy are studied in the context
of communication and storage of information using square-integrable,
bandlimited signals subject to perturbation. The $(\epsilon,\delta)$-capacity,
that extends the Kolmogorov $\epsilon$-capacity to packing sets of overlap at
most $\delta$, is introduced and compared to the Shannon capacity. The
functional form of the results indicates that in both Kolmogorov and Shannon's
settings, capacity and entropy grow linearly with the number of degrees of
freedom, but only logarithmically with the signal to noise ratio. This basic
insight transcends the details of the stochastic or deterministic description
of the information-theoretic model. For $\delta=0$, the analysis leads to new
bounds on the Kolmogorov $\epsilon$-capacity, and to a tight asymptotic
expression of the Kolmogorov $\epsilon$-entropy of bandlimited signals. A
deterministic notion of error exponent is introduced. Applications of the
theory are briefly discussed.