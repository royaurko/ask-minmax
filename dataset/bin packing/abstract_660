With the rapid progress in metallicity gradient studies at high-redshift, it
is imperative that we thoroughly understand the systematics in these
measurements. This work investigates how the [NII]/Halpha ratio based
metallicity gradients change with angular resolution, signal-to-noise (S/N),
and annular binning parameters. Two approaches are used: 1. We downgrade the
high angular resolution integral-field data of a gravitationally lensed galaxy
and re-derive the metallicity gradients at different angular resolution; 2. We
simulate high-redshift integral field spectroscopy (IFS) observations under
different angular resolution and S/N conditions using a local galaxy with a
known gradient. We find that the measured metallicity gradient changes
systematically with angular resolution and annular binning. Seeing-limited
observations produce significantly flatter gradients than higher angular
resolution observations. There is a critical angular resolution limit beyond
which the measured metallicity gradient is substantially different to the
intrinsic gradient. This critical angular resolution depends on the intrinsic
gradient of the galaxy and is < 0.02 arcsec for our simulated galaxy. We show
that seeing-limited high-redshift metallicity gradients are likely to be
strongly affected by resolution-driven gradient flattening. Annular binning
with a small number of annuli produces a more flattened gradient than the
intrinsic gradient due to weak line smearing. For 3-annuli bins, a minimum S/N
of ~ 5 on the [NII] line is required for the faintest annulus to constrain the
gradients with meaningful errors.