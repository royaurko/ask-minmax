A well-known technique in estimating probabilities of rare events in general
and in information theory in particular (used, e.g., in the sphere-packing
bound), is that of finding a reference probability measure under which the
event of interest has probability of order one and estimating the probability
in question by means of the Kullback-Leibler divergence. A method has recently
been proposed in [2], that can be viewed as an extension of this idea in which
the probability under the reference measure may itself be decaying
exponentially, and the Renyi divergence is used instead. The purpose of this
paper is to demonstrate the usefulness of this approach in various
information-theoretic settings. For the problem of channel coding, we provide a
general methodology for obtaining matched, mismatched and robust error exponent
bounds, as well as new results in a variety of particular channel models. Other
applications we address include rate-distortion coding and the problem of
guessing.