Workload consolidation, sharing physical resources among multiple workloads,
is a promising technique to save cost and energy in cluster computing systems.
This paper highlights a few challenges of workload consolidation for Hadoop as
one of the current state-of-the-art data-intensive cluster computing system.
Through a systematic step-by-step procedure, we investigate challenges for
efficient server consolidation in Hadoop environments. To this end, we first
investigate the inter-relationship between last level cache (LLC) contention
and throughput degradation for consolidated workloads on a single physical
server employing Hadoop distributed file system (HDFS). We then investigate the
general case of consolidation on multiple physical servers so that their
throughput never falls below a desired/predefined utilization level. We use our
empirical results to model consolidation as a classic two-dimensional bin
packing problem and then design a computationally efficient greedy algorithm to
achieve minimum throughput degradation on multiple servers. Results are very
promising and show that our greedy approach is able to achieve near optimal
solution in all experimented cases.