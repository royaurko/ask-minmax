We study the theoretical properties of learning a dictionary from a set of
$N$ signals $\mathbf x_i\in \mathbb R^K$ for $i=1,...,N$ via
$l_1$-minimization. We assume that the signals $\mathbf x_i$'s are generated as
$i.i.d.$ random linear combinations of the $K$ atoms from a complete reference
dictionary $\mathbf D_0 \in \mathbb R^{K\times K}$. For the random linear
coefficients, we consider two generative models: the $s$-sparse Gaussian model
with $s = 1,..,K$, and the Bernoulli($p$)-Gaussian model with $p\in (0,1]$.
First, for the population case and under each of the two generative models, we
establish a sufficient and almost necessary condition for the reference
dictionary $\mathbf D_0$ to be locally identifiable, i.e. a local minimum of
the expected $l_1$-norm objective function. Our condition covers both the
sparse and dense cases of signal generation, and significantly improves the
sufficient condition by Gribonval and Schnass (2010). It fully describes the
interaction between the collinearity of dictionary atoms $\mathbf M_0 = \mathbf
D_0^T\mathbf D_0$, and the sparsity parameter $s$ or $p$ of the random
coefficients in achieving local identifiability. We also provide sharp and
easy-to-compute lower and upper bounds for the quantities involved in our
conditions. With these bounds, we show that local identifiability is possible
with sparsity level $s$ or $pK$ up to the order $O(\mu^{-2})$ for a complete
$\mu$-coherent reference dictionary, i.e. a dictionary with maximum absolute
collinearity $\mu = \max_{i\neq j} |\mathbf M_0[i,j]|$. Moreover, our local
identifiability results also translate to the finite sample case with high
probability provided that the number of signals $N$ scales as $O(K\log K)$.