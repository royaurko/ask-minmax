Existing cellular network analyses, and even simulations, typically use the
standard path loss model where received power decays like $\|x\|^{-\alpha}$
over a distance $\|x\|$. This standard path loss model is quite idealized, and
in most scenarios the path loss exponent $\alpha$ is itself a function of
$\|x\|$, typically an increasing one. Enforcing a single path loss exponent can
lead to orders of magnitude differences in average received and interference
powers versus the true values. In this paper we study \emph{multi-slope} path
loss models, where different distance ranges are subject to different path loss
exponents. We focus on the dual-slope path loss function, which is a piece-wise
power law and continuous and accurately approximates many practical scenarios.
We derive the distributions of SIR, SNR, and finally SINR before finding the
potential throughput scaling, which provides insight on the observed
cell-splitting rate gain. The exact mathematical results show that the SIR
monotonically decreases with network density, while the converse is true for
SNR, and thus the network coverage probability in terms of SINR is maximized at
some finite density. With ultra-densification (network density goes to
infinity), there exists a \emph{phase transition} in the near-field path loss
exponent $\alpha_0$: if $\alpha_0 >1$ unbounded potential throughput can be
achieved asymptotically; if $\alpha_0 <1$, ultra-densification leads in the
extreme case to zero throughput.