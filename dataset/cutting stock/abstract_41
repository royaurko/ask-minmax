Standard quantitative models of the stock market predict a log-normal
distribution for stock returns (Bachelier 1900, Osborne 1959), but it is
recognised (Fama 1965) that empirical data, in comparison with a Gaussian,
exhibit leptokurtosis (it has more probability mass in its tails and centre)
and fat tails (probabilities of extreme events are underestimated). Different
attempts to explain this departure from normality have coexisted. In
particular, since one of the strong assumptions of the Gaussian model concerns
the volatility, considered finite and constant, the new models were built on a
non finite (Mandelbrot 1963) or non constant (Cox, Ingersoll and Ross 1985)
volatility. We investigate in this thesis a very recent model (Dragulescu et
al. 2002) based on a Brownian motion process for the returns, and a stochastic
mean-reverting process for the volatility. In this model, the forward
Kolmogorov equation that governs the time evolution of returns is solved
analytically. We test this new theory against different stock indexes (Dow
Jones Industrial Average, Standard and Poor s and Footsie), over different
periods (from 20 to 105 years). Our aim is to compare this model with the
classical Gaussian and with a simple Neural Network, used as a benchmark. We
perform the usual statistical tests on the kurtosis and tails of the expected
distributions, paying particular attention to the outliers. As claimed by the
contributors, the new model outperforms the Gaussian for any time lag, but is
artificially too complex for medium and low frequencies, where the Gaussian is
preferable. Moreover this model is still rejected for high frequencies, at a
0.05 level of significance, due to the kurtosis, incorrectly handled.