We introduce a new measure of interdependence among the components of a
random vector along the main diagonal of the vector copula, i.e. along the line
$u_{1}=\ldots=u_{J}$, for
$\left(u_{1},\ldots,u_{J}\right)\in\left[0,1\right]^{J}$. Our measure is
related to the Shannon entropy of a discrete random variable, hence we call it
an "entropy index". This entropy index is invariant with respect to marginal
non-decreasing transformations and can be used to quantify the intensity of the
vector components association in arbitrary dimensions. We show the
applicability of our entropy index by an example with real data of 4 stock
prices of the DAX index. In case the random vector is in the domain of
attraction of an extreme value distribution, our index is shown to have as
limit the distribution's extremal coefficient, which can be interpreted as the
effective number of asymptotically independent components in the vector.