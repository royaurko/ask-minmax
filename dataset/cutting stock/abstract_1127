The predictability of a time series is determined by the sensitivity to
initial conditions of its data generating process. In this paper our goal is to
characterize this sensitivity from a finite sample by assuming few hypotheses
on the data generating model structure. In order to measure the distance
between two trajectories induced by a same noisy chaotic dynamic from two close
initial conditions, a symmetric Kullback-Leiber divergence measure is used. Our
approach allows to take into account the dependence of the residual variance on
initial conditions. We show it is linked to a Fisher information matrix and we
investigated its expressions in the cases of covariance-stationary processes
and ARCH($\infty$) processes. Moreover, we propose a consistent non-parametric
estimator of this sensitivity matrix in the case of conditionally
heteroscedastic autoregressive nonlinear processes. Various statistical
hypotheses can so be tested as for instance the hypothesis that the data
generating process is "almost" independently distributed at a given moment.
Applications to simulated data and to the stock market index S&P500 illustrate
our findings. More particularly, we highlight a significant relationship
between the sensitivity to initial conditions of the daily returns of the S&P
500 and their volatility.