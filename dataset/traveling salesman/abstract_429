Password: Most combinatorial optimization problems belong to the NP-complete or NP-hard classes, which means that they may require an infeasible processing time to be solved by an exhaustive search method. Thus, less expensive heuristics in respect to the processing time are commonly used. These heuristics can obtain satisfactory solutions in short running times, but there is no guarantee that the optimal solution will be found. Artificial Neural Networks (ANNs) have been widely studied to solve combinatorial problems, presenting encouraging results. This paper proposes some modifications on RABNET-TSP, an immune-inspired self-organizing neural network, for the solution of the Traveling Salesman Problem (TSP). The modified algorithm is compared with other neural methods from the literature and the results obtained suggest that the proposed method is competitive in relation to the other ones, outperforming them in many cases with regards to the quality (cost) of the solutions found, though demanding a greater time for convergence in many cases. Keywords Traveling salesman problem ; Self-organizing networks ; Artificial immune systems ; Artificial neural networks ; Combinatorial optimization 1. Introduction The study of combinatorial optimization problems is relevant from a theoretical and practical perspective. They are commonly found in real-world situations, such as routing and scheduling, and usually belong to the NP-complete and NP-hard classes [17]  and  [25] . Thus, the time required to find a satisfactory solution may be infeasible, and the use of heuristics [18]  and  [32] is often preferred, for they are capable of providing a good trade-off between the quality of the solution and the time spent to find it. Probably the most widely studied combinatorial optimization problem is the Traveling Salesman Problem (TSP), which can be described as follows: given a set of n cities (nodes in a graph), a salesman has to visit all the cities once and return to its origin minimizing the cost of the trip [24] . It is well-known that a symmetric TSP with n cities has ( n  − 1)!/2 possible routes to perform, a value that grows factorially with the number of cities. Several algorithms have already been devised to solve TSP-like problems, such as branch and cut algorithms [9]  and  [33] which uses cutting-plane and branch and bound [4]  and  [35] techniques to find the global solution; the k -opt algorithm, which uses a technique that permutes k edges of the current route to find better routes; and Lin–Kerninghan [26] , which adapts k -opt such that an ideal k is found. For the symmetric TSP, Lin–Kerninghan is considered one of the most effective methods to generate optimal or quasi-optimal solutions [20] . Concerning a broader range of heuristics within the Natural Computing [11] area of research, it is important to mention the use of Ant Colony Optimization [13] and Evolutionary Algorithms [28] to solve the TSP. Further information about the TSP and its solution approaches can be found in [2] . Still under the natural computing umbrella, a class of algorithms that have been broadly applied to combinatorial problems, particularly to the TSP, are the self-organized artificial neural networks [6] , [27] , [38]  and  [39] or slight variations of Kohonen’s self-organizing feature map (SOM) [23] . The application of these networks to optimization problems in general is important and intriguing. This is because they usually do not involve a quality measure to assess the performance of the solutions they generate, but are still capable of finding high quality solutions that can be used either as the final solution to the problem or that can be combined with another technique to find the global optimal. A self-organized competitive process is the main mechanism used to adjust the network weights and, thus, find a solution to the problem. In the particular case of competitive networks, which will be the focus of this paper, the network architecture is almost invariably of single layer with a circular neighborhood. In SOM-like networks for solving the TSP, the number N of neurons in the network is usually greater than or equal to the number n of cities in the TSP ( N  ⩾  n ) and, usually, never greater than 2 n . The present paper proposes some modifications in the RABNET-TSP [34] algorithm aiming at improving its performance in terms of computational efficiency (time to solve the problem) and optimality or efficacy (quality of the solution). The modified algorithm is directly compared with the original RABNET-TSP, termed here oRABNET-TSP, and two other well-known self-organized algorithms designed to solve TSP-like problems, namely Angeniol’s network [1] and Somhom’s network [40] . The performance of all methods in terms of optimality is compared with the best values in the literature for a number of TSP instances taken from the TSPLIB [37] . Besides, RABNET-TSP is compared with other approaches whose performances were taken directly from the literature. This paper is organized as follows. Section 2 reviews the literature on self-organized neural networks applied to the TSP, and Section 3 describes RABNET-TSP stressing the modifications introduced. Section 4 presents the simulation results, including the heuristics used to tune the main parameters of all algorithms, and Section 5 concludes the paper with a discussion about the results presented and some perspectives for future investigation. 2. Self-organized networks applied to the TSP: a brief review This section makes a brief review of self-organized neural nets to solve TSP problems, focusing on the main differences between the proposals and describing in greater detail the works of Angeniol et al. [1] and Somhom et al. [40] , which were re-implemented and used for direct performance comparisons. In the work of Durbin and Wilshaw [12] , the authors proposed the Elastic Net , in which the neurons are moved according to two strengths that force the network to be expanded as an elastic band until all cities be covered by the neurons, thus forming a TSP route. These forces act upon the neurons aiming at minimizing the size of the band. The elastic net was used to solve 50-city instances of the traveling salesman problem with results 2% worse than those obtained with the simulated annealing algorithm [22] , regarding the quality of the solutions. The work of Angeniol et al. [1] was one of the pioneers in the use of self-organized neural networks to solve TSP-like problems. In their algorithm, the network is initialized with a single neuron and, through a process of duplication and pruning, the network architecture is allowed to vary with time. Based on experimental results the authors observed that the number of neurons is never greater than twice the number of cities in the instance investigated. Before the iterative process starts, the order of the cities is randomized and maintained fixed throughout all epochs. For each city (input pattern), the winner neuron is the one closest to it. All neurons in the network are moved towards the input city based on a function that takes into account the neighborhood between the neurons in a circular ring and the winner neuron: equation ( 1 ) f(G,d)=(1/sqrt(2))exp(-d 2 /G 2 ), f ( G , d ) = ( 1 / sqrt ( 2 ) ) exp ( - d 2 / G 2 ) , where d is the neighborhood degree between a given neuron and the winner neuron, and G is a parameter that controls the neighborhood influence. Parameter G usually starts with high values and decreases along the iterative procedure of adaptation.