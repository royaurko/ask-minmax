Password: This paper presents an approach to the well-known traveling salesman problem (TSP) using self-organizing maps (SOM). There are many types of SOM algorithms to solve the TSP found in the literature, whereas the purpose of this paper is to look for the incorporation of an efficient initialization methods and the definition of a parameters adaptation law to achieve better results and a faster convergence. Aspects of parameters adaptation, selecting the number of nodes of neurons, index of winner neurons and effect of the initial ordering of the cities, as well as the initial synaptic weights of the modified SOM algorithm are discussed. The complexity of the modified SOM algorithm is analyzed. The simulated results show an average deviation of 2.32% from the optimal tour length for a set of 12 TSP instances. 1. Introduction The traveling salesman problem (TSP) is a typical combinatorial optimization problem. It can be understood as a search for the shortest closed tour that visits each city once and only once. The decision-problem form of TSP is a NP-complete [1] , thence the great interest in efficient heuristics to solve it. There are many of the heuristics that utilize the paradigm of neural computation or related notions recently [2] . The first approach to the TSP via neural networks was the work of Hopfield and Tank in 1985 [3] , which was based on the minimization of an energy function and the local minima should correspond to a good solution of a TSP instance. However, this approach does not assure the feasibility, that is, not all the minima of the energy function represent feasible solution to the TSP. The SOM algorithm, originally introduced by Kohonen [4] , is an unsupervised learning algorithm, where the learning algorithm established a topological relationship among input data. This is the main characteristic explored by this work and by many others found in the literature that approached the TSP via modifications of the SOM algorithm. There are many types of SOM algorithms to solve the TSP [5] , [6] , [7] , [8] , [9]  and  [10] , the purpose of this paper is to look for the incorporation of an efficient initialization method, and the definition of a parameters adaptation law to achieve better results and a faster convergence. This paper is organized as follows. The SOM algorithm is briefly described in Section 2 . Parameters adaptation of the learning rate and the neighborhood function variance are presented in Section 3 . The initialization methods are discussed in Section 4 . Computational experiments and comparisons with other methods are presented in Section 5 . The algorithm complexity is discussed in Section 6 . Conclusions are presented in Section 7 . 2. Self-organizing maps and algorithm There are many different types of self-organizing maps; however, they all share a common characteristic, i.e. the ability to assess the input patterns presented to the networks, organize itself to learn, on its own, based on similarities among the collective set of inputs, and categorize them into groups of similar patterns. In general, self-organizing learning (unsupervised learning) involving the frequent modification of the network’s synaptic weights in response to a set of input patterns. The weight modifications are carried out in accordance with a set of learning rules. After repeated applications of the patterns to the network, a configuration emerges that is of some significance. Kohonen self-organizing map belong to a special class of neural networks denominated Competitive Neural Networks, where each neuron competes with the others to get activated. The result of this competition is the activation of only one output neuron at a given moment. The purpose of Kohonen self-organizing map is to capture the topology and probability distribution of input data [11] . This network generally involved an architecture consisting of uni- or bi-dimensional array of neurons. The original uni-dimensional SOM topology is similar to a bar and is presented in Fig. 1 . A competitive training algorithm is used to train the neural network. In this training mode, not only the winning neuron is allowed to learn, but some neurons within a predefined radius from the winning neuron are also allowed to learn with a decreasing learning rate as the cardinality distance from the winning neuron increases. During the training procedure, synaptic weights of neurons are gradually changed in order to preserve the topological information of the input data when it is introduced to the neural networks. Fig. 1.  To apply this approach to the TSP, a two-layer network, which consists of a two-dimensional input unit and m output units, is used. The evolution of the network may be geometrically imagined as stretching of a ring toward the coordinates of the cities. Fig. 2 represents the ring structure proposed. The input data is the coordinates of cities and the weights of nodes are the coordinates of the points on the ring. The input data (a set of n cities) are presented to the network in a random order and a competition based on Euclidian distance will be held among nodes. The winner node is the node ( J ) with the minimum distance to the presenting city. J  = Argmin j {∥ x i  −  y j ∥ 2 }, where x i is the coordinate of city i , y j is the coordinate of nodes j and ∥ · ∥ 2 is the Euclidean distance. Fig. 2. 