Password: The self-organizing map (SOM) has been successfully employed to handle the Euclidean traveling salesman problem (TSP). By incorporating its neighborhood preserving property and the convex-hull property of the TSP, we introduce a new SOM-like neural network, called the expanding SOM (ESOM). In each learning iteration, the ESOM draws the excited neurons close to the input city, and in the meantime pushes them towards the convex-hull of cities cooperatively. The ESOM may acquire the neighborhood preserving property and the convex-hull property of the TSP, and hence it can yield near-optimal solutions. Its feasibility is analyzed theoretically and empirically. A series of experiments are conducted on both synthetic and benchmark TSPs, whose sizes range from 50 to 2400 cities. Experimental results demonstrate the superiority of the ESOM over several typical SOMs such as the SOM developed by Budinich, the convex elastic net, and the KNIES algorithms. Though its solution accuracy is not yet comparable to some other sophisticated heuristics, the ESOM is one of the most accurate neural networks for the TSP in the literature. Keywords Self-organizing map ; Traveling salesman problem ; Neural networks ; Elastic net ; Convex-hull 1. Introduction As a classical combinatorial optimization problem, the traveling salesman problem (TSP) can be defined as follows: Given a graph G =( V , A ), where V is a set of n vertices and A is a set of arcs between vertices with each arc associated with a nonnegative distance, the TSP is to determine a minimum distance of closed circuits passing through each vertex once and only once [25] . In its simplest form, the Euclidean TSP (ETSP) is the TSP in which all vertices are on a two-dimensional plane with the distance between two vertices calculated according to the Euclidean metric. There are several real-life applications of the TSP, such as VLSI routing [11] , hole punching [6]  and  [30] , and wallpaper cutting [25] . The TSP is NP-hard, i.e., any algorithm for finding optimal tours must have a worst-case running time that grows faster than any polynomial if P≠NP [19]  and  [29] . Recent theoretical research shows a randomized algorithm can be designed to compute a (1+1/ c )-approximation to the optimal tour in time for every fixed c >1 [5] . But its running time still depends exponentially on c . Because of these, researchers have been either attempting to develop optimization algorithms that work well on “real-world,” rather than worst-case instances [6]  and  [15] , or looking for heuristics that merely find near -optimal tours quickly. These heuristics include simulated annealing (SA) [20] , genetic algorithms [19] , tabu search [21] , artificial ant systems [11] , automata networks [35] , and neural networks [4] , [10] , [17]  and  [22] . These diverse approaches have demonstrated various degrees of strength and success [19] . This paper will present an improved neural network that can generate near-optimal TSP solutions with quadratic computation complexity. There are mainly two types of neural networks for the TSP: the Hopfield-type neural networks [17] and the Kohonen-type self-organizing map (SOM-like) neural networks [22] . The Hopfield-type neural networks get tours by searching for the equilibrium states of one dynamic system corresponding to the TSP under consideration. They have been successfully applied to handle small scale TSPs [1]  and  [10] , but could not generate promising solutions for large scale TSPs [9] , [33]  and  [35] . In comparison, the SOM-like neural networks solve the TSP through unsupervised learning [22] . A SOM inspects the input cities for regularities and patterns, and then adjusts its neurons to fit the input cooperatively. The SOM finally brings about the localized response to the input, and thus reflects the neighborhood of the cities. This neighborhood preserving map then results in a tour. From each city, the resultant tour tries to visit its nearest city. The shortest subtours can intuitively lead to a near-optimal tour. Due to their low computation complexity and promising performance, the SOM-like neural networks have attracted a large amount of research to explore and enhance the capability of handling the TSP [3] , [9] , [13]  and  [14] . Though the solution accuracy of the SOM-like neural networks are still not comparable to some state-of-the-art heuristics such as ant colony system [1] , Lin–Kernighan [19]  and  [26] , memetic algorithms [24] , tabu search [19] , and guided local search [34] . These heuristics usually have much higher computation complexity than a SOM [7] , [18]  and  [22] . Furthermore, improvements of neural networks for the TSP are being made [2] , [4] , [7] , [10]  and  [18] . To enhance a SOM, there are following three strategies. Introducing the variable network structure : Instead of the static structure, the output neurons may be dynamically deleted/inserted. Typical examples are the SOM with a dynamic structure [3] , the Kohonen network incorporating explicit statistics (KNIES) [4] , and FLEXMAP with a growing structure [14] . Amending the competition criterion : Burke and Damany [9] have developed the guilty net by introducing a bias term into the competition criterion for inhibiting the too-often-winning neurons. In the work of Favata and Walker [13] , the competition criterion is based on the inner product, which is slightly different from the Euclidean distance when all weights are normalized. In the SOMs developed by Budinich [7]  and  [8] , the visiting order of a city is determined by not only its nearest neuron but also its neighbors. Enhancing the learning rule : For example, the KNIES [4] , besides drawing excited neurons closer to the input city as a classic SOM does, disperses other neurons so as to keep their mean unchanged. In the elastic net [12] , an elastic force is embodied in its learning rule, which is often used to enhance SOMs [2] . Though the SOM-like neural networks may generate promising solutions by establishing a neighborhood preserving map, they have little considered some properties of optimal tours directly. In this paper we present a SOM-like neural network, called the expanding SOM (ESOM), for the TSP. The idea is to train the neural network tactfully so as to preserve the perfect topological relations, as well as to satisfy a necessary condition of any optimal tour—the convex-hull property of the TSP. Experimental results show that, with the similar computation complexity as a traditional SOM, the proposed ESOM can find very near-optimal tours for both synthetic and benchmark TSPs. Comparison results show that it outperforms such typical SOMs as Budinich's SOM [7] , the convex elastic net (CEN) [2] , and the KNIES [4] . We believe that our present results are among the most accurate ones in the neural networks literature. After a brief review of the SOM implementation scheme in the next section, we present the details of our ESOM in Section 3 . Its feasibility and complexity are analyzed in Section 4 . In Section 5 , we provide a comprehensive series of experimental results to demonstrate the effectiveness and efficiency of the ESOM in comparison with five algorithms. We conclude the paper in the last section.