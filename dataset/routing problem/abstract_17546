We derive an information-theoretic lower bound for sample complexity in
sparse recovery problems where inputs can be chosen sequentially and
adaptively. This lower bound is in terms of a simple mutual information
expression and unifies many different linear and nonlinear observation models.
Using this formula we derive bounds for adaptive compressive sensing (CS),
group testing and 1-bit CS problems. We show that adaptivity cannot decrease
sample complexity in group testing, 1-bit CS and CS with linear sparsity. In
contrast, we show there might be mild performance gains for CS in the sublinear
regime. Our unified analysis also allows characterization of gains due to
adaptivity from a wider perspective on sparse problems.