We propose an iterative algorithm for the minimization of a $\ell_1$-norm
penalized least squares functional, under additional linear constraints. The
algorithm is fully explicit: it uses only matrix multiplications with the three
matrices present in the problem (in the linear constraint, in the data misfit
part and in penalty term of the functional). None of the three matrices must be
invertible. Convergence is proven in a finite-dimensional setting. We apply the
algorithm to a synthetic problem in magneto-encephalography where it is used
for the reconstruction of divergence-free current densities subject to a
sparsity promoting penalty on the wavelet coefficients of the current
densities. We discuss the effects of imposing zero divergence and of imposing
joint sparsity (of the vector components of the current density) on the current
density reconstruction.