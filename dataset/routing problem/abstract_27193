A frequent matter of debate in Bayesian inversion is the question, which of
the two principle point-estimators, the maximum-a-posteriori (MAP) or the
conditional mean (CM) estimate is to be preferred. As the MAP estimate
corresponds to the solution given by variational regularization techniques,
this is also a constant matter of debate between the two research areas.
Following a theoretical argument - the Bayes cost formalism - the CM estimate
is classically preferred for being the Bayes estimator for the mean squared
error cost while the MAP estimate is classically discredited for being only
asymptotically the Bayes estimator for the uniform cost function. In this
article we present recent theoretical and computational observations that
challenge this point of view, in particular for high-dimensional
sparsity-promoting Bayesian inversion. Using Bregman distances, we present new,
proper convex Bayes cost functions for which the MAP estimator is the Bayes
estimator. We complement this finding by results that correct further common
misconceptions about MAP estimates. In total, we aim to rehabilitate MAP
estimates in linear inverse problems with log-concave priors as proper Bayes
estimators.