The analysis of online least squares estimation is at the heart of many
stochastic sequential decision making problems. We employ tools from the
self-normalized processes to provide a simple and self-contained proof of a
tail bound of a vector-valued martingale. We use the bound to construct a new
tighter confidence sets for the least squares estimate.
  We apply the confidence sets to several online decision problems, such as the
multi-armed and the linearly parametrized bandit problems. The confidence sets
are potentially applicable to other problems such as sleeping bandits,
generalized linear bandits, and other linear control problems.
  We improve the regret bound of the Upper Confidence Bound (UCB) algorithm of
Auer et al. (2002) and show that its regret is with high-probability a problem
dependent constant. In the case of linear bandits (Dani et al., 2008), we
improve the problem dependent bound in the dimension and number of time steps.
Furthermore, as opposed to the previous result, we prove that our bound holds
for small sample sizes, and at the same time the worst case bound is improved
by a logarithmic factor and the constant is improved.