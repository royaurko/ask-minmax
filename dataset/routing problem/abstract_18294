A sequence $x_1,\dots,x_n,\dots$ of discrete-valued observations is generated
according to some unknown probabilistic law (measure) $\mu$. After observing
each outcome, one is required to give conditional probabilities of the next
observation. The realizable case is when the measure $\mu$ belongs to an
arbitrary but known class $\mathcal C$ of process measures. The non-realizable
case is when $\mu$ is completely arbitrary, but the prediction performance is
measured with respect to a given set $\mathcal C$ of process measures. We are
interested in the relations between these problems and between their solutions,
as well as in characterizing the cases when a solution exists and finding these
solutions. We show that if the quality of prediction is measured using the
total variation distance, then these problems coincide, while if it is measured
using the expected average KL divergence, then they are different. For some of
the formalizations we also show that when a solution exists, it can be obtained
as a Bayes mixture over a countable subset of $\mathcal C$. We also obtain
several characterization of those sets $\mathcal C$ for which solutions to the
considered problems exist. As an illustration to the general results obtained,
we show that a solution to the non-realizable case of the sequence prediction
problem exists for the set of all finite-memory processes, but does not exist
for the set of all stationary processes.
  It should be emphasized that the framework is completely general: the
processes measures considered are not required to be i.i.d., mixing,
stationary, or to belong to any parametric family.