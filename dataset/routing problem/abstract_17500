In this paper we propose distributed dual gradient algorithms for linearly
constrained separable convex problems and analyze their rate of convergence
under different assumptions. Under the strong convexity assumption on the
primal objective function we propose two distributed dual fast gradient schemes
for which we prove sublinear rate of convergence for dual suboptimality but
also primal suboptimality and feasibility violation for an average primal
sequence or for the last generated primal iterate. Under the additional
assumption of Lipshitz continuity of the gradient of the primal objective
function we prove a global error bound type property for the dual problem and
then we analyze a dual gradient scheme for which we derive global linear rate
of convergence for both dual and primal suboptimality and primal feasibility
violation. We also provide numerical simulations on optimal power flow
problems.