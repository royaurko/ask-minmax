In view of the minimization of a function which is the sum of a
differentiable function $f$ and a convex function $g$ we introduce descent
methods which can be viewed as produced by inexact auxiliary problem
principleor inexact variable metric forward-backward algorithm. Assuming that
the global objective function satisfies the Kurdyka-Lojasiewicz inequalitywe
prove the convergence of the proposed algorithm weakening assumptions found in
previous works.