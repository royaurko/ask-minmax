We consider the problem of estimating a rank-one matrix in Gaussian noise
under a probabilistic model for the left and right factors of the matrix. The
probabilistic model can impose constraints on the factors including sparsity
and positivity that arise commonly in learning problems. We propose a simple
iterative procedure that reduces the problem to a sequence of scalar estimation
computations. The method is similar to approximate message passing techniques
based on Gaussian approximations of loopy belief propagation that have been
used recently in compressed sensing. Leveraging analysis methods by Bayati and
Montanari, we show that the asymptotic behavior of the estimates from the
proposed iterative procedure is described by a simple scalar equivalent model,
where the distribution of the estimates is identical to certain scalar
estimates of the variables in Gaussian noise. Moreover, the effective Gaussian
noise level is described by a set of state evolution equations. The proposed
method thus provides a computationally simple and general method for rank-one
estimation problems with a precise analysis in certain high-dimensional
settings.