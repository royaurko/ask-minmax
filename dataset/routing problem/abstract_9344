The theory of degenerate parabolic equations of the forms \[
u_t=(\Phi(u_x))_{x} \quad {\rm and} \quad v_{t}=(\Phi(v))_{xx} \] is used to
analyze the process of contour enhancement in image processing, based on the
evolution model of Sethian and Malladi. The problem is studied in the framework
of nonlinear diffusion equations. It turns out that the standard initial-value
problem solved in this theory does not fit the present application since it it
does not produce image concentration. Due to the degenerate character of the
diffusivity at high gradient values, a new free boundary problem with singular
boundary data can be introduced, and it can be solved by means of a non-trivial
problem transformation. The asymptotic convergence to a sharp contour is
established and rates calculated.