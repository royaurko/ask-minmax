The method of maximum entropy has been very successful but there are cases
where it has either failed or led to paradoxes that have cast doubt on its
general legitimacy. My more optimistic assessment is that such failures and
paradoxes provide us with valuable learning opportunities to sharpen our skills
in the proper way to deploy entropic methods. The central theme of this paper
revolves around the different ways in which constraints are used to capture the
information that is relevant to a problem. This leads us to focus on four
epistemically different types of constraints. I propose that the failure to
recognize the distinctions between them is a prime source of errors. I
explicitly discuss two examples. One concerns the dangers involved in replacing
expected values with sample averages. The other revolves around
misunderstanding ignorance. I discuss the Friedman-Shimony paradox as it is
manifested in the three-sided die problem and also in its original
thermodynamic formulation.