We study the sensitivity to estimation error of portfolios optimized under
various risk measures, including variance, absolute deviation, expected
shortfall and maximal loss. We introduce a measure of portfolio sensitivity and
test the various risk measures by considering simulated portfolios of varying
sizes N and for different lengths T of the time series. We find that the effect
of noise is very strong in all the investigated cases, asymptotically it only
depends on the ratio N/T, and diverges at a critical value of N/T, that depends
on the risk measure in question. This divergence is the manifestation of a
phase transition, analogous to the algorithmic phase transitions recently
discovered in a number of hard computational problems. The transition is
accompanied by a number of critical phenomena, including the divergent sample
to sample fluctuations of portfolio weights. While the optimization under
variance and mean absolute deviation is always feasible below the critical
value of N/T, expected shortfall and maximal loss display a probabilistic
feasibility problem, in that they can become unbounded from below already for
small values of the ratio N/T, and then no solution exists to the optimization
problem under these risk measures. Although powerful filtering techniques exist
for the mitigation of the above instability in the case of variance, our
findings point to the necessity of developing similar filtering procedures
adapted to the other risk measures where they are much less developed or
nonexistent. Another important message of this study is that the requirement of
robustness (noise-tolerance) should be given special attention when considering
the theoretical and practical criteria to be imposed on a risk measure.