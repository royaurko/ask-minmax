We introduce a challenging real-world planning problem where actions must be
taken at each location in a spatial area at each point in time. We use forestry
planning as the motivating application. In Large Scale Spatial-Temporal (LSST)
planning problems, the state and action spaces are defined as the
cross-products of many local state and action spaces spread over a large
spatial area such as a city or forest. These problems possess state
uncertainty, have complex utility functions involving spatial constraints and
we generally must rely on simulations rather than an explicit transition model.
We define LSST problems as reinforcement learning problems and present a
solution using policy gradients. We compare two different policy formulations:
an explicit policy that identifies each location in space and the action to
take there; and an abstract policy that defines the proportion of actions to
take across all locations in space. We show that the abstract policy is more
robust and achieves higher rewards with far fewer parameters than the
elementary policy. This abstract policy is also a better fit to the properties
that practitioners in LSST problem domains require for such methods to be
widely useful.