Many real-world applications are addressed through a linear least-squares
problem formulation, whose solution is calculated by means of an iterative
approach. A huge amount of studies has been carried out in the optimization
field to provide the fastest methods for the reconstruction of the solution,
involving choices of adaptive parameters and scaling matrices. However, in
presence of an ill-conditioned model and real data, the need of a regularized
solution instead of the least-squares one changed the point of view in favour
of iterative algorithms able to combine a fast execution with a stable
behaviour with respect to the restoration error. In this paper we want to
analyze some classical and recent gradient approaches for the linear
least-squares problem by looking at their way of filtering the singular values,
showing in particular the effects of scaling matrices and non-negative
constraints in recovering the correct filters of the solution.