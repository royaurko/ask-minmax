The adoption of probabilistic models for the best individuals found so far is
a powerful approach for evolutionary computation. Increasingly more complex
models have been used by estimation of distribution algorithms (EDAs), which
often result better effectiveness on finding the global optima for hard
optimization problems. Supervised and unsupervised learning of Bayesian
networks are very effective options, since those models are able to capture
interactions of high order among the variables of a problem. Diversity
preservation, through niching techniques, has also shown to be very important
to allow the identification of the problem structure as much as for keeping
several global optima. Recently, clustering was evaluated as an effective
niching technique for EDAs, but the performance of simpler low-order EDAs was
not shown to be much improved by clustering, except for some simple multimodal
problems. This work proposes and evaluates a combination operator guided by a
measure from information theory which allows a clustered low-order EDA to
effectively solve a comprehensive range of benchmark optimization problems.