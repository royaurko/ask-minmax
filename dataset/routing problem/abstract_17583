We take an information theoretic perspective on a classical sparse-sampling
noisy linear model and present an analytical expression for the mutual
information, which plays central role in a variety of communications/processing
problems. Such an expression was addressed previously either by bounds, by
simulations and by the (non-rigorous) replica method. The expression of the
mutual information is based on techniques used in [1], addressing the minimum
mean square error (MMSE) analysis. Using these expressions, we study
specifically a variety of sparse linear communications models which include
coding in different settings, accounting also for multiple access channels and
different wiretap problems. For those, we provide single-letter expressions and
derive achievable rates, capturing the communications/processing features of
these timely models.