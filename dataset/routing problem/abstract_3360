In this article we investigate the possibilities of accelerating the double
smoothing technique when solving unconstrained nondifferentiable convex
optimization problems. This approach relies on the regularization in two steps
of the Fenchel dual problem associated to the problem to be solved into an
optimization problem having a differentiable strongly convex objective function
with Lipschitz continuous gradient. The doubly regularized dual problem is then
solved via a fast gradient method. The aim of this paper is to show how do the
properties of the functions in the objective of the primal problem influence
the implementation of the double smoothing approach and its rate of
convergence. The theoretical results are applied to linear inverse problems by
making use of different regularization functionals.