Recently, several convergence rate results for Douglas-Rachford splitting and
the alternating direction method of multipliers (ADMM) have been presented in
the literature. In this paper, we show linear convergence rate bounds for
Douglas-Rachford splitting under strong convexity and smoothness assumptions.
We show that these bounds generalize and/or improve on similar bounds in the
literature and that the bounds are tight for the class of problems under
consideration. For finite dimensional Euclidean problems, we show how the rate
bounds depend on the metric that is used in the algorithm. We also show how to
select this metric to optimize the bound. Many real-world problems do not
satisfy both the smoothness and strongly convexity assumptions. Therefore, we
also propose heuristic metric and parameter selection methods to improve the
performance of a wider class of problems. The efficiency of the proposed
heuristics is confirmed in a numerical example on a model predictive control
problem, where improvements of more than one order of magnitude are observed.