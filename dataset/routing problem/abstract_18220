We consider the problem of recovery of an unknown multivariate signal $f$
observed in a $d$-dimensional Gaussian white noise model of intensity
$\varepsilon$. We assume that $f$ belongs to a class of smooth functions ${\cal
F}^d\subset L_2([0,1]^d)$ and has an additive sparse structure determined by
the parameter $s$, the number of non-zero univariate components contributing to
$f$. We are interested in the case when $d=d_\varepsilon \to \infty$ as
$\varepsilon \to 0$ and the parameter $s$ stays "small" relative to $d$. With
these assumptions, the recovery problem in hand becomes that of determining
which sparse additive components are non-zero. Attempting to reconstruct most
non-zero components of $f$, but not all of them, we arrive at the problem of
almost full variable selection in high-dimensional regression. For two
different choices of ${\cal F}^d$, we establish conditions under which almost
full variable selection is possible, and provide a procedure that gives almost
full variable selection. The procedure does the best (in the asymptotically
minimax sense) in selecting most non-zero components of $f$. Moreover, it is
adaptive in the parameter $s$.