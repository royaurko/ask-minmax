Consider the following three important problems in statistical inference,
namely, constructing confidence intervals for (1) the error of a
high-dimensional (p > n) regression estimator, (2) the linear regression noise
level, and (3) the genetic signal-to-noise ratio of a continuous-valued trait
(related to the heritability). All three problems turn out to be closely
related to the little-studied problem of performing inference on the
$\ell_2$-norm of the coefficient vector in high-dimensional linear regression.
We derive a novel procedure for this, which is asymptotically correct and
produces valid confidence intervals in finite samples as well. The procedure,
called EigenPrism, is computationally fast and makes no assumptions on
coefficient sparsity or knowledge of the noise level. We investigate the width
of the EigenPrism confidence intervals, including a comparison with a Bayesian
setting in which our interval is just 5% wider than the Bayes credible
interval. We are then able to unify the three aforementioned problems by
showing that the EigenPrism procedure with only minor modifications is able to
make important contributions to all three. We also investigate the robustness
of coverage and find that the method applies in practice and in finite samples
much more widely than in the regime covered by the theory. Finally, we apply
EigenPrism to a genetic dataset to estimate the genetic signal-to-noise ratio
for a number of continuous phenotypes.