In the Bayesian approach, the a priori knowledge about the input of a
mathematical model is described via a probability measure. The joint
distribution of the unknown input and the data is then conditioned, using
Bayes' formula, giving rise to the posterior distribution on the unknown input.
In this setting we prove posterior consistency for nonlinear inverse problems:
a sequence of data is considered, with diminishing fluctuations around a single
truth and it is then of interest to show that the resulting sequence of
posterior measures arising from this sequence of data concentrates around the
truth used to generate the data. Posterior consistency justifies the use of the
Bayesian approach very much in the same way as error bounds and convergence
results for regularisation techniques do. As a guiding example, we consider the
inverse problem of reconstructing the diffusion coefficient from noisy
observations of the solution to an elliptic PDE in divergence form. This
problem is approached by splitting the forward operator into the underlying
continuum model and a simpler observation operator based on the output of the
model.
  In general, these splittings allow us to conclude posterior consistency
provided a deterministic stability result for the underlying inverse problem
and a posterior consistency result for the Bayesian regression problem with the
push-forward prior.
  Moreover, we prove posterior consistency for the Bayesian regression problem
based on the regularity, the tail behaviour and the small ball probabilities of
the prior.