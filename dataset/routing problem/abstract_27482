Motivated by recent work on stochastic gradient descent methods, we develop
two stochastic variants of greedy algorithms for possibly non-convex
optimization problems with sparsity constraints. We prove linear convergence in
expectation to the solution within a specified tolerance. This generalized
framework applies to problems such as sparse signal recovery in compressed
sensing, low-rank matrix recovery, and covariance matrix estimation, giving
methods with provable convergence guarantees that often outperform their
deterministic counterparts. We also analyze the settings where gradients and
projections can only be computed approximately, and prove the methods are
robust to these approximations. We include many numerical experiments which
align with the theoretical analysis and demonstrate these improvements in
several different settings.