The multi-armed bandit (MAB) problem features the classical tradeoff between
exploration and exploitation. The input specifies several stochastic arms which
evolve with each pull, and the goal is to maximize the expected reward after a
fixed budget of pulls. The celebrated work of Gittins et al. [GGW89] presumes a
condition on the arms called the martingale assumption. Recently, A. Gupta et
al. obtained an LP-based 1/48-approximation for the problem with the martingale
assumption removed [GKMR11]. We improve the algorithm to a 4/27-approximation,
with simpler analysis. Our algorithm also generalizes to the case of MAB
superprocesses with (stochastic) multi-period actions. This generalization
captures the framework introduced by Guha and Munagala in [GM07a, GM07b], and
yields new results for their budgeted learning problems.
  Also, we obtain a (1/2-eps)-approximation for the variant of MAB where
preemption (playing an arm, switching to another arm, then coming back to the
first arm) is not allowed. This contains the stochastic knapsack problem of
Dean, Goemans, and Vondrak [DGV08] with correlated rewards, where we are given
a knapsack of fixed size, a set of jobs each with a joint distribution for its
size and reward, and the objective is to maximize expected reward before the
knapsack size is exhausted. Our (1/2-eps)-approximation improves the 1/16 and
1/8 approximations of [GKMR11] for correlated stochastic knapsack with
cancellation and no cancellation, respectively, providing the first tight
algorithm for these problems that matches the integrality gap of 2. We sample
probabilities from an exponential-sized dynamic programming solution, whose
existence is guaranteed by an LP projection argument. We hope this technique
can also be applied to other dynamic programming problems which can be
projected down onto a small LP.