Instances of monotone submodular function maximization with cardinality
constraint occur often in practical applications. One example is feature
selection in machine learning, where in many models, adding a new feature to an
existing set of features always improves the modeling power (monotonicity) and
the marginal benefit of adding a new feature decreases as we consider larger
sets (submodularity). However, we would like to choose a robust set of features
such that, there is not too much dependence on a small subset of chosen
features [Krause et al. (08), Globerson & Roweis (06)].
  We consider a formulation of this problem that was formally introduced by
Krause et al. (08). It is not difficult to show that even if we look at
robustness to removing a single element, the problem is at least as hard as the
ordinary maximization problem, which is approximable up to a factor of $1-1/e$.
For this case of single element removal, we give: (i) an algorithm with
parameter $m$, that has asymptotic guarantee $(1-1/e)-\Omega(1/m)$ using
$O(n^{m+1})$ queries and (ii) a fast asymptotically 0.5547 approximate
algorithm. For the general case of subset removal, we give a fast algorithm
with asymptotic guarantee $0.285$. These are the first constant factor results
for this problem.