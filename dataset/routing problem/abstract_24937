We introduce estimation and test procedures through divergence minimization
for models satisfying linear constraints with unknown parameter. Several
statistical examples and motivations are given. These procedures extend the
empirical likelihood (EL) method and share common features with generalized
empirical likelihood (GEL). We treat the problems of existence and
characterization of the divergence projections of probability measures on sets
of signed finite measures. Our approach allows for a study of the estimates
under misspecification. The asymptotic behavior of the proposed estimates are
studied using the dual representation of the divergences and the explicit forms
of the divergence projections. We discuss the problem of the choice of the
divergence under various respects. Also we handle efficiency and robustness
properties of minimum divergence estimates. A simulation study shows that the
Hellinger divergence enjoys good efficiency and robustness properties.