We consider the problem of estimating the population probability distribution
given a finite set of multivariate samples, using the maximum entropy approach.
In strict keeping with Jaynes' original definition, our precise formulation of
the problem considers contributions only from the smoothness of the estimated
distribution (as measured by its entropy) and the loss functional associated
with its goodness-of-fit to the sample data, and in particular does not make
use of any additional constraints that cannot be justified from the sample data
alone. By mapping the general multivariate problem to a tractable univariate
one, we are able to write down exact expressions for the goodness-of-fit of an
arbitrary multivariate distribution to any given set of samples using both the
traditional likelihood-based approach and a rigorous information-theoretic
approach, thus solving a long-standing problem. As a corollary we also give an
exact solution to the `forward problem' of determining the expected
distributions of samples taken from a population with known probability
distribution.