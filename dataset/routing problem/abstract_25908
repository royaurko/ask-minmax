Let $X| \mu \sim N_p(\mu,v_xI)$ and $Y| \mu \sim N_p(\mu,v_yI)$ be
independent p-dimensional multivariate normal vectors with common unknown mean
$\mu$. Based on only observing $X=x$, we consider the problem of obtaining a
predictive density $\hat{p}(y| x)$ for $Y$ that is close to $p(y| \mu)$ as
measured by expected Kullback--Leibler loss. A natural procedure for this
problem is the (formal) Bayes predictive density $\hat{p}_{\mathrm{U}}(y| x)$
under the uniform prior $\pi_{\mathrm{U}}(\mu)\equiv 1$, which is best
invariant and minimax. We show that any Bayes predictive density will be
minimax if it is obtained by a prior yielding a marginal that is superharmonic
or whose square root is superharmonic. This yields wide classes of minimax
procedures that dominate $\hat{p}_{\mathrm{U}}(y| x)$, including Bayes
predictive densities under superharmonic priors. Fundamental similarities and
differences with the parallel theory of estimating a multivariate normal mean
under quadratic loss are described.