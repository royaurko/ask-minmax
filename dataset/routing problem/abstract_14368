We consider the problem of distributed estimation, where local processors
observe independent samples conditioned on a common random parameter of
interest, map the observations to a finite number of bits, and send these bits
to a remote estimator over independent noisy channels. We derive converse
results for this problem, such as lower bounds on Bayes risk. The main
technical tools include a lower bound on the Bayes risk via mutual information
and small ball probability, as well as strong data processing inequalities for
the relative entropy. Our results can recover and improve some existing results
on distributed estimation with noiseless channels, and also capture the effect
of noisy channels on the estimation performance.