In the regression setting, dimension reduction allows for complicated
regression structures to be detected via visualization in a low-dimension
framework. However, some popular dimension reduction methodologies fail to
achieve this aim when faced with a problem often referred to as symmetric
dependency. In this paper we show how vastly superior results can be achieved
when carrying out response and predictor transformations for methods such as
least squares and Sliced Inverse Regression. These transformations are simple
to implement and utilize estimates from other dimension reduction methods that
are not faced with the symmetric dependency problem. We highlight the
effectiveness of our approach via simulation and an example. Furthermore, we
show that ordinary least squares can effectively detect multiple dimension
reduction directions. Methods robust to extreme response values are also
considered.