We study the tradeoff between the statistical error and communication cost
for distributed statistical estimation problems in high dimensions. In the
distributed sparse Gaussian mean estimation problem, each of the $m$ machines
receives $n$ data points from a $d$-dimensional Gaussian distribution with
unknown mean $\theta$ which is promised to be $k$-sparse. The machines
communicate by message passing and aim to estimate the mean $\theta$. We
provide a tight (up to logarithmic factors) tradeoff between the estimation
error and the number of bits communicated between the machines. This directly
leads to a lower bound for the distributed \textit{sparse linear regression}
problem: to achieve the statistical minimax error, the total communication is
at least $\Omega(\min\{n,d\}m)$, where $n$ is the number of observations that
each machine receives and $d$ is the ambient dimension. As our main technique,
we prove a \textit{distributed data processing inequality}, as a generalization
of usual data processing inequalities, which might be of independent interest.
Finally, we give a communication-optimal protocol for distributed Gaussian mean
estimation, improving the number of rounds of the previous such protocol from
$O(\log m)$ to $1$.