We consider the problem of simultaneous variable selection and constant
coefficient identification in high-dimensional varying coefficient models based
on B-spline basis expansion. Both objectives can be considered as some type of
model selection problems and we show that they can be achieved by a double
shrinkage strategy. We apply the adaptive group Lasso penalty in models
involving a diverging number of covariates, which can be much larger than the
sample size, but we assume the number of relevant variables is smaller than the
sample size via model sparsity. Such so-called ultra-high dimensional settings
are especially challenging in semiparametric models as we consider here and has
not been dealt with before. Under suitable conditions, we show that consistency
in terms of both variable selection and constant coefficient identification can
be achieved, as well as the oracle property of the constant coefficients. Even
in the case that the zero and constant coefficients are known a priori, our
results appear to be new in that it reduces to semivarying coefficient models
(a.k.a. partially linear varying coefficient models) with a diverging number of
covariates. We also theoretically demonstrate the consistency of a
semiparametric BIC-type criterion in this high-dimensional context, extending
several previous results. The finite sample behavior of the estimator is
evaluated by some Monte Carlo studies.