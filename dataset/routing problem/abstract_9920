Online and stochastic gradient methods have emerged as potent tools in large
scale optimization with both smooth convex and nonsmooth convex problems from
the classes $C^{1,1}(\reals^p)$ and $C^{1,0}(\reals^p)$ respectively. However
to our best knowledge, there is few paper to use incremental gradient methods
to optimization the intermediate classes of convex problems with H\"older
continuous functions $C^{1,v}(\reals^p)$. In order fill the difference and gap
between methods for smooth and nonsmooth problems, in this work, we propose the
several online and stochastic universal gradient methods, that we do not need
to know the actual degree of smoothness of the objective function in advance.
We expanded the scope of the problems involved in machine learning to H\"older
continuous functions and to propose a general family of first-order methods.
Regret and convergent analysis shows that our methods enjoy strong theoretical
guarantees. For the first time, we establish an algorithms that enjoys a linear
convergence rate for convex functions that have H\"older continuous gradients.