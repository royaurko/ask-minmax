Regularized kernel methods such as support vector machines (SVM) and support
vector regression (SVR) constitute a broad and flexible class of methods which
are theoretically well investigated and commonly used in nonparametric
classification and regression problems. As these methods are based on a
Tikhonov regularization which is also common in inverse problems, this article
investigates the use of regularized kernel methods for inverse problems in a
unifying way. Regularized kernel methods are based on the use of reproducing
kernel Hilbert spaces (RKHS) which lead to very good computational properties.
It is shown that similar properties remain true in solving statistical inverse
problems and that standard software implementations developed for ordinary
regression problems can still be used for inverse regression problems.
Consistency of these methods and a rate of convergence for the risk is shown
under quite weak assumptions and rates of convergence for the estimator are
shown under somehow stronger assumptions. The applicability of these methods is
demonstrated in a simulation.