In this paper we demonstrate that two common problems in Machine
Learning---imbalanced and overlapping data distributions---do not have
independent effects on the performance of SVM classifiers. This result is
notable since it shows that a model of either of these factors must account for
the presence of the other. Our study of the relationship between these problems
has lead to the discovery of a previously unreported form of "covert"
overfitting which is resilient to commonly used empirical regularization
techniques. We demonstrate the existance of this covert phenomenon through
several methods based around the parametric regularization of trained SVMs. Our
findings in this area suggest a possible approach to quantifying overlap in
real world data sets.