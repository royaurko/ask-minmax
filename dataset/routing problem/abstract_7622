We consider stochastic variational inequality problems where the mapping is
monotone over a compact convex set. We present two robust variants of
stochastic extragradient algorithms for solving such problems. Of these, the
first scheme employs an iterative averaging technique where we consider a
generalized choice for the weights in the averaged sequence. Our first
contribution is to show that using an appropriate choice for these weights, a
suitably defined gap function attains the optimal rate of convergence ${\cal
O}\left(\frac{1}{\sqrt{k}}\right)$. In the second part of the paper, under an
additional assumption of weak-sharpness, we update the stepsize sequence using
a recursive rule that leverages problem parameters. The second contribution
lies in showing that employing such a sequence, the extragradient algorithm
possesses almost-sure convergence to the solution as well as convergence in a
mean-squared sense to the solution of the problem at the rate ${\cal
O}\left(\frac{1}{k}\right)$. Motivated by the absence of a Lipschitzian
parameter, in both schemes we utilize a locally randomized smoothing scheme.
Importantly, by approximating a smooth mapping, this scheme enables us to
estimate the Lipschitzian parameter. The smoothing parameter is updated per
iteration and we show convergence to the solution of the original problem in
both algorithms.