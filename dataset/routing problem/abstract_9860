The problem of maximizing the $p$-th power of a $p$-norm over a
halfspace-presented polytope in $\R^d$ is a convex maximization problem which
plays a fundamental role in computational convexity. It has been shown in 1986
that this problem is $\NP$-hard for all values $p \in \mathbb{N}$, if the
dimension $d$ of the ambient space is part of the input. In this paper, we use
the theory of parametrized complexity to analyze how heavily the hardness of
norm maximization relies on the parameter $d$.
  More precisely, we show that for $p=1$ the problem is fixed parameter
tractable but that for all $p \in \mathbb{N} \setminus \{1\}$ norm maximization
is W[1]-hard.
  Concerning approximation algorithms for norm maximization, we show that for
fixed accuracy, there is a straightforward approximation algorithm for norm
maximization in FPT running time, but there is no FPT approximation algorithm,
the running time of which depends polynomially on the accuracy.
  As with the $\NP$-hardness of norm maximization, the W[1]-hardness
immediately carries over to various radius computation tasks in Computational
Convexity.