Shrinkage estimators of covariance are an important tool in modern applied
and theoretical statistics. They play a key role in regularized estimation
problems, such as ridge regression (aka Tykhonov regularization), regularized
discriminant analysis and a variety of optimization problems.
  In this paper, we bring to bear the tools of random matrix theory to
understand their behavior, and in particular, that of quadratic forms involving
inverses of those estimators, which are important in practice.
  We use very mild assumptions compared to the usual assumptions made in random
matrix theory, requiring only mild conditions on the moments of linear and
quadratic forms in our random vectors. In particular, we show that our results
apply for instance to log-normal data, which are of interest in financial
applications.
  Our study highlights the relative sensitivity of random matrix results (and
their practical consequences) to geometric assumptions which are often
implicitly made by random matrix theorists and may not be relevant in data
analytic practice.