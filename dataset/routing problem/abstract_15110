We study the convergence properties of an alternating proximal minimization
algorithm for nonconvex structured functions of the type:
$L(x,y)=f(x)+Q(x,y)+g(y)$, where $f:\R^n\rightarrow\R\cup{+\infty}$ and
$g:\R^m\rightarrow\R\cup{+\infty}$ are proper lower semicontinuous functions,
and $Q:\R^n\times\R^m\rightarrow \R$ is a smooth $C^1$ function which couples
the variables $x$ and $y$. The algorithm can be viewed as a proximal
regularization of the usual Gauss-Seidel method to minimize $L$. We work in a
nonconvex setting, just assuming that the function $L$ satisfies the Kurdyka-\L
ojasiewicz inequality. An entire section illustrates the relevancy of such an
assumption by giving examples ranging from semialgebraic geometry to
"metrically regular" problems. Our main result can be stated as follows: If L
has the Kurdyka-\L ojasiewicz property, then each bounded sequence generated by
the algorithm converges to a critical point of $L$. This result is completed by
the study of the convergence rate of the algorithm, which depends on the
geometrical properties of the function $L$ around its critical points. When
specialized to $Q(x,y)=|x-y|^2$ and to $f$, $g$ indicator functions, the
algorithm is an alternating projection mehod (a variant of Von Neumann's) that
converges for a wide class of sets including semialgebraic and tame sets,
transverse smooth manifolds or sets with "regular" intersection. In order to
illustrate our results with concrete problems, we provide a convergent proximal
reweighted $\ell^1$ algorithm for compressive sensing and an application to
rank reduction problems.