Motivated by problems of anomaly detection, this paper implements the
Neyman-Pearson paradigm to deal with asymmetric errors in binary classification
with a convex loss. Given a finite collection of classifiers, we combine them
and obtain a new classifier that satisfies simultaneously the two following
properties with high probability: (i) its probability of type I error is below
a pre-specified level and (ii), it has probability of type II error close to
the minimum possible. The proposed classifier is obtained by solving an
optimization problem with an empirical objective and an empirical constraint.
New techniques to handle such problems are developed and have consequences on
chance constrained programming.