Markov control algorithms that perform smooth, non-greedy updates of the
policy have been shown to be very general and versatile, with policy gradient
and Expectation Maximisation algorithms being particularly popular. For these
algorithms, marginal inference of the reward weighted trajectory distribution
is required to perform policy updates. We discuss a new exact inference
algorithm for these marginals in the finite horizon case that is more efficient
than the standard approach based on classical forward-backward recursions. We
also provide a principled extension to infinite horizon Markov Decision
Problems that explicitly accounts for an infinite horizon. This extension
provides a novel algorithm for both policy gradients and Expectation
Maximisation in infinite horizon problems.