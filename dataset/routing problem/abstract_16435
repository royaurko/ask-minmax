The approximation of a general $d$-variate function $f$ by the shifts
$\phi(\cdot-\xi)$, $\xi\in\Xi\subset \Rd$, of a fixed function $\phi$ occurs in
many applications such as data fitting, neural networks, and learning theory.
When $\Xi=h\Z^d$ is a dilate of the integer lattice, there is a rather complete
understanding of the approximation problem \cite{BDR,Johnson1} using Fourier
techniques. However, in most applications the {\it center} set $\Xi$ is either
given, or can be chosen with complete freedom. In both of these cases, the
shift-invariant setting is too restrictive. This paper studies the
approximation problem in the case $\Xi$ is arbitrary. It establishes
approximation theorems whose error bounds reflect the local density of the
points in $\Xi$. Two different settings are analyzed. The first is when the set
$\Xi$ is prescribed in advance. In this case, the theorems of this paper show
that, in analogy with the classical univariate spline approximation, improved
approximation occurs in regions where the density is high. The second setting
corresponds to the problem of non-linear approximation. In that setting the set
$\Xi$ can be chosen using information about the target function $f$. We discuss
how to `best' make these choices and give estimates for the approximation
error.