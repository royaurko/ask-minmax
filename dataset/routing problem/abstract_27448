The theory behind compressive sampling pre-supposes that a given sequence of
observations may be exactly represented by a linear combination of a small
number of basis vectors. In practice, however, even small deviations from an
exact signal model can result in dramatic increases in estimation error; this
is the so-called "basis mismatch" problem. This work provides one possible
solution to this problem in the form of an iterative, biconvex search
algorithm. The approach uses standard $\ell_1$-minimization to find the signal
model coefficients followed by a maximum likelihood estimate of the signal
model. The algorithm is illustrated on harmonic signals of varying sparsity and
outperforms the current state-of-the-art.