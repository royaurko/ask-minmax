Numerical analysts might be expected to pay close attention to a branch of
complexity theory called information-based complexity theory (IBCT), which
produces an abundance of impressive results about the quest for approximate
solutions to mathematical problems. Why then do most numerical analysts turn a
cold shoulder to IBCT? Close analysis of two representative papers reveals a
mixture of nice new observations, error bounds repackaged in new language,
misdirected examples, and misleading theorems.
  Some elements in the framework of IBCT, erected to support a rigorous yet
flexible theory, make it difficult to judge whether a model is off-target or
reasonably realistic. For instance, a sharp distinction is made between
information and algorithms restricted to this information. Yet the information
itself usually comes from an algorithm, so the distinction clouds the issues
and can lead to true but misleading inferences. Another troublesome aspect of
IBCT is a free parameter $F$, the class of admissible problem instances. By
overlooking $F$'s membership fee, the theory sometimes distorts the economics
of problem solving in a way reminiscent of agricultural subsidies.
  The current theory's surprising results pertain only to unnatural
situations, and its genuinely new insights might serve us better if expressed
in the conventional modes of error analysis and approximation theory.