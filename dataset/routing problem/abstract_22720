We extend our study of phase transitions in the generalization behaviour of
multilayer perceptrons with non-overlapping receptive fields to the problem of
the influence of noise, concerning e.g. the input units and/or the couplings
between the input units and the hidden units of the second layer (='input
noise'), or the final output unit (='output noise'). Without output noise, the
output itself is given by a general, permutation-invariant Boolean function of
the outputs of the hidden units. As a result we find that the phase
transitions, which we found in the deterministic case, mostly persist in the
presence of noise. The influence of the noise on the position of the phase
transition, as well as on the behaviour in other regimes of the loading
parameter $\alpha$, can often be described by a simple rescaling of $\alpha$
depending on strength and type of the noise. We then consider the problem of
the optimal noise level for Gibbsian and Bayesian learning, looking on replica
symmetry breaking as well. Finally we consider the question why learning with
errors is useful at all.