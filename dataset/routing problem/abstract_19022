We present an analysis of the Locally Competitive Algorithm (LCA), a
Hopfield-style neural network that efficiently solves sparse approximation
problems (e.g., approximating a vector from a dictionary using just a few
non-zero coefficients). This class of problems plays a significant role in both
theories of neural coding and applications in signal processing. However, the
LCA lacks analysis of its convergence properties and previous results on neural
networks for nonsmooth optimization do not apply to the specifics of the LCA
architecture. We show that the LCA has desirable convergence properties, such
as stability and global convergence to the optimum of the objective function
when it is unique. Under some mild conditions, the support of the solution is
also proven to be reached in finite time. Furthermore, some restrictions on the
problem specifics allow us to characterize the convergence rate of the system
by showing that the LCA converges exponentially fast with an analytically
bounded convergence rate. We support our analysis with several illustrative
simulations.