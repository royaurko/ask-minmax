This paper introduces a coordinate descent version of the V\~u-Condat
algorithm. By coordinate descent, we mean that only a subset of the coordinates
of the primal and dual iterates is updated at each iteration, the other
coordinates being maintained to their past value. Our method allows us to solve
optimization problems with a combination of differentiable functions,
constraints as well as non-separable and non-differentiable regularizers. We
show that the sequences generated by our algorithm converge to a saddle point
of the problem at stake, for a wider range of parameter values than previous
methods. In particular, the condition on the step-sizes depends on the
coordinate-wise Lipschitz constant of the differentiable function's gradient,
which is a major feature allowing classical coordinate descent to perform so
well when it is applicable. We illustrate the performances of the algorithm on
a total-variation regularized least squares regression problem and on large
scale support vector machine problems.