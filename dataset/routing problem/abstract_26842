Information geometric optimization (IGO) is a general framework for
stochastic optimization problems aiming at limiting the influence of arbitrary
parametrization choices. The initial problem is transformed into the
optimization of a smooth function on a Riemannian manifold, defining a
parametrization-invariant first order differential equation. However, in
practice, it is necessary to discretize time, and then, parametrization
invariance holds only at first order in the step size.
  We define the Geodesic IGO update (GIGO), which uses the Riemannian manifold
structure to obtain an update entirely independent from the parametrization of
the manifold. We test it with classical objective functions.
  Thanks to Noether's theorem from classical mechanics, we find an efficient
way to write a first order differential equation satisfied by the geodesics of
the statistical manifold of Gaussian distributions, and thus to compute the
corresponding GIGO update. We then compare GIGO, pure rank-$\mu$ CMA-ES
\cite{CMATuto} and xNES \cite{xNES} (two previous algorithms that can be
recovered by the IGO framework), and show that while the GIGO and xNES updates
coincide when the mean is fixed, they are different in general, contrary to
previous intuition. We then define a new algorithm (Blockwise GIGO) that
recovers the xNES update from abstract principles.