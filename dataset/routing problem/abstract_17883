Many psychologists do not realize that exploratory use of the popular
multiway analysis of variance (ANOVA) harbors a multiple comparison problem. In
the case of two factors, three separate null hypotheses are subject to test
(i.e., two main effects and one interaction). Consequently, the probability of
at least one Type I error (if all null hypotheses are true) is 14% rather than
5% if the three tests are independent. We explain the multiple comparison
problem and demonstrate that researchers almost never correct for it. To
mitigate the problem, we describe four remedies: the omnibus F test, the
control of familywise error rate, the control of false discovery rate, and the
preregistration of hypotheses.