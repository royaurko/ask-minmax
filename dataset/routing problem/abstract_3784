Sparsity is a key driver in modern statistical problems, from linear
regression via the Lasso to matrix regression with nuclear norm penalties in
matrix completion and beyond. In stark contrast to sparsity motivations for
such problems, it is known in the field of robust optimization that a variety
of vector regression problems, such as Lasso which appears as a loss function
plus a regularization penalty, can arise by simply immunizing a nominal problem
(with only a loss function) to uncertainty in the data. Such a robustification
offers an explanation for why some linear regression methods perform well in
the face of noise, even when these methods do not produce reliably sparse
solutions. In this paper we deepen and extend the understanding of the
connection between robustification and regularization in regression problems.
Specifically, (a) in the context of linear regression, we characterize under
which conditions on the model of uncertainty used and on the loss function
penalties robustification and regularization are equivalent; (b) we show how to
tractably robustify median regression problems; and (c) we extend the
characterization of robustification and regularization to matrix regression
problems (matrix completion and Principal Component Analysis).