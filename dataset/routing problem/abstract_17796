The $\ell_1$ norm is the tight convex relaxation for the $\ell_0$ "norm" and
has been successfully applied for recovering sparse signals. For problems with
fewer samplings, one needs to enhance the sparsity by nonconvex penalties such
as $\ell_p$ "norm". As one method for solving $\ell_p$ minimization problems,
iteratively reweighted $\ell_1$ minimization updates the weight for each
component based on the value of the same component at the previous iteration.
It assigns large weights on small components in magnitude and small weights on
large components in magnitude. In this paper, we consider a weighted $\ell_1$
penalty with the set of the weights fixed and the weights are assigned based on
the sort of all the components in magnitude. The smallest weight is assigned to
the largest component in magnitude. This new penalty is called nonconvex sorted
$\ell_1$. Then we propose two methods for solving nonconvex sorted $\ell_1$
minimization problems: iteratively reweighted $\ell_1$ minimization and
iterative sorted thresholding, and prove that both methods will converge to a
local optimum. We also show that both methods are generalizations of iterative
support detection and iterative hard thresholding respectively. The numerical
experiments demonstrate the better performance of assigning weights by sort
compared to $\ell_p$ minimization.