In some estimation problems, especially in applications dealing with
information theory, signal processing and biology, theory provides us with
additional information allowing us to restrict the parameter space to a finite
number of points. In this case, we speak of discrete parameter models. Even
though the problem is quite old and has interesting connections with testing
and model selection, asymptotic theory for these models has hardly ever been
studied. Therefore, we discuss consistency, asymptotic distribution theory,
information inequalities and their relations with efficiency and
superefficiency for a general class of $m$-estimators.