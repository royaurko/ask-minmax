In low-level sensory systems, it is still unclear how the noisy information
collected locally by neurons may give rise to a coherent global percept. This
is well demonstrated for the detection of motion in the aperture problem: as
luminance of an elongated line is symmetrical along its axis, tangential
velocity is ambiguous when measured locally. Here, we develop the hypothesis
that motion-based predictive coding is sufficient to infer global motion. Our
implementation is based on a context-dependent diffusion of a probabilistic
representation of motion. We observe in simulations a progressive solution to
the aperture problem similar to physio-logy and behavior. We demonstrate that
this solution is the result of two underlying mechanisms. First, we demonstrate
the formation of a tracking behavior favoring temporally coherent features
independent of their texture. Second, we observe that incoherent features are
explained away, while coherent information diffuses progressively to the global
scale. Most previous models included ad hoc mechanisms such as end-stopped
cells or a selection layer to track specific luminance-based features as
necessary conditions to solve the aperture problem. Here, we have proved that
motion-based predictive coding, as it is implemented in this functional model,
is sufficient to solve the aperture problem. This solution may give insights
into the role of prediction underlying a large class of sensory computations.