Theoretical models of neuronal function consider different mechanisms through
which networks learn, classify and discern inputs. A central focus of these
models is to understand how associations are established amongst neurons, in
order to predict spiking patterns that are compatible with empirical
observations. Although these models have led to major insights and advances,
they still do not account for the astonishing velocity with which the brain
solves certain problems and what lies behind its creativity, amongst others
features. We examine two important components that may crucially aid
comprehensive understanding of said neurodynamical processes. First, we argue
that once presented with a problem, different putative solutions are generated
in parallel by different groups or local neuronal complexes, with the
subsequent stabilization and spread of the best solutions. Using mathematical
models we show that this mechanism accelerates finding the right solutions.
This formalism is analogous to standard replicator-mutator models of evolution
where mutation is analogous to the probability of neuron state switching
(on/off). The second factor that we incorporate is structural synaptic
plasticity, i.e. the making of new and disbanding of old synapses, which we
apply as a dynamical reorganization of synaptic connections. We show that
Hebbian learning alone does not suffice to reach optimal solutions. However,
combining it with parallel evaluation and structural plasticity opens up
possibilities for efficient problem solving. In the resulting networks,
topologies converge to subsets of fully connected components. Imposing costs on
synapses reduces the connectivity, although the number of connected components
remains robust. The average lifetime of synapses is longer for connections that
are established early, and diminishes with synaptic cost.