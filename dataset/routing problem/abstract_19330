In this paper, we deal with the problem of jointly determining the optimal
coding strategy and the scheduling decisions when receivers obtain layered data
from multiple servers. The layered data is encoded by means of Prioritized
Random Linear Coding (PRLC) in order to be resilient to channel loss while
respecting the unequal levels of importance in the data, and data blocks are
transmitted simultaneously in order to reduce decoding delays and improve the
delivery performance. We formulate the optimal coding and scheduling decisions
problem in our novel framework with the help of Markov Decision Processes
(MDP), which are effective tools for modeling adapting streaming systems.
Reinforcement learning approaches are then proposed to derive reduced
computational complexity solutions to the adaptive coding and scheduling
problems. The novel reinforcement learning approaches and the MDP solution are
examined in an illustrative example for scalable video transmission. Our
methods offer large performance gains over competing methods that deliver the
data blocks sequentially. The experimental evaluation also shows that our novel
algorithms offer continuous playback and guarantee small quality variations
which is not the case for baseline solutions. Finally, our work highlights the
advantages of reinforcement learning algorithms to forecast the temporal
evolution of data demands and to decide the optimal coding and scheduling
decisions.