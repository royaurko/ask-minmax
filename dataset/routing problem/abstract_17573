In previous work, a Cooperative Receding Horizon (CRH) controller was
developed for solving cooperative multi-agent problems in uncertain
environments. In this paper, we overcome several limitations of this
controller, including potential instabilities in the agent trajectories and
poor performance due to inaccurate estimation of a reward-to-go function. We
propose an event-driven CRH controller to solve the maximum reward collection
problem (MRCP) where multiple agents cooperate to maximize the total reward
collected from a set of stationary targets in a given mission space. Rewards
are non-increasing functions of time and the environment is uncertain with new
targets detected by agents at random time instants. The controller sequentially
solves optimization problems over a planning horizon and executes the control
for a shorter action horizon, where both are defined by certain events
associated with new information becoming available. In contrast to the earlier
CRH controller, we reduce the originally infinite-dimensional feasible control
set to a finite set at each time step. We prove some properties of this new
controller and include simulation results showing its improved performance
compared to the original one.