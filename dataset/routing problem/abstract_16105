This paper considers the problem of adaptive estimation of a template in a
randomly shifted curve model. Using the Fourier transform of the data, we show
that this problem can be transformed into a stochastic linear inverse problem.
Our aim is to approach the estimator that has the smallest risk on the true
template over a finite set of linear estimators defined in the Fourier domain.
Based on the principle of unbiased empirical risk minimization, we derive a
nonasymptotic oracle inequality in the case where the law of the random shifts
is known. This inequality can then be used to obtain adaptive results on
Sobolev spaces as the number of observed curves tend to infinity. Some
numerical experiments are given to illustrate the performances of our approach.