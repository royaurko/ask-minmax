We study the stochastic multi-armed bandit problem with non-equivalent
multiple plays where, at each step, an agent chooses not only a set of arms,
but also their order, which influences reward distribution. In several problem
formulations with different assumptions, we provide lower bounds for regret
with standard asymptotics $O(\log{t})$ but novel coefficients and provide
optimal algorithms, thus proving that these bounds cannot be improved.