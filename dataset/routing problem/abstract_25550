Let $(X,Y)\in\mathcal{X}\times \mathcal{Y}$ be a random couple with unknown
distribution $P$. Let $\GG$ be a class of measurable functions and $\ell$ a
loss function. The problem of statistical learning deals with the estimation of
the Bayes: $$g^*=\arg\min_{g\in\GG}\E_P \ell(g(X),Y). $$ In this paper, we
study this problem when we deal with a contaminated sample $(Z_1,Y_1),...,
(Z_n,Y_n)$ of i.i.d. indirect observations. Each input $Z_i$, $i=1,...,n$ is
distributed from a density $Af$, where $A$ is a known compact linear operator
and $f$ is the density of the direct input $X$. We derive fast rates of
convergence for empirical risk minimizers based on regularization methods, such
as deconvolution kernel density estimators or spectral cut-off. These results
are comparable to the existing fast rates in Koltchinskii for the direct case.
It gives some insights into the effect of indirect measurements in the presence
of fast rates of convergence.