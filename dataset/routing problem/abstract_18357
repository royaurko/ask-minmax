In recent years, stochastic gradient descent (SGD) methods and randomized
linear algebra (RLA) algorithms have been applied to many large-scale problems
in machine learning and data analysis. SGD methods are easy to implement and
applicable to a wide range of convex optimization problems. In contrast, RLA
algorithms provide much stronger performance guarantees but are applicable to a
narrower class of problems. We aim to bridge the gap between these two methods
in solving constrained overdetermined linear regression problems---e.g.,
$\ell_2$ and $\ell_1$ regression problems. We propose a hybrid algorithm named
pwSGD that uses RLA techniques for preconditioning and constructing an
importance sampling distribution, and then performs an SGD-like iterative
process with weighted sampling on the preconditioned system. We prove that
pwSGD inherits faster convergence rates that only depend on the lower dimension
of the linear system, while maintaining low computation complexity.
Particularly, when solving $\ell_1$ regression with size $n$ by $d$, pwSGD
returns an approximate solution with $\epsilon$ relative error in the objective
value in $O(\log n \cdot nnz(A) + poly(d)/\epsilon^2)$ time. This complexity is
uniformly better than that of RLA methods in terms of both $\epsilon$ and $d$
when the problem is unconstrained. For $\ell_2$ regression, pwSGD returns an
approximate solution with $\epsilon$ relative error in the objective value and
the solution vector measured in prediction norm in $O(\log n \cdot nnz(A) +
poly(d) \log(1/\epsilon) /\epsilon)$ time. Finally, the effectiveness of such
algorithms is illustrated numerically on both synthetic and real datasets, and
the results are consistent with our theoretical findings and demonstrate that
pwSGD converges to a medium-precision solution, e.g., $\epsilon=10^{-3}$, more
quickly.