Additive models are popular in high--dimensional regression problems because
of flexibility in model building and optimality in additive function
estimation. Moreover, they do not suffer from the so-called {\it curse of
dimensionality} generally arising in nonparametric regression setting. Less
known is the model bias incurring from the restriction to the additive class of
models. We introduce a new class of estimators that reduces additive model bias
and at the same time preserves some stability of the additive estimator. This
estimator is shown to partially relieve the dimensionality problem as well. The
new estimator is constructed by localizing the assumption of additivity and
thus named {\it local additive estimator}. Implementation can be easily made
with any standard software for additive regression. For detailed analysis we
explicitly use the smooth backfitting estimator by Mammen, Linton and Nielsen
(1999).