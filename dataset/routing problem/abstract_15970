The main object of this paper is to present some general concepts of Bayesian
inference and more specifically the estimation of the hyperparameters in
inverse problems. We consider a general linear situation where we are given
some data $\yb$ related to the unknown parameters $\xb$ by $\yb=\Ab \xb+\nb$
and where we can assign the probability laws $p(\xb|\thetab)$,
$p(\yb|\xb,\betab)$, $p(\betab)$ and $p(\thetab)$. The main discussion is then
how to infer $\xb$, $\thetab$ and $\betab$ either individually or any
combinations of them. Different situations are considered and discussed. As an
important example, we consider the case where $\theta$ and $\beta$ are the
precision parameters of the Gaussian laws to whom we assign Gamma priors and we
propose some new and practical algorithms to estimate them simultaneously.
Comparisons and links with other classical methods such as maximum likelihood
are presented. Keywords: Bayesian inference, Hyperparameter estimation, Inverse
problems, Maximum likelihood.