Support vector machines (SVMs) appeared in the early nineties as optimal
margin classifiers in the context of Vapnik's statistical learning theory.
Since then SVMs have been successfully applied to real-world data analysis
problems, often providing improved results compared with other techniques. The
SVMs operate within the framework of regularization theory by minimizing an
empirical risk in a well-posed and consistent way. A clear advantage of the
support vector approach is that sparse solutions to classification and
regression problems are usually obtained: only a few samples are involved in
the determination of the classification or regression functions. This fact
facilitates the application of SVMs to problems that involve a large amount of
data, such as text processing and bioinformatics tasks. This paper is intended
as an introduction to SVMs and their applications, emphasizing their key
features. In addition, some algorithmic extensions and illustrative real-world
applications of SVMs are shown.