This paper addresses the problem of minimizing a convex cost function under
non-negativity and equality constraints, with the aim of solving the linear
unmixing problem encountered in hyperspectral imagery. This problem can be
formulated as a linear regression problem whose regression coefficients
(abundances) satisfy sum-to-one and positivity constraints. A normalized scaled
gradient iterative method (NSGM) is proposed for estimating the abundances of
the linear mixing model. The positivity constraint is ensured by the Karush
Kuhn Tucker conditions whereas the sum-to-one constraint is fulfilled by
introducing normalized variables in the algorithm. The convergence is ensured
by a one-dimensional search of the step size. Note that NSGM can be applied to
any convex cost function with non negativity and flux constraints. In order to
compare the NSGM with the well-known fully constraint least squares (FCLS)
algorithm, this latter is reformulated in term of a penalized function, which
reveals its suboptimality. Simulations on synthetic data illustrate the
performances of the proposed algorithm in comparison with other unmixing
algorithms and, more particulary, demonstrate its efficiency when compared to
the popular FCLS. Finally, results on real data are given.