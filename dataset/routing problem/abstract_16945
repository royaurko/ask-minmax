Model selection and sparse recovery are two important problems for which many
regularization methods have been proposed. We study the properties of
regularization methods in both problems under the unified framework of
regularized least squares with concave penalties. For model selection, we
establish conditions under which a regularized least squares estimator enjoys a
nonasymptotic property, called the weak oracle property, where the
dimensionality can grow exponentially with sample size. For sparse recovery, we
present a sufficient condition that ensures the recoverability of the sparsest
solution. In particular, we approach both problems by considering a family of
penalties that give a smooth homotopy between $L_0$ and $L_1$ penalties. We
also propose the sequentially and iteratively reweighted squares (SIRS)
algorithm for sparse recovery. Numerical studies support our theoretical
results and demonstrate the advantage of our new methods for model selection
and sparse recovery.