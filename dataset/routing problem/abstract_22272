Model reduction attempts to guarantee a desired "model quality", e.g. given
in terms of accuracy requirements, with as small a model size as possible. This
article highlights some recent developments concerning this issue for the so
called Reduced Basis Method (RBM) for models based on parameter dependent
families of PDEs. In this context the key task is to sample the {\em solution
manifold} at judiciously chosen parameter values usually determined in a {\em
greedy fashion}. The corresponding {\em space growth} concepts are closely
related to so called {\em weak greedy} algorithms in Hilbert and Banach spaces
which can be shown to give rise to convergence rates comparable to the best
possible rates, namely the {\em Kolmogorov $n$-widths} rates. Such algorithms
can be interpreted as {\em adaptive sampling} strategies for approximating
compact sets in Hilbert spaces. We briefly discuss the results most relevant
for the present RBM context. The applicability of the results for weak greedy
algorithms has however been confined so far essentially to well-conditioned
coercive problems. A critical issue is therefore an extension of these concepts
to a wider range of problem classes for which the conventional methods do not
work well. A second main topic of this article is therefore to outline recent
developments of RBMs that do realize $n$-width rates for a much wider class of
variational problems covering indefinite or singularly perturbed unsymmetric
problems. A key element in this context is the design of {\em well-conditioned
variational formulations} and their numerical treatment via saddle point
formulations. We conclude with some remarks concerning the relevance of
uniformly approximating the whole solution manifold also when the {\em quantity
of interest} is only of a {\em functional} of the parameter dependent
solutions.