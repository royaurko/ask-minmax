In modern data analysis, one is frequently faced with statistical inference
problems involving massive datasets. Processing such large datasets is usually
viewed as a substantial computational challenge. However, if data are a
statistician's main resource then access to more data should be viewed as an
asset rather than as a burden. In this paper we describe a computational
framework based on convex relaxation to reduce the computational complexity of
an inference procedure when one has access to increasingly larger datasets.
Convex relaxation techniques have been widely used in theoretical computer
science as they give tractable approximation algorithms to many computationally
intractable tasks. We demonstrate the efficacy of this methodology in
statistical estimation in providing concrete time-data tradeoffs in a class of
denoising problems. Thus, convex relaxation offers a principled approach to
exploit the statistical gains from larger datasets to reduce the runtime of
inference algorithms.