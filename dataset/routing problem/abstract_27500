Nonparametric empirical Bayes methods provide a flexible and attractive
approach to high-dimensional data analysis. One particularly elegant empirical
Bayes methodology, involving the Kiefer-Wolfowitz nonparametric maximum
likelihood estimator (NPMLE) for mixture models, has been known for decades.
However, implementation and theoretical analysis of the Kiefer-Wolfowitz NPMLE
are notoriously difficult. A fast algorithm was recently proposed that makes
NPMLE-based procedures feasible for use in large-scale problems, but the
algorithm calculates only an approximation to the NPMLE. In this paper we make
two contributions. First, we provide upper bounds on the convergence rate of
the approximate NPMLE's statistical error, which have the same order as the
best known bounds for the true NPMLE. This suggests that the approximate NPMLE
is just as effective as the true NPMLE for statistical applications. Second, we
illustrate the promise of NPMLE procedures in a high-dimensional binary
classification problem. We propose a new procedure and show that it vastly
outperforms existing methods in experiments with simulated data. In real data
analyses involving cancer survival and gene expression data, we show that it is
very competitive with several recently proposed methods for regularized linear
discriminant analysis, another popular approach to high-dimensional
classification.