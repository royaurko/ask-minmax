We consider the problem of transmitting data at rate R over a state dependent
channel p(y|x,s) with the state information available at the sender and at the
same time conveying the information about the channel state itself to the
receiver. The amount of state information that can be learned at the receiver
is captured by the mutual information I(S^n; Y^n) between the state sequence
S^n and the channel output Y^n. The optimal tradeoff is characterized between
the information transmission rate R and the state uncertainty reduction rate
\Delta, when the state information is either causally or noncausally available
at the sender. This result is closely related and in a sense dual to a recent
study by Merhav and Shamai, which solves the problem of masking the state
information from the receiver rather than conveying it.