The notion of approachability was introduced by Blackwell in the context of
vector-valued repeated games. The famous approachability theorem prescribes a
strategy for approachability, i.e., for `steering' the average vector-cost of a
given player towards a given target set, irrespective of the strategies of the
other players. In this paper, motivated from the multi-objective
optimization/decision making problems in dynamically changing environments, we
address the approachability problem in Markov Decision Processes (MDPs) and
Stackelberg stochastic games with vector-valued cost functions. We make two
main contributions. Firstly, we give simple and computationally tractable
strategy for approachability for MDPs and Stackelberg stochastic games.
Secondly, we give reinforcement learning algorithms to learn the approachable
strategy when the transition kernel is unknown. We also show that the
conditions that we give for approachability are both necessary and sufficient
for convex sets and thus a complete characterization. We also give sufficient
conditions for non-convex sets.