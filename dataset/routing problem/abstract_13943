Let $X,Y$ be jointly Gaussian vectors, and consider random variables $U,V$
that satisfy the Markov constraint $U-X-Y-V$. We prove an extremal inequality
relating the mutual informations between all ${4 \choose 2}$ pairs of random
variables from the set $(U,X,Y,V)$. As a first application, we show that the
rate region for the two-encoder quadratic Gaussian source coding problem
follows as an immediate corollary of the the extremal inequality. In a second
application, we establish the rate region for a vector-Gaussian source coding
problem where L\"{o}wner-John ellipsoids are approximated based on
rate-constrained descriptions of the data.