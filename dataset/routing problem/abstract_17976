Metric learning has been shown to be highly effective to improve the
performance of nearest neighbor classification. In this paper, we address the
problem of metric learning for Symmetric Positive Definite (SPD) matrices such
as covariance matrices, which arise in many real-world applications. Naively
using standard Mahalanobis metric learning methods under the Euclidean geometry
for SPD matrices is not appropriate, because the difference of SPD matrices can
be a non-SPD matrix and thus the obtained solution can be uninterpretable. To
cope with this problem, we propose to use a properly parameterized LogEuclidean
distance and optimize the metric with respect to kernel-target alignment, which
is a supervised criterion for kernel learning. Then the resulting non-trivial
optimization problem is solved by utilizing the Riemannian geometry. Finally,
we experimentally demonstrate the usefulness of our LogEuclidean metric
learning algorithm on real-world classification tasks for EEG signals and
texture patches.