The maximum entropy principle is a powerful tool for solving underdetermined
inverse problems. This paper considers the problem of discretizing a continuous
distribution, which arises in various applied fields. We obtain the
approximating distribution by minimizing the Kullback-Leibler information
(relative entropy) of the unknown discrete distribution relative to an initial
discretization based on a quadrature formula subject to some moment
constraints. We study the theoretical error bound and the convergence of this
approximation method as the number of discrete points increases. We prove that
(i) the theoretical error bound of the approximate expectation of any bounded
continuous function has at most the same order as the quadrature formula we
start with, and (ii) the approximate discrete distribution weakly converges to
the given continuous distribution. Moreover, we present some numerical examples
that show the advantage of the method and apply to numerically solving an
optimal portfolio problem.