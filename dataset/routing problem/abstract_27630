We study parameter estimation in linear Gaussian covariance models, which are
$p$-dimensional Gaussian models with linear constraints on the covariance
matrix. Maximum likelihood estimation for this class of models leads to a
non-convex optimization problem which typically has many local optima. We prove
that the log-likelihood function is concave over a large region of the cone of
positive definite matrices. Using recent results on the asymptotic distribution
of extreme eigenvalues of the Wishart distribution, we provide sufficient
conditions for any hill climbing method to converge to the global optimum. The
proofs of these results utilize large-sample asymptotic theory under the scheme
$n/p \to \gamma > 1$. Remarkably, our numerical simulations indicate that our
results remain valid for $\min\{n,p\}$ as small as 2. An important consequence
of this analysis is that for sample sizes $n \simeq 14 p$, maximum likelihood
estimation for linear Gaussian covariance models behaves as if it were a convex
optimization problem.