Date: 15 June 2005 Using runtime measured workload characteristics in parallel processor scheduling Thu D. Nguyen Affiliated with Department of Computer Science and Engineering, University of Washington , Raj Vaswani Affiliated with Department of Computer Science and Engineering, University of Washington , John Zahorjan Affiliated with Department of Computer Science and Engineering, University of Washington We consider the use of runtime measured workload characteristics in parallel processor scheduling. Although many researchers have considered the use of application characteristics in this domain, most of this work has assumed that such information is available a priori . In contrast, we propose and evaluate experimentally dynamic processor allocation policies that rely on determining job characteristics at runtime; in particular, we focus on measuring and using job efficiency and speedup. Our work is intended to be a first step towards the eventual development of production schedulers that use runtime measured workload characteristics in making their decisions. The experimental results we present validate the following observations: Despite the inherent inaccuracies of runtime measurements and the added overhead of more frequent reallocations, schedulers that use runtime measurements of workload characteristics can significantly outperform schedulers that are oblivious to these characteristics. Runtime measurements are sufficient for schedulers to achieve performance surprisingly close to that possible when a priori efficiency and speedup information is available. The primary performance loss, relative to the use of a priori information, is due to the transient decisions of the schedulers as they acquire information on the running applications, rather than to measurement and reallocation overheads. We consider both interactive environments, in which a response time directed scheduler is appropriate, and batch environments, in which maximizing useful instruction throughput is the primary goal. Our experiments are performed using prototype implementations running on a 50-node KSR-2 shared memory multiprocessor. This work was supported in part by the National Science Foundation (Grants CCR-9123308 and CCR-9200832) and the Washington Technology Center.