Password: This paper presents a novel Bee Colony based optimization algorithm, named Job Data Scheduling using Bee Colony (JDS-BC). JDS-BC consists of two collaborating mechanisms to efficiently schedule jobs onto computational nodes and replicate datafiles on storage nodes in a system so that the two independent, and in many cases conflicting, objectives (i.e., makespan and total datafile transfer time) of such heterogeneous systems are concurrently minimized. Three benchmarks – varying from small- to large-sized instances – are used to test the performance of JDS-BC. Results are compared against other algorithms to show JDS-BC's superiority under different operating scenarios. These results also provide invaluable insights into data-centric job scheduling for grid environments. Keywords Bee colony optimization ; Data replication ; Grid computing ; Job scheduling ; Resource allocation 1. Introduction Grid computing has matured into an essential technology that enables the effective exploitation of diverse distributed computing resources to deal with large-scale and resource-intensive applications, such as those found in science and engineering. A grid usually consists of a large number of heterogeneous resources spanning across multiple administrative domains. The effective coordination of these heterogeneous resources plays a vital key role in achieving performance objectives. Grids can be broadly classified into two main categories, computational and data, based on their application focus. In recent years, the distinction between these two classes of grids is much blurred, mainly due to the ever increasing data processing demand in many scientific, engineering, and business applications, such as drug discovery, economic forecasting, seismic analysis, back-office data processing in support of e-commerce, Web services, etc. [1] . In a typical scientific environment such as in High-Energy Physics (HEP), hundreds of end-users may individually or collectively submit thousands of jobs to access peta-bytes of distributed HEP data. Given the large number of tasks resulting from splitting these bulk submitted jobs and the amount of data being used by them, their optimal scheduling along with allocating their demanding datafiles becomes a serious problem for grids—where jobs compete for scarce compute and storage resources among available nodes. The Compact Muon Solenoid (CMS) [2] and the Large Hadron Collider (LHC) [3] are two well known case studies for such applications and are used as a motivation to design many systems including the algorithm in this article. Both systems constantly submit thousands of parallel jobs to access many shared datafiles. In such systems, each job is an acyclic data flow of hundreds of tasks in which CMS/LHR executable modules must run them in parallel [4] . Table 1 shows a typical number of jobs from users and their computation and data related requirements for CMS jobs [5] . Table 1. Number of simultaneously active users 100–1000 Number of jobs submitted per day 250–10,000 Number of jobs being processed in parallel 50–1000 Job turnaround time for jobs 0.2 s–5 months Number of datasets that serve as input to a sub job 0–50 Average number of datasets accessed by a job 250–10,000K Average size of the dataset accessed by a job from 30 GB to 3 TB Full-size table Grid schedulers are mainly divided into two types: (1) job-oriented and (2) data-oriented systems. In job-oriented systems, datafiles are fixed in location and jobs are scheduled, usually adhering to some objective such as power consumption [6]  and  [7] . In this case, the goal is to schedule jobs among computational nodes (CNs) to minimize the overall makespan of the whole system; here, it also is assumed that the overall transfer time of all datafiles are relatively negligible compare to executing jobs. The speed and number of available computer resources in different CNs and the network capacity between CNs and storage nodes (SNs) are typical considerations taken into account in such systems. For data-oriented systems, on the other hand, jobs are fixed in location and datafiles are moved or replicated in the system so that their accessibility by relevant jobs is increased. In contrast to the previous mode, here it is assumed that transfer time of all datafiles are much more time consuming than executing their dependent jobs. As a result, jobs will need less time to download the associated datafiles to execute; and therefore, the total execution time (i.e., makespan plus transfer time) of the system is reduced. The available storage in SNs and the capacity of interconnected network links between CNs and SNs are typical considerations in such allocations. From a practical point of view, neither of these two types of systems is adequate to deal with cases in which both computational jobs and datafiles are equally influential factors for efficient system utilization. Therefore, inappropriate distribution of resources, large queues, reduced performance, and throughput degradation for the remainder of the jobs are some of the drawbacks of assuming systems fit into just one of these two types. There are three main phases of scheduling in such complex systems [8] : (1) resource discovery, (2) matchmaking, and (3) job execution. In the first phase, resource discovery, grid schedulers conduct a global search to generate a list of all available resources as well as their limitations and history profiles in a system. In the second phase, matchmaking, schedulers try to determine best choices for executing jobs and replicating datafiles. Capacities of CNs/SNs as well as quality of the network connecting them are among the basic characteristics that need to be considered by schedulers to perform this phase. In the last phase, job execution, schedulers produce commands for CNs and SNs to execute jobs and replicate datafiles, respectively. Here, schedulers do not interfere with details of such commands and leave CNs/SNs to perform their allocated commands, including – but not limited to – datafile staging or system cleanups. In this work, the matchmaking process of schedulers was targeted and our contribution is a holistic scheduling approach to concurrently minimize two very important performance factors of a grid system, i.e., (1) makespan for executing all jobs, and (2) transfer time of all datafiles. Our approach adopts two collaborating mechanisms to schedule jobs and replicate datafiles with respect to their inter-dependencies as well as network bandwidth among CNs/SNs to host these jobs and datafiles. The rest of this paper is organized as follows. Section 2 presents related works. Section 3 overviews our proposed framework. Section 4 presents the problem statement. Section 5 briefly introduces the Bee Colony optimization algorithm as well as our approach (JDS-BC). Section 6 demonstrates the performance of our approach in comparison with other techniques. Discussion and analysis is presented in Section 7 , followed by conclusions in Section 8 . 2. Related work Several approaches already have been proposed to solve the bi-objective scheduling problem that is the focus of this work. Most of these methods make certain assumptions about the nature of jobs and datafiles to present a specific real system. Their solutions can be roughly categorized into two classes: online and batch [9] . In the online methods, it is assumed that jobs arrive one-by-one, usually following a predetermined distribution, and grid schedulers must immediately dispatch these jobs upon receiving them. In the batch methods (also known as batch-of-jobs or bulk) jobs are assumed to be submitted in bulk; and thus, grid schedulers need to allocate several jobs at the same time. Although the online mode can be a fair representation of small grids, CMS and LHR as well as many other massive systems always process the jobs in the batch mode. To date, most approaches usually use only one mode (online or batch) and only few exist that use both.