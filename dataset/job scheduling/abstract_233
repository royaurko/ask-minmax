Gang scheduling is an approach for resource allocation in parallel and distributed systems that combines time-sharing with space-sharing to ensure a short response time for interactive tasks and high overall system throughput. In this paper, we present queueing theoretic models for a particular gang scheduling system under a workload representative of large-scale engineering and scientific computing environments. We derive a detailed mathematical analysis of these models, from which we obtain closed-form expressions for different performance measures of interest. Our model and analysis is then used to analyze several fundamental performance tradeoffs associated with gang scheduling.