  Calculate near optimal solution for all sorts of grid. • Designed for real worlds jobs instead of the traditional simplistic view of jobs. • Significant outperformance in comparison with current algorithms. • Fast convergence speed; usually less than a minute for a medium-sized grid. This paper presents a novel heuristic approach, named JDS-HNN, to simultaneously schedule jobs and replicate data files to different entities of a grid system so that the overall makespan of executing all jobs as well as the overall delivery time of all data files to their dependent jobs is concurrently minimized. JDS-HNN is inspired by a natural distribution of a variety of stones among different jars and utilizes a Hopfield Neural Network in one of its optimization stages to achieve its goals. The performance of JDS-HNN has been measured by using several benchmarks varying from medium- to very-large-sized systems. JDS-HNN’s results are compared against the performance of other algorithms to show its superiority under different working conditions. These results also provide invaluable insights into scheduling and replicating dependent jobs and data files as well as their performance related issues for various grid environments. Keywords Job scheduling ; Network aware scheduling ; Data file migration policies ; Grid environments 1. Introduction Grid computing has matured into an essential technology that enables the effective exploitation of diverse distributed computing resources to deal with large-scale and resource-intensive applications, such as those found in science and engineering. A grid usually consists of a large number of heterogeneous resources spanning across multiple administrative domains. The effective coordination of these heterogeneous resources plays a vital key role in achieving performance objectives. Grids can be broadly classified into two main categories: computational and data, based on their application focus. In recent years, the distinction between these two classes of grids is much blurred, mainly due to the ever increasing data processing demand in many scientific, engineering, and business applications, such as drug discovery, economic forecasting, seismic analysis, back-office data processing in support of e-commerce, Web services, etc.  [1] . In a typical scientific environment such as in High-Energy Physics (HEP), hundreds of end-users may individually or collectively submit thousands of jobs to access peta-bytes of distributed HEP data. Given the large number of tasks resulting from splitting these bulk submitted jobs and the amount of data being used by them, their optimal scheduling along with allocating their demanding data files becomes a serious problem for grids—where jobs compete for scarce compute and storage resources among available nodes. The Compact Muon Solenoid (CMS)  [2] and the Large Hadron Collider (LHC)  [3] are two well known case studies for such applications and are used as a motivation to design many systems including the algorithm in this article. Both systems constantly submit thousands of parallel jobs to access many shared data files. In such systems, each job is an acyclic data flow of hundreds of tasks in which CMS/LHR executable modules must run them in parallel  [4] . Table 1 shows a typical number of jobs from users and their computation and data related requirements for CMS jobs  [5] . Table 1. Grid schedulers are mainly divided into two types: (1) job-oriented and (2) data-oriented systems. In job-oriented systems, data files are fixed in location and jobs are scheduled, usually adhering to some objective such as power consumption  [6]  and  [7] . In this case, the goal is to schedule jobs among Computational Nodes (CNs) to minimize the overall makespan of executing all jobs in the whole system; here, it also is assumed that the overall transfer time of all data files is relatively negligible compared to executing jobs. The speed and number of available computer resources in different CNs and the network capacity between CNs and Storage Nodes (SNs) are typical considerations taken into account in such systems. For data-oriented systems, on the other hand, jobs are fixed in location and data files are moved and/or replicated in the system so that their accessibility by their relevant jobs is increased. In contrast to the previous mode, here it is assumed that transfer time of all data files is much more time consuming than executing their dependent jobs. As a result, jobs will need less time to download the associated data files to execute and therefore, the execution time (i.e., makespan of executing jobs plus transfer time of data files) of the system is reduced. The available storage in SNs and the capacity of interconnected network links between CNs and SNs are typical considerations in such allocations. From a practical point of view, neither of these two system types is adequate to deal with cases in which both computational jobs and data files are equally influential factors for efficient system utilization. Therefore, inappropriate distribution of resources, large queues, reduced performance, and throughput degradation for the remainder of the jobs are some of the drawbacks of assuming systems fit into just one of these two types. There are three main phases of scheduling in such complex systems  [8] : (1) resource discovery, (2) matchmaking, and (3) job execution. In the first phase, resource discovery, grid schedulers conduct a global search to generate a list of all available resources as well as their limitations and history profiles in a system. In the second phase, matchmaking, schedulers try to determine best choices for executing jobs and replicating data files. Capacities of CNs/SNs as well as quality of the network connecting them are among the basic characteristics that need to be considered by schedulers to perform this phase. In the last phase, job execution, schedulers produce commands for CNs and SNs to execute jobs and replicate data files, respectively. Here, schedulers do not interfere with details of such commands and leave CNs/SNs to perform their allocated commands, including–but not limited to–data file staging or system cleanups. In this work, the matchmaking process of schedulers was targeted and our contribution is a holistic scheduling approach to concurrently minimize two very important performance factors of a grid system, i.e., (1) makespan for executing all jobs, and (2) transfer time of all data files. Our approach schedules jobs and replicates data files with respect to (1) characteristics of CNs/SNs in a system, (2) inter-dependences between jobs and data files, and (3) network bandwidth among CNs/SNs to host these jobs and data files.